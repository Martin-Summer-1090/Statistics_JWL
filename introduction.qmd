# **Introduction**

What is the biggest source of electricity production in Kenya? Answering 
this question needs data listing and recording various forms of 
electricity production in specific countries. An internationally 
acknowledged organisation, which collects and reports these data 
is the International Energy Agency (IEA). 

:::{.callout-tip}
## Accessing IEA data on the internet

If you have access to the internet,
you can look up data and reports by the IEA on it's website
https://www.iea.org/ 
:::



Now let's take a quiz and guess. What do you think is
actually the biggest source of electricity production in Kenya?

1. Coal
2. Renewable energy
3. Natural gas

Consulting the IEA data, you will find that the correct answer 
is *renewable energy*. A very interesting website 
called *gapminder*, which analyzes the answers of
many people to this question, finds that 61 % give a wrong answer to this
question. Maybe they find it hard to imagine that 80 % of energy production in 
Kenya is already fossil free thanks to huge 
sources of geothermal- and hydropower.^[Geothermal electricity generation uses the earth's natural heating energy - geothermal energy. A country needs to be located on a geothermal hot spot to make effective use of this energy source for electricity generation. At such a hotspot there are high temperatures beneath the earth's surface which naturally produces steam. This steam can be used to spin turbines connected to a generator. This mechanism then produces electricity. Hydropower uses  the water cycle to generate electricity by using dams to alter the flow of a river. The kinetic energy of the water spins turbines connected to a generator which produces electricity.]  Even if you break the answers down by
country you see that 38 % of people from Kenya get the answer wrong. Among
the people from the UK, who answer this question even 72 % answer wrongly.

If you are not familiar with the details of electricity production using 
geothermal- and hydropower or if you are not completely sure how 
to interpret a number like 38 %, don't worry for now. The point here is 
that you see that one useful consequence of being able to access, read and 
interpret data is that it can help to establish facts about the world. In this
way data can
help us to perhaps correct misconceptions we might have had about these
facts. 

:::{.callout-tip}
## The gapminder webpage
If you have internet access you can reach the gapminder webpage at https://www.gapminder.org/). I encourage you to visit this page at an occasion 
when you have access to the internet. You will be surprised how often you
might have a wrong guess about basic facts in the world.
:::

In this course you will learn how to work with data and how to learn from these
data in a systematic way. 

Learning from data entails more than just establishing
facts. This might not always be possible, either because you cannot access the
relevant data or you cannot completely access them, since doing so would be
way too expensive. When working with data you also need rigorous definitions
of concepts, so that you can transform your experience about the world into
data

Think for a moment about the following interesting 
example, which I learned from a wonderful book by the British 
statistician David Spiegelhalter [@Spi2019]. The example shows that even
just categorizing and labeling things in the world to measure them and turn
them into data can be challenging. A very basic question raised in 
the introduction of this book is:

::: {.callout-note icon=false}

## Question:

How many trees are there on the planet?

:::

It is clear that answering this question is more challenging than the task 
the IEA had to solve when listing energy sources by country around the world 
in a given time period. 
But before you go about to think how you might count all the trees on
the planet, you have to answer
an even more basic question, namely: What is a tree? Some of you might think
this is a silly and obvious question, which every child can answer. But 
what some might consider a tree others will consider just a shrub. 
Turning experience into data requires rigorous definitions. It turns out 
that such definitions can be given for trees. ^[For example the 
forestry expert Michael Kuhns writes: "...*Though no scientific definition exists to separate trees and shrubs, a useful definition for a tree is a woody plant having one erect perennial stem (trunk) at least three inches in diameter at a point 4-1/2 feet above the ground, a definitely formed crown of foliage, and a mature height of at least 13 feet. This definition works fine, though some trees may have more than one stem and young trees obviously don't meet the size criteria. A shrub can then be defined as a woody plant with several perennial stems that may be erect or may lay close to the ground. It will usually have a height less than 13 feet and stems no more than about three inches in diameter.*" [@Kuh]] 

But even with the definition at
hand you cannot just go around the planet and count every plant that meets the
criteria. So this is what the researchers investigating that question 
did according to [@Spi2019]: 

"...*They first took a series of of areas with a common type of landscape, known as a biome and counted the average number of trees per square kilometer. They then used satellite imaging to estimate the total area of the planet covered by each type of biome, carried out some complex statistical modelling, and eventually came up with an estimate of 3.04 trillion (3,040,000,000,000) trees on the planet. This sounds a lot, except that they reckoned there used to be twice this number*."

Now imagine that if long expert discussions are needed to precisely 
define something
so seemingly obvious as a tree, clearly more complex concepts
such as unemployment or the definition of the total value of goods and services
produced in a country in a given year, known as Gross Domestic Product 
or GDP, is
even more challenging. 

There is no automatism or mechanical receipt how we can 
turn experience into data
and the statistics that we use and produce are constructed on the basis of
judgement. It is a starting point for a deeper understanding of the world 
around us. This is one of the reasons why in this course statistics is
not only referred to as science, which it arguably is to some degree, but also
as an art.

One of the limitations of data as a source of knowledge about the world is
that anything we choose to measure will differ across places, across persons
or across time. When analyzing and trying to understand data we will always
face the problem how we can extract meaningful insights from this
apparent random variation. One challenge faced by statistics and one
core topic in this course will thus be how we can distinguish in data the
important relationships from the unimportant background variability. 

Exploring and finding such meaningful relationships or patterns 
in data using the
science of statistics and computational tools is one of the skills you will
learn in this course.

An example of a pattern in data is if we can spot a trend, data values 
which are for example increasing or decreasing.

Consider the following data from the World Bank, reporting the 
share of people in the world who are living in extreme poverty. 
Extreme poverty is defined by the World Bank, an international 
development finance organisation for low and middle income countries located in the US^[The Worldbank is an international financial institution founded along with the International Monetary Fund in the Bretton Wods conference in 1944. It is located in Washington D.C. and finances projects in low and middle income countries. It also collects and processes data globally to support its activities and conduct development research. The Worldbank makes its data public in print or through its website https://www.worldbank.org/en/home] as the percentage 
of people in the world who
have to live on less that $ 1.90 per day. 

Let us have a look at a table showing the first 10 observations of this
share

```{r}
#| label: tbl-example-poverty
#| tbl-cap: "Share of world polpulation living in extreme poverty, Source: World Bank"


# read poverty data from our project data folder
povdat_by_country <- read.csv("data/extreme_poverty/share-of-population-in-extreme-poverty.csv")
# select the years from 2000
povdat_world <- with(povdat_by_country, povdat_by_country[Entity == "World" & Year >= 2000, ])
# Keep only the year and the share
plot_data <- povdat_world[,c(3,4)]
# Rename variables
names(plot_data) <- c("Year", "Share")

# produce a table
library(knitr)
kable(head(plot_data, n= 10), row.names = F, digits = 1)
```

The table shows that the share of people living in extreme poverty has been decreasing year
after year for the first 10 years since the year 2000. This is a pattern which is called a
*downward trend*. 

Note that we did not show the whole series of numbers. The data points in our data-set actually
range until the year 2017. Printing them all in a table quickly produces very large and unwieldy
number array which is awkward to read.

Exploring data and detecting patterns is usually easier when we use the power of the human
visual system. Humans are very good in finding visual patterns. Looking for patterns is almost
viscerall for us. We can't help but looking for patterns. This almost instinctive human urge can
also be misleading and suggesting patterns to us where there are in fact none. In data
exploration we can make use of the power of visualization by plotting data and looking at them
graphically. The modern computer has made this form of displaying data particularly easy and
powerful and visualizing data in data exploration is another core skill you are going to
learn in this course.

So let us visualize the world poverty data. In our plot we draw the year on  the x-axes and
the share of people living in extreme poverty on the y-axes. This will give us a point for each
year. To facilitate the spotting of a trend, we connect the annual observations by a line.

```{r}
#| label: fig-share-of-people-in-extreme-poverty-world
#| fig-cap: Share of world population living in extreme poverty from 2000 - 2017
#| warning: false

library(ggplot2)

p <- ggplot(plot_data, aes(x = Year, y = Share)) + 
     geom_line() +
     geom_point() +
     xlab("")
p
```


Visualizing trends in world extreme poverty as an example of data
exploration. In this case the data pattern reveals a stunning fact. Over almost
two decades we can see a sharp fall in the share of extremely poor people when looked
at from a global perspective. Of course when we drill down to the level of individual
countries this trend will not look the same everywhere and there might be countries where the
share has actually increased. But overall we have seen a breathtaking steady decline. This is
good news.^[When you have access to the internet you can have a closer look at these data
at the very interesting website "our world in data" maintained by a consortium of Oxford University
and University College London. See https://ourworldindata.org/extreme-poverty. The website has
many interesting visualizations and options to select individual countries, country aggregates and
make other selections of the data.]

But does this trend mean that extreme poverty must disappear some years down the road? No, because
nobody can tell whether the trend of the last two decades will go on also in the future.

Statistics can help us to think more systematically about patterns and in 
making systematic guesses how a pattern might continue in the future. 
This is another core skill you will learn in this course: Making predictions, which 
means using available data and information to make informed guesses 
about data and information we do not have.

Let us go back to the share of people in the world living in extreme poverty. As
reported in our data the last actual observation for the global share in extreme
poverty from the world bank is from 2017. There are more recent data for some regions
but the global data since then are by now forecasts based on statistical techniques. The
basic ideas of these techniques and how to apply them to data is a core skill you will learn
in this course.

Now what does the World bank predict for the share of extreme poverty in the world? Let
us look at the data again graphically to visualize the prediction.

```{r}
#| label: fig-share-of-people-in-extreme-poverty-world-forecast
#| fig-cap: Share of world population living in extreme poverty from 2000 - 2017 with predictions until 2021
#| warning: false

obsdat <- data.frame(Year = plot_data$Year, Scenario = rep("Observed", 18), Share = plot_data$Share)

add_dat <- data.frame(Year = 2018, Scenario = "Observed", Share = 8.6)

pred_dat_precovid <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep("Pre-Covid-19", 4), Share = c(8.6, 8.4, 7.9, 7.5))

pred_dat_covidbase <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep("Covid-19-Baseline", 4), Share = c(8.6, 8.4, 9.1, 8.9))

pred_dat_coviddown <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep("Covid-19-Downside", 4), Share = c(8.6, 8.4, 9.4, 9.4))

dat <- rbind(obsdat, add_dat, pred_dat_precovid, pred_dat_covidbase, pred_dat_coviddown)

p <- ggplot(dat, aes(x = Year, y = Share, group = Scenario, color = Scenario)) + 
  geom_line(alpha = 0.5)+
  geom_point()+
  scale_color_manual(values=c('green', 'red', 'black', 'blue'))
p
```

Observe that the line showing the share of extreme poverty in the world takes different shapes
as we make predictions. What does this mean? 

At the root of the predictions is an abstract model, how the share of poverty changes over time. If
the underlying data would correspond to a world before Covid the falling trend in poverty 
would just continue to fall, as it has done continuously from 2000 onward. In the graph you
can see this scenario if you follow the black and the blue line. But taking the pandemic and
the consequences into account the prediction of the World Bank is that extreme poverty after a 
almost two decades downward trend will rise again. This you can see by following the 
green and the red line. How much, this rise actually will be in the end depends on data we can
not yet know.

When we make informed guesses based on observed data on information we
do not yet have there is uncertainty involved. Using the theory of probability in combination with
statistics we can quantify this uncertainty. Quantifying the uncertainty attached to
predictions is the third basic skill you will learn in this course.

:::{.callout-important}
## The three basic skills you will learn in this course

After successfully completing this course students you will have learned three core skills: 

1.  **Data exploration** or finding patterns in data and information through visualisation and computation.

2.  **Making predictions**, which means using available data and information to make informed guesses about data and information we do not have.

3.  To **quantify the uncertainty** we have to attach to our predictions.

:::

## The course broken down by units

In this course you will learn these three basic skills that will allow you to achieve a
level of data literacy supporting your future studies and providing you with powerful know
how for your future professional life in various fields. The modern world is full of data. But
reading these data with a critical mind and being able to extract information from them 
you need to know statistics and its basic techniques. This is exactly what you will learn in
this course if you actively participate and work through all the exercises and tasks as well as
the assigned projects.

The course is split into 8 units in total.

:::{.callout-note}
## Unit 1

In unit 1 we will give an overview of the course and we begin 
with the analysis and understanding of binary variables, variables that can be
imagined as simple yes or no questions and how they can be summarized as proportions or
percentages. You will learn about how the idea of expected frequency will promote the 
understanding of the meaning of these shares and how this provides a basic understanding of the
importance of these numbers.

:::

:::{.callout-note}
## Unit 2

In unit 2 we learn how to deal with lots of data, the typical situation we will face when we
do statistics. When there are lots of data we need instruments and tools to summarize them and
to get an overview. This overview is usually also very important for communicating the
data and the information they might contain.
We will also learn how we can use statistics to learn properties about large populations
by only making limited observations on some appropriately chosen 
subset of individuals from this population. The
gold standard in making such inferences possible is the concept of a so called random sample. But
at each step of the sampling procedure bias can crop up and invalidate our results leading to
wrong conclusions. The circumstances under which inference from samples to populations can be made
and how we can make sure to minimize bias we need a firm understanding of the opportunities 
and limits of this important technique.


:::

:::{.callout-note}
## Unit 3

In unit 3 we discuss when data analysis allows us to say something about what causes what. We learn
about the important concept of randomized trials. We will also learn what an observational study
is and how it differs from a randomized trial. This is an important concept we will learn through
the discussion of real world examples.
:::

:::{.callout-note}

## Unit 4

In unit 4 we will learn a key concept needed to make predictions. The technical term for this
concept in statistics is regression. It is a simple mathematical model describing how a set of
explanatory variables varies systematically with a response variable. We will learn to construct
such models and to interpret them correctly. We will also cover related techniques which have
become very important recently and entered the statistical toolbox from the field 
of computer science. These methods are known under the notion of algorithmic prediction or machine
learning.

:::

:::{.callout-note}

## Unit 5

In unit 5 we encounter the first time tools for quantifying uncertainty. We learn how to determine
and use uncertainty intervals by using a technique which is called the bootstrap. 
Being able to determine such intervals is extremely important in communicating statistics and 
for supporting a systematic and sound answer to the question: How sure can we be about an estimate.

:::

:::{.callout-note}

## Unit 6

In unit 6 we deepen the knowledge how to quantify uncertainty by introducing 
basic ideas of probability theory. Probability theory provides a formal language and 
mathematics for dealing with chance phenomena. Probability is often 
counter intuitive but using the idea of expected frequency improves intuition. 
Probability ideas can be very useful in statistics even if there is no 
explicit use of a randomizing mechanism. Many social phenomena show a 
remarkable regularity in their overall pattern while individual events are entirely unpredictable.

:::

:::{.callout-note}

## Unit 7

In unit 7 we put statistics and probability theory together. This allows us to both simplify
ideas and techniques how to quantify uncertainty. The combination of the two field makes 
the tools for quantifying uncertainty at the same time more powerful. Combining statistics and
probability theory is at the heart of statistics as a science. It makes all the ideas developed
in unit 1 to 6 very versatile and powerful.

:::


:::{.callout-note}

## Unit 8

In unit 8 we learn how to leverage the knowledge of this course to answer questions and to claim
discoveries. You will learn how statistics is used in the sciences and how it supports to develop
our knowledge of the world. It pulls many ideas of  the whole course together and when you have 
mastered this unit you have mastered all the basic skills we want to develop in this course, data
exploration, prediction and quantifying uncertainty.

:::




## Activity: Guessing ages, cancer etc.