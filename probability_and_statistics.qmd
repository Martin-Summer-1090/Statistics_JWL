# Putting probability and statistics together

In the last unit on probability we analyzed 
randomizing devices such as dice and coins. 
We have assumed the in these cases that we can enlist all the possible 
outcomes that might potentially be observed and we can attach chances or probabilities
to these outcomes. One example was  1/2 for a fair coin coming up Heads, or 1/6
for the die landing on any of its faces, or 1/36 for two dice showing one of the
possible combinations of points. 

When we attach to each of these outcomes some quantity, for example 1 for Heads
and 0 for Tails, probability theorists say that we have a **random variable** with
a probability distribution. The randomizing device is such that it is ensured 
that the observation is generated at random from this distribution. This
probability distribution can be described by parameters, such as the mean,
for example in the case of the Poisson Distribution, or the mean and the
standard deviation in the case of the normal distribution. We also learned how
we can use R to draw data points from a probability distribution described
by its parameters.

The usual situation, we face in statistics is that we have to deal with lots
of data and we give a summary description of these data usually by summary
statistics like the mean, the median or other percentiles.

A fundamental step and a key idea in statistics, where probability and 
statistics come together, is the idea that we can view a particular statistics 
like
the mean also as a random variable and their values as draws from their
own probability distribution. This is a conceptually challenging and
difficult idea but we have encountered it already once when we
discussed the bootstrap.

There we took repeated samples with replacement from the distribution of
human height data and computed the mean height at each step. In this way
we generated a distribution of the mean height that gave us some idea
about the precision of this estimate.

In contrast to bootstrapping which is a computer intensive simulation 
technique, probability theory provides another approach to give
a mathematical answer to the same question: How confident can I be
about the precision of my statistics. In this approach we can
save on computing resources but we need - as in any mathematical theory -
assumptions and we need to think about whether these assumptions are 
justified in each single situation where we appy the theory.


::: {.callout-note appearance="simple"}
## Overview


:::

::: {.callout-important appearance="simple"}

## Keywords


:::

## Boys and Girls: Using probability theory to find a sampling distribution.

In unit 3 we talked about random samples, where we draw a sample from 
a population and want to learn from the sample statistics about the
value of the statistics - say a proportion - in the underlying
population.

For this we need to know how a known population gives rise to different
samples. Let us think about this problem in terms of concrete example. 

In a sample of
100 newborn babies we find that there are 48.7 % girls and 51.3 %
boys. What can we say about the proportion of baby girls in the
population of all newborns in a country?

Suppose the sample has a size of only 1. Then the proportion of baby
girls must be either 0 or 1 depending whether we have selected a 
girl or a boy. These events occur with probability 0.513 (boy)
ans 0.487 (girl).

This is actually the natural
probability that a baby is a girl or a boy and it has been
historically very stable. The outcomes are also independent across births.
The resulting probability distribution is shown here:

```{r}
barplot(c(0.513, 0.487), names.arg = c("0 (boy)", "1 (girl)"), 
        main = "Probability Distribution: Sample size 1",
        xlab = "Proportion of girls",
        ylab = "Probability")
```

Suppose our sample consists of two babies drawn at random from the population
of newborns. Then the proportion of girls will be either 0 (both boys), 
0.5 (one girl and one boy) or 1 (both girls).

Let's work out the probabilities of these proportions: The probability of 
two boys must be $0.513 \times 0.513 = 0.263$. (multiplication rule and
independence) The probability of two girls
must be $0.487 \times 0.487 = 0.237$ and so the probability of one of each must
be $1 - 0.263 - 0.237 = 0.5$ (Complement rule). The resulting probability 
distribution is here.

```{r}
barplot(c(0.236, 0.5, 0.237), names.arg = c("0 (both boys)", "0.5 (boy and girl)", "1 (both girls"),
        main = "Probability Distribution: Sample size 2",
        xlab = "Proportion of Girls",
        ylab = "Probability")
```

We could now use probability theory to work out the probability distribution 
for even larger and larger samples. How would we do that?

Suppose we would like to find the probability that in a random sample of
5 newborns we have two boys and then three girls, like,
for example, this: BBGGG. There are, of course, many other ways in which there
can be two boys in a sample of five, for example GGGBB.

To solve the problem using probability theory we must first find all the 
possible ways two boys and three girls can occur, compute the chance of
each and then use the addition rule to add up the chances. Let us skip
the complicated task of finding all the possible ways and concentrate 
on the second task first.

The chance of a pattern BBGGG is:
$0.513 \times 0.513 \times 0.487 \times 0.487 \times 0.487 = (0.513)^2 \times
(0.487)^3$ This follows from the multiplication rule as well as from the 
assumption of independence. Similarly the chance of GGGBB would compute as:
$0.487 \times 0.487 \times 0.487 \times 0.513 \times 0.513 = 
(0.487)^3 \times (0.513)^2$, so exactly the same chance.

Now, how many patterns are there? The number of patterns is given by
the **binomial coefficient**:
\begin{equation*}
\frac{5 \times 4 \times 3 \times 2 \times 1}
{(2 \times 1) \times (3 \times 2 \times 1)} = 10
\end{equation*}
There are 10 different patterns of 2 boys and three girls, so the chance is:
$10 \times (0.513)^2 \times (0.487)^3$, or roughly 30 %.

Binomial coefficientas look messy and so mathematicians have invented 
a cleaner notation using an exclamation mark ! as a special 
symbol to indicate the result of multiplying together a number
and all the numbers which come before it. For example
\begin{eqnarray*}
1! &=& 1\\
2! &=& 2 \times 1 = 2 \\
3! &=& 3 \times 2 \times 1 =  6 \\
4! &=& 4 \times 3 \times 2 \times 1  =  24
\end{eqnarray*}
The exclamation mark reads **factorial** and sure enough there is an
R function for it:
```{r}
#| code-fold: false
factorial(4)
```

### The binomial distribution

All these arguments and reasoning can be packed into a
formula giving the chance that an event will occur 
exactly $k$ times out of $n$. This is

::: {.callout-important}
## Binomial Formula

The chance that an event $X$ will occur exactly $k$ times out of $n$ is
given by the **binomial formula**
\begin{equation}
P(X = k) = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}
\end{equation}

In this formula, $n$ is the number of trials, $k$ is the number of
times the event is to occur, an $p$ is the probability that the event
will occur on any particular trial. 

Mathematicians often use a particular
notation for the binomial coefficient in this formula, so the
formula looks like:
\begin{equation}
P(X = k) = \binom{n}{k}p^k(1-p)^{n-k}
\end{equation}
where $\binom{n}{k}$ denotes the binomial coefficient.

The assumptions are:

- The value of $n$ must be fixed in advance
- $p$ must be the same for each trial
- The trials must be independent
:::

R has a built in function for the binomial formula
and we can use it for probability
computations where it is involved. This function is
called `dbinom()` and is to be used like this:

::: {.callout-important}
## Binomial Formula in R

The chance that an event will occur exactly $k$ times out of $n$ is
given by the **binomial formula** implemented in R as

```
dbinom(x,          # X-axis values (x = 0, 1, 2, ..., n)
       
       size,        # Number of trials (n > = 0)
       
       prob,        # The probability of success on each trial
       
       log = FALSE) # If TRUE, probabilities are given as log
```
:::

Let us demonstrate its use this to work out the theoretical probabilities for
the proportion of girls in a sample of 5 for all possible cases 
(0, 1, 2, 3, 4, or 5 girls).

```{r}
#| code-fold: false

barplot(dbinom(seq(0,5, by = 1), 5, 0.487), names.arg = as.character(seq(0,5, by = 1)/5),
        main = "Probability distribution: Sample size 5",
        xlab = "Proportion of girls",
        ylab = "Probability")

```

Let's check your understanding of the formula: Denote the event that a
randomly chosen newborn will be a girl by $X$. According to the binomial 
formula
the probability that in a sample of 5 newborns we will have, say two girls, 
given
the probability that a baby is a girl of $0.487$ is:

\begin{eqnarray*}
P(X = 0) &=& \binom{5}{2}0.487^2(1-0.487)^{5-2} \\
         &=& \frac{5!}{2!(5-2)!} 0.487^2(1-0.487)^{5-2} \\
         &=& \frac{5\,4\,3\,2\,1}{(2\,3\,2\,1)} 0.487^2(1-0.487)^{5-2} \\
         &=& 0.3201917
\end{eqnarray*}

This is what we get if we do the computation by hand and using pencil and
paper. When we use R we should get the same result:

```{r}
#| code-fold: false
dbinom(2,5,0.487)

```

Now look at the barplot: If we have 2 girls out of 5 the proportion of girls
in the sample would be $2/5$ or $0.4$. The theoretical probability
for this proportion would be about $0.32$. This is exactly what you
see in the barplot.

For the plot we have computed these probabilities and proportions
for all constellations 0, 1, 2, 3, 4 and 5 girls. We have created
this sequence with the R `seq()` function. For the `names.arg`
argument we have created the same sequence but divided every 
element of the sequence by 5 to express the number as a proportion
and then transformed the numbers to a vector of type character by
the `as.character()` function, since `names.arg` requires a
vector of characters.

I think by now you understand how we apply the formula both in math and in
R to figure out the probabilities. Here we do the same exercise for
$n= 100$ and $n=1000$. We do not show the code anymore her just the
plots:

```{r}
par(mfrow = c(1,2))

barplot(dbinom(seq(0,100, by = 1), 100, 0.487), names.arg = as.character(seq(0,100, by = 1)/100),
        main = "Sample size 100",
        xlab = "Proportion of girls",
        ylab = "Probability")

barplot(dbinom(seq(0,1000, by = 1), 1000, 0.487), names.arg = as.character(seq(0,1000, by = 1)/1000),
        main = "Sample size 1000",
        xlab = "Proportion of girls",
        ylab = "Probability")
```

When we draw random samples, like in this example drawing babies from a
population of newborns, then the chance process of random
sampling delivers a number and then another and
then another and so on, in the specific example, 1 for a girl
and 0 for a boy.

Mathematicians call the value around which these number vary the
**expected value**, and the amounts of the numbers being off
the in size from the expected value the **standard error**.
The formulas for the expected value and the standard error depend on the chance
process which generates the numbers.

The figures above have two notable features. 

1. The probability distribution
for the proportion tends to become regular, symmetric and bell shaped, like
a normal curve. This is something we already observed when we discussed the
bootstrap.

2. The distribution gets tighter as the sample size increases.

But before we disucss these ideas and their inplications further,
let us unpack the notions of expected value and standard error 
a bit more.

### Expected value

The formula for the expected value depends on the chance process which
generates the random numbers. Let us introduce the general formulas
by an example. Suppose we make hundered random draws with replacement
from the box, containing the following tickets, symbolized by an
R vector called `box` and the draws symbolized by an R vector
called `draws`.

```{r}
#| code-fold: false

box <- c(1,1,1,5)

draws <- sample(box, size = 100, replace = TRUE)
```

We would like to know the expected value of the sum of these
draws, when we add up the numbers on the tickets we have drawn at
random from the box. How large should it approximately be?

There should be about 1/4 cases in which the ticket shows 5 and
3/4 of cases where it shows 1. With 100 draws, this would be
$25 \times 5 + 75 \times 1 = 200$. This is the expected value.

The formula has two ingredients:

1. The number of draws
2. The average of the numbers of the tickets in the box.

::: {.callout-important}
## Expected value for the sum of random draws from a box

The expected value for the sum of draws made at random with
replacement from a box equals

\begin{equation}
(\text{number of draws}) \times (\text{average of box})
\end{equation}

:::

The logic here is that the average of the box amounts to
\begin{equation}
\frac{1+1+1+5}{4} = 2
\end{equation}
On average each draw adds around 2 to the sum. With 100 draws then
the sum must be around $100 \times 2 = 200$.

See what we have in our actual random draws:
```{r}
#| code-fold: false

sum(draws)
```
pretty close.

::: {.callout-tip}


## Example: Boys and girls again.

In the example with boys and girls the logic is the same. Here the box is:
```{r}
#| code-fold: false

baby_box <- c("girl", "boy")
```

We draw 100 babies with replacement and with probabilities $0.487, 0.513$.
```{r}
#| code-fold: false

baby_draws <- sample(baby_box, 100, replace = T, prob = c(0.487, 0.513))
```
Here you see a nice feature of the `sample()`function, we have used a lot in 
this lecture. You can tell sample with which probabilities it should select
boys and girls and we took the natural probabilities we already used above.

We draw 100 babies and the average in  the box is 0.487, if we encode the
outcome girl as 1 and the outcome boy as 0, like we did already before. So
the expected value is:

$100 \times 0.487 = 48.7$

Now count the girls using logicals:

```{r}
#| code-fold: false

sum(baby_draws == "girl")
```

:::

### Standard error

We have seem before that the expected value of the sum of the tickets in the
box is $200$. The actual outcome of the random process was, however,
`r sum(draws)`, which is `r sum(draws) - 200` off the expected value.

This is a chance error. The sum will be off the expected value by the
size of a chance error, so that:

\begin{equation}
\text{sum} = \text{expected value} + \text{chance error}
\end{equation}

How big is this chance error likely to be? 

::: {.callout-important}
## Likely size of chance error

The sum is likely to be
around its expected value but to be off by a chance error similar in
size to the standard error.
:::

There is also a formula to use when we compute the standard error
for a sum of draws made at random from a box with replacement. This
formula plays a key role in the statistical procedures we discuss in this
and in the next unit.

::: {.callout-important}
## Square root law

When drawing at random with replacement form a box of numbered tickets,
the standard error for the sum of the draws is
\begin{equation}
\sqrt{\text{number of draws}} \times (\text{standard deviation of box})
\end{equation}
:::

This formula has two ingredients: The square root of the number of draws and the
standard deviation of the list of numbers in the box. If there is a lot of
spread in the box, the standard deviation is big and it is hard to predict
how the draws will turn out. So the standard error must be big too.

The sum of two draws has more variability than one draw. The sum of 100 draws
is still ore variable. As the number of draws grows the sum gets harder to
predict, the chance error gets bigger and so does the standard error. However
the standard error goes up slowly with a factor equal to the square root
of the number of draws. With 100 draws, therefore the sum is 10 times more
variable than with a single draw.

Note that the standard deviation and the standard error are two different
concepts:

The standard deviation is for the list of numbers in the box, in our example
$1,1,1,5$. This is 
`r round(sd(c(1,1,1,5)*sqrt(3/4)),2)`

The standard error refers to the outcome of the chance process, in
our case drawing at random from the box of numbers. This is
for our particular example `r 10*round(sd(c(1,1,1,5)*sqrt(3/4)),2)`

A statistics, like the mean or a proportion, when viewed as a random
variable has a standard error. This is to distinguish from the standard
deviation of the population distribution from which this statistics derives.

## The central limit theorem and the normal distribution

When we get numbers from a chance process, the expected value and
the standard error give us an impression where the numbers will
be. If we want to get the full picture, however, we get the
whole picture from a **probability histogram**.

::: {.callout-tip}
## Probability histogram

- A probability histogram represents probabilities, not data.

- A probability histogram represents chance by area.
:::

Here is an example from gambling:

Craps is a casino game where you throw two dice and gamblers bet
on the sum of numbers the two dice will show. We can find the 
chances of the combinations by probability theory (try it). But here
we will simulate using concepts we already know and learend in this
course. We write a function tossing two dice and sum up their outcomes
and repeat this many times and plot the outome (try it)

```{r}

craps <- function(){
  # construct a die
  die <- 1:6
  # roll twice
  out <- sample(die, 2, replace = T)
  # sum the outcome of the rolls
  sum(out)
}

crap_sim <- replicate(100, craps())
#hist(crap_sim, breaks = seq(1.5, 12.5, by = 1), freq = F, plot = F)

h100 <- hist(crap_sim, breaks = seq(1.5, 12.5, by = 1), plot = F)

h100$counts <- h100$counts/sum(h100$counts)

plot(h100,
     xlab = "Number of dots",
     ylab = "Percent",
     main = "One Hundered Repetitions",
     axes = F,
     xlim = c(0,13),
     ylim = c(0,0.3))

# add labels below each column
axis(1, at = 2:12, labels = 2:12)
axis(2, at =c(0,0.1, 0.2, 0.3), labels = c("0 %","10 %", "20 %", "30 %"))


```

Now increase the repetitions to 1000

```{r}
crap_sim <- replicate(1000, craps())
#hist(crap_sim, breaks = seq(1.5, 12.5, by = 1), freq = F, plot = F)

h1000 <- hist(crap_sim, breaks = seq(1.5, 12.5, by = 1), plot = F)

h1000$counts <- h1000$counts/sum(h1000$counts)

plot(h1000,
     xlab = "Number of dots",
     ylab = "Percent",
     main = "One Thousand Repetitions",
     axes = F,
     xlim = c(0,13),
     ylim = c(0,0.3))

# add labels below each column
axis(1, at = 2:12, labels = 2:12)
axis(2, at =c(0,0.1, 0.2, 0.3), labels = c("0 %","10 %", "20 %", "30 %"))
```

And to 10000

```{r}
crap_sim <- replicate(10000, craps())
#hist(crap_sim, breaks = seq(1.5, 12.5, by = 1), freq = F, plot = F)

h10000 <- hist(crap_sim, breaks = seq(1.5, 12.5, by = 1), plot = F)

h10000$counts <- h10000$counts/sum(h10000$counts)

plot(h10000,
     xlab = "Number of dots",
     ylab = "Percent",
     main = "Ten Thousand Repetitions",
     axes = F,
     xlim = c(0,13),
     ylim = c(0,0.3))

# add labels below each column
axis(1, at = 2:12, labels = 2:12)
axis(2, at =c(0,0.1, 0.2, 0.3), labels = c("0 %","10 %", "20 %", "30 %"))
```

You can see that the probability histograms observed from the random experiments 
of throwing the two dice 100, 1000 and 10000 times get closer and closer to the
ideal probability historgam, where the relative frequency of events from the
random experiment come arbitrarily close to the theoretical probabilities.

There are 6 events in 36 where the sum of the die could be 7 (try to convince 
yourself!). This is 16 2/3 %. The area of the rectangle over 7 in the probability
histogram, therefore equals 16.3%. The total area of all the rectangles sum up to
100 %.

### Probability histograms and the normal curve

When we discussed the theoretical share of baby girls in a random sample 
drawn from a large using the binomial distribution, we saw a remarkable
fact:
population of newborn babies:

```{r}
par(mfrow = c(1,2))

barplot(dbinom(seq(0,100, by = 1), 100, 0.487), names.arg = as.character(seq(0,100, by = 1)/100),
        main = "Sample size 100",
        xlab = "Proportion of girls",
        ylab = "Probability")

barplot(dbinom(seq(0,1000, by = 1), 1000, 0.487), names.arg = as.character(seq(0,1000, by = 1)/1000),
        main = "Sample size 1000",
        xlab = "Proportion of girls",
        ylab = "Probability")
```

We saw in this example that the variability of the observed proportion gets
smaller as the sample size increases. This is the classic 
**law of large numbers** detected by Jacob Bernoulli in the 17th
century. As we increase the sample, the proportion of girls in the
outcome of the random experiment comes closer and close to the
true proportion of 0.487.

In unit 3 we introduced the normal curve, which we found described the
empirical distribution of human height data with surprising accuracy.
The argument wy this might be the case was that human height depends on
a huge number of factors, all of which have for themselves little influence
on the outcome. Adding up all these small effects gets to a normal 
distribution.

This is also the reasoning behind the famous **central limit theorem**
detected in 1733 by Abraham de Moivre. This theorem says:

::: {.callout-important}
## Central limit theorem

When drawing at random with replacement from a box, the probability
histogram for the sum will follow the normal curve, even is the
contents of the box is not normally distributed, provided the
number of draws is large enough.

:::

This is a remarkable fact. Whatever the the shape of the population
distribution from which each of the original measurements were sampled,
for large sample sizes their average can be considered as drawn from a 
normal curve. This will have a mean that is equal to the standard deviation
that has a simple relationship to the standard deviation of the original
population and is known, as we discussed before, as the standard error.

Now we see also why in the bootstrap examples we discussed in unit 5, the
distribution of the statistics we got from repeated resamples looked more
and more like a normal distribution. This is the consequence of the central
limit theorem.

## The 95 % confidence interval

While the central limit theorem is a remarkable and extraordinary law of
nature, the question arises, how this law can help us work out the accuracy of 
estimates without using the heavy machinery of computer simulation.

Probability theory ha s shown how the distributions of statistics like
means and proportions look like when the data are drawn from known 
populations.

But the usual situation in practice is the other way round: Most of the time
in a practical data analysis situation we have to go from a single sample back
to saying something about a possible underlying population. This is the
process of inductive inference, we discussed in unit 3.

So far in this unit we have studies chance processes using probability
theory. We ran simulations, like flipping coins or drawing tickets from
a box or sampling girls and boys from a population of newborns using emprical
probabilities. We have to distinguish two form of uncertainty. One type
of uncertainty is related to the stituation when I run a simulation before I
press the ENTER key on my computer. This relates to the chance of an 
unpredictable event. This is called **aleatory uncertainty**.

When I have run the simulation and before I look at the outcome, there is 
another situation of uncertainty. The outcome is fixed, because I have already
run the simulation but I do not yet know the outcome yet. This form of 
uncertainty is called **epistemic uncenrtainty**.

Statistics is used to deal with situations of epistemic uncertainty about some
quantity in the world, like the mean human hight or the proportion of baby girls
among the newborns in a country. 

For example, wehn we conduct a surevy we do not know the true proportions of
- for example stunted and wasted children - as in the DHS example we discussed
in unit 2. These fixed but unknown quamtities are called **parameters**  in
statistics. Just as in the example we gave before, before we do the survey we
have aleatory uncertainty because of the random sampling. After we have drawn
the sample and have the data, we use a probability model to deal with and
understand our epistemic uncertainty. Probability theory which tells us what to
expect is used to tell us what we can learn from what we have observed. This
is the basis for statistical inference and this is the way in which
statistics and probability work together to understand data.

One of the most important concepts from statistical inference the 
**confidence interval** is built on this idea. Let me explain the
steps how we derive an uncertainty interval around an estimate, without
resorting to a bootstrap simulation. 

There are three stages, which we describe here following @Spi2019:

1. Probability theory tell us for any particular population parameter,
an interval in which we expect the observed statistics to lie with
95 % probability.

2. Then we observe the particular statistics, say a mean or a proportion.

3. Finally we work out the range of possible population parameters, for which
our statistics lies in their 95 % prediction intervals. This range we call a
**confidence interval**.


The label of this interval is 95% confidence interval, because with repeated 
application 95 % of these intervals contain the true value. Note that this is
different from saying that there is a 95 % probability that this particular
interval contains the true value, an incorrect interpretation which you might
encounter often even in the academic literature.

### Computing confidence intervals

The theory of computing confidence intervals was consolidated and introduced
in the 1930ies by Jerzy Neyman and Egon Pearson, wo was the son of Karl Person,
whose data on the heights of fathers and sons we had begin studying in unit 4.

In our leading example we found that the sample percentage of girls in our
simulated sample of 1000 was 487 or 48.7 %. How far can the population percentage
be from 48.7% ? The standard error is estimated as 15.8, suggesting that the
chance error is around 1.58 %. So the population percentage could be off by 
around 1.6 %

Let us now show how the probability histogram for the number of baby girls gets
close to the normal curves when the sample gets large, as in our example of
1000 draws.

```{r}
# x-axis grid

set.seed(45)

x <- rbinom(1000, 1000, 0.487)
x2 <- seq(0,1000, by = 1)

# normal curve

fun <- dnorm(x2, mean = mean(x), sd = sd(x))

# Histogram
h <- hist(x, breaks = 100,
     plot = F)

# par(mar = c(5, 4, 4, 8) + 0.3)  
# 
plot(h, col = "white",
     freq = FALSE,
     xlab = " ",
     ylab = "Percent per girl",
     ylim = c(0, max(fun)),
     main = "Histogram of number of girls",
     xaxt = "n",
     yaxt = "n"
     )

# overlay normal curve

par(new = TRUE)

lines(x2, fun, col = 2, lwd = 2)
 axis(side = 4, at = c(0,0.005,0.01, 0.015, 0.02, 0.025), 
      labels = c(labels = c("0 %", "8 %", "16 %", "24 %", "32 %", "40 %")), 
      ylab = "Percent per standard unit")
 
# adding axis

 axis(1, at = c(440, 460, 480, 500, 520, 540),
      labels = c("440 cm", "460 cm", "480 cm", "500 cm", "520 cm", "540 cm"))

 axis(1, at = c(440, 460, 480, 500, 520, 540), 
     labels = as.character(round(c(-2.9746835, -1.7088608, -0.4430380,  0.8227848,  2.0886076,  3.3544304), 2)), line = 2.5)
 
axis(2, at = c(0,0.005,0.01, 0.015, 0.02, 0.025), 
      labels = c(labels = c("0 %", "0.5 %", "1 %", "1.5%", "2 %", "2.5 %")), 
      ylab = "Percent per girl")

abline(v = 487, col = 4)

```

There are two vertical axes in the figure. The probability histogram is 
drawn relative to the left axis showing percent per baby-girl. The normal
curve is drawn relative to the right axis showing percent per standard unit.
To see how the scales match up:  2 % per baby girl matches up - for example-  
with 32 % per standard unit because the Standard error is about 16 (15.8) so
there are about 16 baby-girls to the standard unit and 32/16 = 2. The
same reasoning applies to the other numbers.

The normal curve can now used to approximate probabilities. Assume we would
like to know the probability that we observe between 482 and 492 girls in 
a sample of 1000. The expected number of girls is 487 and the standard error 
is 15.8. The chance of getting exactly 482 girls is the
area of the rectangle between 475 and 480 on the
number of girls scale and has a height of 0.103. The area is thus 0.52.

In standard
units the base of the rectangle goes from -0.76 to - 0.44.

Since the histogram and the normal curve almost coincide, so the
area of the rectangle between 482 and 483 is approximately
equal to the area under the normal curve between -0.32 and
- 0.25.

The area of the rectangle is 0.021. When we approximate by the 
normal curve we get:

```{r}
#| code-fold: false

pnorm(-0.25) - pnorm(0.32)
```

This approximation is excellent.

With this approximation method justified by the central limit theorem we
can ask how far the population share of new born baby girls can deviate
from 48.7 %- The standard error was estimated at 1.58 %, so the
population proportion could easily be between 47.12 and 50.28.

Since these are chance errors there is the possibility that errors
of larger size, for example errors of 2 standard errors occur, if
infrequently. What happens, when we take the cutoff at that?
Take for example the interval of two standar errors above and below the
expected value:

![Confidence Interval](pictures/confidence_interval.png)
This is a **confidence interval** for the population percentage, with a 
**confidence level** of about 95%. This means that you can be about 95% 
confident that the population proportion of baby girls is inside the+interval 
(47.12, 50.28) percent.

What, if you want different confidence levels? Anything except 100 % is
allowed by going the appropriate number of standard errors right and left
from the expected value. For example:

1. The interval "sample percentage" plus/minus 1 stndard error is an
68 % confidence interval for the population percentage.

2. The interval "sample percentage" of plus/minus 2 standard errors is
a 95 % confidence interval fro the population percentage.

3. The interval "sample percentage" of plus minus 3 standard errors is
a 99.7 confidnece interval for the population percentage.

You can go on but even 10 standard deviations may not give you 100 % 
confidence, because for any interval you take, the normal curve always
has some area (if very very small) outside of it.
 


## The role of systematic error due to non random causes and the role of judgement

## Confidence intervals when we observe all the data

## Contents 

This will be conceptually the most difficult part of the course. The main ideas that should be conveyed in this unit are


1. Using probability theory we can derive the sampling distribution of summary statistics from which formulae for confidence intervals can be derived. 
2. Explain what a 95 \% confidence interval means
3. The central limit theorem and the normal distribution
4. The role of systematic error due to non random causes and the role of judgment
5. Explain the idea that confidence intervals can be calculated even when we observe all the data which then represent uncertainty about the parameters of an underlying metaphorical population.


## Outcome

The students should gain a firm understanding of confidence intervals and how they help us in quantifying uncertainty of predictions we make based on our available data. They should see and understand how and why it is sometimes more convenient and parsimonious to have formulae for confidence intervals rather than quantifying the uncertainty from simulation. The intuitive understanding of the limit theorems and when they can be legitimately applied will be important here.