{
  "hash": "7a183f6b2692cc8a368847fcaf3cbd64",
  "result": {
    "markdown": "# Summarizing and communicating lots of data\n\n::: {.callout-note appearance=\"simple\"}\n## Overview\n\n1. In this chapter we will learn how to summarize and communicate lots of data. Most\n   datasets you will be working with in statistics are large and can contain thousands or\n   even millions of observations. To understand these data and their pattern of\n   variation you need tools to summarize them in a meaningful and informative way. In this\n   chapter you will learn the tools to do so.\n\n2. We first discuss a graphical tool to summarize the variation in a single variable by\n   organizing the data in a way that shows us a the distribution of these\n   data. This tool is called a **histogram** and you will encounter it very often across\n   all applied data work and across all of statistics. To get a good understanding how to\n   construct and work with a histogram, we first work with an example you can in principle\n   do without a computer. This is useful to understand the basic principles of constructing\n   and reading a histogram.\n   \n3. You will also learn to summarize large datasets by single numbers that provide information\n   about the central tendency of the values of a variable. The **mean** is such a number, used\n   over and over in statistics. You will learn what a mean is, what it tells you and when\n   it gives a meaningful summary about the center of the distribution of a variable.\n\n4. For a summary of a large dataset by numbers, you need not only an idea of at which values\n   of a variable the data are centered but also how much they are spread out around this value.\n   Such a measure of spread is the **standard deviation**. Like the mean understanding and \n   working with standard deviations is fundamental and will be encountered over and over\n   again throughout the large field of statistics and applied work with data. You need\n   to gain a good understanding of what the standard deviation is and how to compute it.\n\n5. For larger datasets, which will be the usual case in practice, you will need to learn\n   how to summarize and communicate the information about variables with lost of different\n   values using the computer. This means in particular, that you need to understand how\n   to read large datafiles from a given source, how to select and modify particular variables.\n   You will thus learn some powerful tools from the R language that will help you to\n   do so.\n\nWhen you have worked through this unit you will be able to summarize large datasets\ngraphically and numerically and understand what these measures tell you about the\nvariation in the data, how they look like, what are the most typical values of the variables\nand how much they are spread out around these values. In this process you will power up\nyour abilities to read, select and manipulate large data files and single variables using the\nR language. This will enlarge your abilities to work with data by a huge and important step.\n\n:::\n\nWhen we analyze data, we usually have to look at lots of them. As a leading example,\nlet us take the case of anthropometric data. These are data measuring height and weight\nof humans in a population.^[Anthropometry (This is a word with roots from ancient greek. \nIn ancient greek anthropos means 'human', and m√©tron means 'measure'. Anthropometry thus \nrefers to the measurement of the human individual. An early tool of physical \nanthropology, it has been used for identification, for the \npurposes of understanding human physical variation]. These data are usually gathered via\nlarge population survey like the Demographic and Health Survey, abbreviated DHS^[The DHS \nprogram, a US based program on demographics and health in various parts of the \ndeveloping world. The data are used to monitor demographic and health \ndevelopments to decide where policy action might be needed. The DHS website \ncan be found here: https://dhsprogram.com/ ] \n\nWhat's the point of recording the height and\nweight of humans? \nFirst of all, human growth depends on good nutrition and the \navailability of medical services to \neffectively treat illness. Thus human height in a population has for a long time been\na subject of study. The average human height in a population varies with the general \nliving standards. This fact makes data on human height especially interesting\nfor historians who study the history of living conditions. \nBecause humans tend to get taller when\nthey have good living conditions, human height can reveal some information \non living conditions, height\ndata can be an indirect measure of living conditions. This is especially interesting for\nperiods when very little or no data were collected and recorded.\n\nBut also tody information about the distribution of height and weight in a \npopulation enables countries \nto make data-driven decisions and to monitor their progress in improving \nnutritional status and achieving the United Nation's Sustainable Development Goals.^[\nThe Sustainable Development Goals (SDGs) or Global Goals are a collection \nof seventeen interlinked objectives designed to serve as a \"shared blueprint for peace \nand prosperity for people and the planet, now and into the future.\" \nThe goals were set and decided by the \nUnited Nations General Assembly and are monitored through various data collection \nefforts, including the DHS. The short titles of the 17 SDGs are: No poverty (SDG 1), \nZero hunger (SDG 2), Good health and well-being (SDG 3),\nQuality education (SDG 4), Gender equality (SDG 5), \nClean water and sanitation (SDG 6), \nAffordable and clean energy (SDG 7), \nDecent work and economic growth (SDG 8), \nIndustry, innovation and infrastructure (SDG 9), \nReduced inequalities (SDG 10), Sustainable cities and communities (SDG 11), \nResponsible consumption and production (SDG 12), \nClimate action (SDG 13), Life below water (SDG 14), Life on land (SDG 15), \nPeace, justice, and strong institutions (SDG 16), Partnerships for the goals (SDG 17).\nSee also the website: https://sdgs.un.org/goals]\n\nThe measurement of human height and weight are also important in the history of statistics.\nEspecially important concepts describing the statistical relation between two variables,\nwere detected in this context. We will learn about these concepts later, when we study the\nstatistical tool of regression.\n\nWe will use a dataset from the DHS later in this chapter, when we discuss how to\nuse R to deal with large datasets. The number of observations in this case is 2559. This\nis not very big but certainly too big to reasonably handle by hand. With a \ncomputer this becomes realistic. \n\nBut before we do so, lets look at a smaller example of human height data.\n\n## The histogram\n\nTo summarize data, statisticians often use a graph which is called a **histogram**. \nIn this section we will discuss all you have to know about histograms and how to use them.\n\n:::{.callout-caution collapse=\"true\"}\n## Seitwerk: Expand for reading comment.\nReda, I think it would be great that among the activities in the study center we collect\nsome data about the students in the group. I would suggest height, weight. These\nvaribles can be measured and recorded and the students could be asked to store these\ndata in a text file, sow e can read them by R here. We could then\nuse these data here to explain how a histogram is constructed or give it as an exercise.\n:::\n\n### Constructing a histogram\n\nSince real world examples in this exposition come from anthropometry, let us \nlook at a sample of 100 observations from a set of real world data about human adults\nof age 18 and above, where the data record their height in cm.^[These data are collected from\nthe internet. For details you can look at the source at http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights] I have\nstored these values in an R object which I called `heights`. So in this discussion I refer\nto the 100 vaues shown below as `heights`.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(JWL)\ndat <- socr_height_weight\ndat$Height <- dat$Height*2.54\n\nset.seed(1)\nheights <- sample(x = dat$Height, size = 100) # randomly select 100 values without replacement.\nheights\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 171.5912 174.0172 175.5666 173.7980 172.0488 171.9966 168.0614 185.3618\n  [9] 174.4489 174.2083 174.8307 171.6123 175.5515 169.8454 174.2598 177.0208\n [17] 176.0683 178.8391 168.3490 171.3392 176.3523 171.3722 173.0217 164.0692\n [25] 171.0165 180.5622 171.1445 177.4639 167.0146 175.9097 170.9656 173.5110\n [33] 166.4426 176.2078 169.4876 176.6579 179.6563 170.2477 167.9883 176.6037\n [41] 173.6044 167.8685 164.5033 161.3102 176.2323 184.1186 175.9741 178.7940\n [49] 162.6569 175.3654 175.5658 164.5769 167.4618 180.5760 176.4807 167.1875\n [57] 170.7098 179.8085 162.4880 182.7100 173.1352 175.5756 169.5662 166.9201\n [65] 176.7185 173.8010 177.2146 164.5343 173.0048 167.5625 175.7398 174.0607\n [73] 182.0625 169.7786 171.2853 172.5116 173.4802 171.7511 172.1317 172.3667\n [81] 172.1702 169.1945 169.0105 171.7650 173.7279 165.9003 175.6967 174.5598\n [89] 177.1643 175.0406 171.9614 173.3289 169.6971 172.8287 170.3538 175.8446\n [97] 169.3031 170.4622 179.2061 167.5248\n```\n:::\n:::\n\nThis is how R prints out the 100 height values. You already learned what the numbers in\nbrackets on the left side mean. They are counters or observations. In total we have\n100 values.\n\nWe start the construction of a histogram by choosing for the horizontal \naxes ranges of numerical\nvalues - in our case of the height data - which are called *bins* or *classes*. There is no\nfixed rule as to how to choose the size of these ranges. These ranges should neither be too \nfine, nor too\ncoarse. While there are lists of mechanical rules, which you \ncan for example find on Wikipedia^[See https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width], it is \nusually best to use your domain\nknowledge and some experimentation to find out the bin size that works best for your data.\n\nFor this example, assume we had chosen a bin size of 5 cm. When you study \nthe list, you will find that the lowest value is at 160.12 cm  while the highest value\nis at 185.16. This is already quite tedious to find out by \neyeballing the numbers with the small \nnumber of values we have chosen for this example. \nIt is impossible to do for really large data sets.\n\nNow lets make a distribution table like this:\n\n| Height-bin | Frequency |\n|-----------:|----------:|\n| 160 -  165 |          7|\n| 165 -  170 |         20|\n| 170 -  175 |         39|\n| 175 -  180 |         28|\n| 180 -  185 |          5|\n| 185 -  190 |          1|\n\n\nIn the column Height-bin we have recorded the bins in steps of 5 and in the right \ncolumn, Frequency, we have recorded the count of values that are in this bin. \n\nWhen we make such a tabulation we have to agree on an endpoint convention. \nThis is important, since \nwhen a height value would for instance be measured as exactly 165, in which bin \nshould it be counted:\n160-165 or 165-170? You, the constructor of the histogram, has to take this decision. Let us\nagree on the convention that when a value falls exactly at the endpoint of the bin, we\nput it in the next bin. In practice you\nwill usually do a histogram by computer. \nThe code of the computer program has to specify an endpoint\nconvention, so the computer knows what to do when a value coincides with an endpoint.\n\nOn the Frequency axes you put the frequency scale: Counts of values. \nThen for each bin, you plot\na bar, which has the width of the bin and the height of the frequency. \nDo this for all the bins\nyou have tabulated and you are ready.\n\nThe histogram provides a certain aggregation of the data because it sorts \nthe 100 data points into 6\nbins, in our example. While loosing some local information on \nindividual data points the global information\nconveyed by the summary gives us a pretty good idea of the overall \npattern of variation on the human height data.\n\n:::{.callout-caution collapse=\"true\"}\n## Seitwerk: Expand for reading comment.\nReda, I think it would be great if you could do some animation that reproduces the \nsteps: Plot a height axis with bins from 160 to 190 in bin sizes of 5, then plot a frequency\naxes from 0 to 40 in steps of 10. Them from the list of 100 numbers, collect all that are\nrecorded in the first bin and then make the height proportional to the count, then the next\netc. The idea would be to realize something like my previous sketch for the Nile data\n![Constructing the river flow histogram](pictures/nile_hist.png)\n:::\n\nWe can see, for instance, that the most frequent height is between 170 and 175 and that the\nvariation is fairly symmetric around this bin. In the extremes this most frequent \nvalue can half or shrink even more, so there is quite some spread in the data.\n\nIf we had just plotted all individual data points, we would have got a picture like this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(as.numeric(heights), xlab = \"Observation\", ylab = \"Height in cm\", pch = 16)\n```\n\n::: {.cell-output-display}\n![](summarizing_and_communicating_lots_of_data_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nYou will agree that as a summary of the data it is not particularly useful and not\nas informative as the histogram.\n\nHistograms are such a common tool in statistics to explore the variation \nin one variable and the shape, how\nit is roughly distributed that every statistical software has functions \nto produce histograms. \n\nIn R, the\nlanguage we use in this course, there is also such a function. \nThe function name is called `hist()` and it\ntakes the data as an argument. This is the second graphic function \nof R you encounter in this course\nafter we played with the `barplot()`function in the last lecture. \n\nTo produce a histogram from the height data with R, we type at the console\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhist(heights)\n```\n\n::: {.cell-output-display}\n![](summarizing_and_communicating_lots_of_data_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note icon=false}\n\n### Now you try\n\nLet us check your understanding of histograms by a little quiz now. The histogram below\nshows the distribution of the final score in a certain class.\n\n(a) Which block represents the people who scored between 60 and 80?\n(b) Ten percent scored between 20 and 40 about what percentage scored between 40 and 60?\n(c) About what percentage scored over 60?\n\n![Final Score](pictures/final_score.png)\n:::\n\n:::{.callout-caution collapse=\"true\"}\n## Seitwerk: Expand for reading comment.\nReda, I think it would be great if you could redo the graphics with\ncomputer. I have these hand drawn graphs only because I did not have\nenough time to play around with computer graphics and because I\nwanted to experiment how such pictures would look like. I think\na computer graph would be better.\n:::\n\n### The relative frequency scale: Absolute versus relative frequency\n\nSometimes it might be useful, to choose a different scale for the y axes of \nyour histogram. Instead\nof absolute frequencies (or counts) it might be useful to show relative \nfrequencies, the proportion\nof occurrences in each bin. The type of scale you choose will depend on what kind of \ncomparisons you want to emphasize about your data.\n\nAssume you would want to draw your histogram such that on the y-axis you do not\nsee the absolute counts of how many individuals in your data fell into a certain\nhight bin but you want to see instead these numbers as a percentage of the whole\ndataset. \n\nThis is sometimes convenient and in some cases more informative. This choice of\na different scale is called the **relative-frequency-scale**. The construction\nof the histogram follows the same principles as we have already discussed but now\nwe just quantify the counts in the bins differently as shares of all observations.\n\nLet's do the histogram for the heights data in a relative frequency scale. In\nour table, which we used to construct the histogram before we had in total\n100 observations. A count of 7 is therefore 7 % or 0.07 in the relative frequency\nscale etc. Thus our table would now be:\n\n| Height-bin | Frequency |\n|-----------:|----------:|\n| 160 -  165 |       0.07|\n| 165 -  170 |       0.20|\n| 170 -  175 |       0.39|\n| 175 -  180 |       0.28|\n| 180 -  185 |       0.05|\n| 185 -  190 |       0.01|\n\nand the histogram in relative frequency scale would look like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nh<-hist(heights, plot=F)\nh$counts <- h$counts / sum(h$counts)\nplot(h, freq=TRUE, ylab=\"Relative Frequency\")\naxis(2, at = c(0,0.1,0.2,0.3,0.4), labels = c(0,0.1,0.2,0.3,0.4))\n```\n\n::: {.cell-output-display}\n![](summarizing_and_communicating_lots_of_data_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nUnfortunately R has no standard argument to its `hist()`function to draw a\nhistogram with a relative frequency scale. To produce a histogram like shown\nhere we would need some more advanced coding than we know at the moment.^[The `hist()` function in R has an argument, called `freq`. It can take the value `TRUE` or `FALSE`. If the\nargument is set to `FALSE` the histogram is shown such that the y axis has yet another\nscale, the so called density scale. This scale is chosen such that the total area of\nthe histogram is 1.] I will come back to explain how to do a histogram with relative\nfrequency scale in R as soon as we have learned the appropriate syntax to implement this.\n\n### Best practices for histograms\n\nWhen you summarize lots of data by a histogram there are some things you should \nconsider carefully. Let us go through the most important best practice \nprinciples for histograms.\n\n#### Bin size\n\nWhen doing exploratory data work it is usually a good idea not to look at a single \nhistogram but at several histograms of the same data by changing the bin size. There\nis no clear rule about the optimal bin size. It often depends on context and field \nknowledge. \n\nIf the bins are to fine, then the data will be be very noisy and give no overview because\nthey show too many individual points. On the other hand if the bins are too wide, they\nwill not show you the overall variability in the data very well and you fail to\nget a good idea about the distribution. \n\nLet us illustrate this point using our heights data. \n\nIn the first case we have chosen 100 bins, which is too fine. There is almost one bar\nfor every single data point. In this way we have a lot of spurious peaks and throughs and can\nnot see the variation pattern in the data very clearly\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(heights, breaks = seq(min(heights), max(heights), length.out = 100))\n```\n\n::: {.cell-output-display}\n![](summarizing_and_communicating_lots_of_data_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\nNow here we have the other extreme, lets assume we have only 3 bins. This would give us\na pattern like this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(heights, breaks = seq(min(heights), max(heights), length.out = 3))\n```\n\n::: {.cell-output-display}\n![](summarizing_and_communicating_lots_of_data_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nHere the histogram is too coarse and we do not see the variation pattern either. \n\nThe computer usually has a built in rule of thumb for the histogram \nwhich will work well in most\nof the cases. Still for individual datasets it is sometimes better \nto choose a different bin size\nthat more adequately mirrors the variation in the data.\n\n#### Choose boundaries that can be clearly interpreted\n\nTick marks and labels should fall on the bin boundaries. As in the examples discussed\nso far, they need not be there for every tick but it is enough if they are there between\nevery few bars. Bin labels should also have not many significant digits, so they are easy to\nread. So bin sized which divide 10 and 20 evenly are easier to read than bin sizes that do not.\nSo always take caution not to arbitrarily split bin sizes. Otherwise you can end up with\noff bin boundaries.\n\nFor example, if we just took the maximum and the minimum of the heights data and\narbitrarily divided them into 7 bins, we would get the difficult to read bin boundaries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseq(min(heights), max(heights), length.out = 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 161.3102 165.3188 169.3274 173.3360 177.3446 181.3532 185.3618\n```\n:::\n:::\n\n\ninstead of the more easily readable boundaries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseq(160,190,5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 160 165 170 175 180 185 190\n```\n:::\n:::\n\n\n#### What's the difference between a histogram and a bar chart?\n\nA histogram depicts the frequency distribution of a continuous, quantitative variable, such \nas height, weight, time, energy consumption etc. These are variables that can take on any\nvalue and these values can be ordered from smallest to highest.\n\nWhen we have a categorical variable, like we encountered them in section 2, \nwe need to use a bar chart.\nThe bars of the bar chart typically will have a small gap between the bars, emphasizing \nthe discrete\nnature of the variable. The categories in a bar chart usually have no natural ordering. \nAs we discussed\nin section 2, we have even to be conscious how we display the categories to \navoid suggesting an\norder that is in fact not there in the data.\n\n## The average and the standard deviation\n\nWith a histogram we can summarize a large amount of data and get some insights about the\nvariation in the data. Often we can summarize data much more drastically by just one number\ndescribing the center of the histogram and the spread around this center. When I write center\nan spread here, these are just ordinary words with no special technical meaning.\n\nWhen we do statistics we need \nprecise definitions and we will study and learn these definitions in this section. \nThe **average** is\noften used to find the center. Another measure to find \nthe center is the **median**. The **standard** deviation measures spread around \nthe average. The **interquartile range** is another measure of \nspreads.\n\nBefore we go into these definitions, let me show for a start \ntwo histograms, both have the same center, but the second one is more spread\nout, there are more observations farther away from the center.\n\n\n\n\n\n::: {#fig-example-hist layout-ncol=2}\n\n![Histogram 1](pictures/example_histogram1.png){#fig-ex-hist-1}\n\n\n![Histogram 2](pictures/example_histogram2.png){#fig-ex-hist-2}\n\nHistogram 1 and Histogram 2 have the same center but Histogram 2 is more spread out\n:::\n\nThese distributions can be summarized by the center and the spread. But what about a situation like this?\n\n\n\n\n\n![A bimodal distribution](pictures/bimodal.png){#fig-ex-hist-bm}\n\nIn some cases such distribution can occur naturally. \nThink, for example, about the distribution of the elevation data of surface area of the earth.\nMost of the surface area of the earth is taken by the sea floor at about three \nmiles below the sea level \nor the continental planes around sea level. If we would report only the center an the spread\nof this histogram, plotted in the picture as the red and blue vertical lines, we would miss these peaks.\n\n### The average\n\nLet us come back to the height data, we have analyzed in a histogram before as the\ncontext for which we study the concept of an average.\n\n::: {.callout-note icon=false}\n\n## Average\n\nThe average of a list of numbers is defined as their sum, divided by how\nmany numbers their are in the list.\n\n:::\n\nFor example the average of the list $L$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nL <- c(9,1,2,2,0)\n```\n:::\n\n\nwould be computed as:\n\n\\begin{equation}\n\\frac{(9+1+2+2+0)}{5} = \\frac{14}{5} = 2.8\n\\end{equation}\n\nBy now it will be no surprise for you that R provides a function for computing averages.\nThis function is known by the alternative name for the average, called the mean.\n\nHere is how you would compute the mean using R.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.8\n```\n:::\n:::\n\n\nwhich is indeed what we should get as a result.\n\nLet's go back to the issue of human height data and take our data set of\n100 observations of the height of adult humans. Let us recall\nour histogram and give it some nice title and a precise label for the\nx-axis:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhist(heights, xlab = \"Height in cm\", main = \"Height of 18 year old humans\")\n```\n\n::: {.cell-output-display}\n![](summarizing_and_communicating_lots_of_data_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThe distribution looks symmetric. If we summarize these data by taking an average in one number\nwe will capture the center of the distribution fairly well. \nLet us compute the average, using the\nmean function. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\naverage_height <- mean(heights)\naverage_height\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 172.8044\n```\n:::\n:::\n\n\nThe average height in this dataset is about 172.8044426  cm. \nThe average is a very powerful way of\ncommunicating data by compressing many observations into one single\nnumber, the mean. \n\nThis compression is, however, only achieved by loosing some \ninformation on individual differences.\nFor example in our dataset the average height is 172.8044426 cm. \n\nBut there are about 6 % who are larger than\n180 cm and there are also about 7 % who are \nsmaller than 165 cm. With 100 individuals these are\nindividuals who are beyond these thresholds. This diversity \nis hidden in the aggregation.\n\n::: {.callout-tip}\n## Tip with Title\n\nThis is a good opportunity to show you a cool feature or trick in R how we could in one line\ncompute such percentages. Assume we want to compute the percentage of individuals in our data\nwho are larger than the mean of 172.8044426. How can we do this?\n\nLogicals are a powerful data type to make such a computation. A logical is\na special data type, similar to the data type numerical, used for representing numbers, or\nthe data type character, representing strings. A logical can \neither be `TRUE` or `FALSE` and will be the output of an R operation testing a condition.\n\nSay we have 10 numbers, given by\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nx <- c(4,5,1,2,4,2,0,10,11,6)\n```\n:::\n\n\nNow we could ask R which entries of x are larger than 2 by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nx > 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n```\n:::\n:::\n\n\nThe output is a vector of logicals where R compares \neach entry in `x`with 2 and if it is larger it \nreturns `TRUE` and otherwise `FALSE` (this includes all entries which are exactly 2). \n\nNote that, when you apply the function `mean()` to a vector of data of \ntype logical R performs a computation by coercing `TRUE`to the number 1 and `FALSE` to the\nnumber 0. We will learn about the details of these R the coercion rules later in more detail.\nIf you apply `mean()` to `x` you will get the proportion of values fulfilling a certain\ncondition. So if you take the mean of\n\n::: {.cell}\n\n```{.r .cell-code}\nx > 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n```\n:::\n:::\n\nR would firts transfrom the vector\n\n`TRUE`,  `TRUE`, `FALSE`, `FALSE`, `TRUE`,  `FALSE`,  `FALSE`,  `TRUE`,  `TRUE`,  `TRUE`\n\ninto a vector\n\n1,1,0,0,1,0,0,1,1,1.\n\nWhen you take the mean\n\\begin{equation}\n\\frac{1+1+0+0+1+0+0+1+1+1}{10}\n\\end{equation}\nyou just get the proportion of 1, i.e. cases where the condition $x > 2$ is `TRUE`, because\nthe 0-es do not contribute to the value of the sum, only the 1-s.\n\nThis insight can be used to compute the percentage of values that \nfulfill a certain condition in R.\nHere the proportions of heights larger and smaller than the mean.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(heights > mean(heights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.52\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(heights < mean(heights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.48\n```\n:::\n:::\n\nHere you see the symmetry of the distribution numerically. About 50 % are larger and also \nsmaller than the mean.\n:::\n\n\n### The standard deviation\n\nWhen summarizing and communicating lots of data, it is useful not only to report \nat which value the center of the distribution is. It is often helpful to also \nthink about the way how the values spread around the average. The quantity which\nmeasures this spread is called the standard deviation. It can be interpreted as \nan average deviation. \n\nLe us go back to our data on human height. There were 100 observations\nin our sample. The average height was 172.8 cm.\n\nThis average tells us that most of the humans measured in the sample had a height of\naround 1 m and 73 cm. But there were deviations from this average. Some humans were\ntaller and others were smaller. We can ask how big these deviations are? In answering \nthis question we need the concept of the standard deviation.\n\n::: {.callout-note icon=false}\n\n## Standard deviation\n\nThe standard deviation says how far away numbers on a list are from their\naverage. Most entries on the list will usually be somewhat around one\nstandard deviation from the average. Very few will be more than two or three\nstandard deviations away.\n\n:::\n\nIn R you can compute the standard deviation by using the function `sd()`. So let us\ncompute the standard deviation of our height data using the `sd()`function.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsd(heights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.726364\n```\n:::\n:::\n\n\nThe standard deviation is at about 4.73 cm. That means \nthat many humans differed from the\naverage height by about 1, 2, 3, 4 or about 5 cm. Let us use R to compute the percent\nof observation that are within one standard deviation from the average.\n\nLet us compute the percentage of observations, which are within 1 standard deviation\nfrom the mean height. Do you remember the trick with using logicals?\n\nLet us compute the percentage of observations which are larger than the average height minus\nund standard deviation and smaller than the average height plus one standard deviation. The\nR symbol for and is `&`. Let us use it to formulate the condition:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(heights >= mean(heights) - sd(heights) & \n     heights <= mean(heights) + sd(heights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.71\n```\n:::\n:::\n\n\nIn our case these are 71 % of all observations.\n\nSometimes it is convenient to subtract the mean from every observation and \nand express the units in terms of standard deviations. By construction this\nis a variable with mean 0 and standard deviation 1. This is called standardization\nor normalization. Let me show you how this works.\n\nWe transform our heights data to \n\\begin{equation*}\nz_i = \\frac{x_i - \\mu}{\\sigma}\n\\end{equation*}\nand compute the mean of $z_i$ and its standard deviation. We can use R to do that:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nz <- (heights - mean(heights))/sd(heights)\n\nmean(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.699057e-15\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nsd(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\nNow you see that the mean is 0 and the standard deviation is 1. \nThe mean in the R computation is not exactly 0. R says it is `2.699057e-15`. What does this\nmean? This is a way of writing 0.000000000000002699057 in a special notation. Do not worry for\nnow how this notation works exactly. What you can see is that this is an very very small \nnumber, practically the same as 0. In R it is not exactly zero because of rounding\nerrors of the computer. For practical purposed the value is as good as zero. \nThis must be the case as a \nconsequence of how we changed the units. The mathematically inclined among you might\ntry to derive this fact more generally using the definition of the mean and the standard\ndeviation.\n\nNow if we ask the same question as before. Which proportions of $z$ are within\n-1 and + 1 standard deviation from the mean, we get\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(z >= mean(z) - sd(z) & \n     z <= mean(z) + sd(z))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.71\n```\n:::\n:::\n\n\nThis is exactly the same value as we got before. Our transformation was just about\nchanging the units in which our data are measured. This change of unit does not change\nthe distribution. So no matter whether we work with $x$ the original heights in cm or\nwith the $z$ the normalized data, the distributional properties do not change. In \nstatistics if data are normalized in this way the normalized values are also often\nreferred to as the **z-score**.\n\n### Computing the standard deviation\n\nUsually we will compute the standard deviation using the \ncomputer and only in rare cases will we\never compute a standard deviation by hand. Still it is important that you understand how the\ncomputation works and what is actually been computed. Here is how.\n\nExample 1: Find the standard deviation of the list 20, 10, 15, 15\n\nStep 1: We first need to find the average, which is\n\\begin{equation}\n\\frac{20 + 10 + 15 + 15}{4} = 15\n\\end{equation}\n\nStep 2: We next need to find the deviation from the average. In order to do so, we just subtract\nthe average from each entry\n\\begin{eqnarray}\n(20-15)&=5\\\\\n(10-15)&=&-5\\\\\n(15-15)&=&0\\\\\n(15-15)&=&0\n\\end{eqnarray}\n\nStep 3: Now we have to square each one of these differences and take the square root, which is \noften called the root mean square\n\n\\begin{eqnarray}\nsd&=&\\sqrt{\\frac{5^2 + (-5)^2 + 0^2 + 0^2}{4}} \\\\\n &=& \\sqrt{\\frac{25 + 25 + 0 + 0}{4}} \\\\\n &=& \\sqrt{\\frac{50}{4}}\\\\\n &=& \\sqrt{12.4}\\\\\n &\\approx& 3.5\n\\end{eqnarray}\n\nThe standard deviation has the same units as the data. \nFor example, when we measure height in cm then at the squaring step the units change to $cm^2$ but the squre root returns $cm$ again. \n\n::: {.callout-note icon=false}\n\n## Now you try\n\n1. Guess which of the following two lists has the larger standard deviation. Check your\nguess by computing the standard deviation for both lists.\n\n(i) 9,9,10,10,12\n(i) 7,8,10,11,11,13\n\n2. Can the standard deviation ever be negative?\n\n3. For a list of positive numbers can the standard deviation ever be larger than the\naverage?\n\n:::\n\n## Using the computer to summarize: Taking the next steps in R {#sec-moreR}\n\nAnthropometric data can help a country to get a data driven picture on the nutritional status\nof its population. These data can help identify areas where policy action is needed. Since\nreal world data sets, which will usually come from surveys, will be too large to be\nhandled manually. For these datasets you need to be able to work with the computer.\n\nWe will now learn these skills using data from the DHS survey. The data are used \nto monitor demographic and health developments to decide where policy action \nmight be needed.\n\nThe data we are using are not from an actual survey but they are from a \nsynthetic model dataset provided by the DHS for teaching and training. \nBut they reflect realistic values and could come form an actual country\nsurvey.^[The DHS website can be found here: https://dhsprogram.com/ and the \nraw model data sets can be found here https://dhsprogram.com/data/model-datasets.cfm]\n\nBefore we go into the details of the data we need a bit of context to understand \nwhat the data we are looking at here are precisely describing. In the \nanalysis we are going to produce a statistical table on the nutritional \nstatus of children under five years.\n\nThe anthropometirc data reveal information about a \npersons nutritional status. The nutritional status of children is observed to \nmonitor and measure malnutrition in a population. The nutritional \ndata of children are included in the United Nations development goals indicators.\n\nThe DHS survey collects data on children's sex, age, height-length, and weight to \nmonitor the nutritional status of children in a population. The nutritional \nindicators are sex and age specific because boys and girls grow at different rates. \nTheir growth rates are also age dependent.\n\nFour indicators on nutritional status are collected in DHS surveys:\n\n(a) **Overweight**: This indicates high weight for height and is a measure of excess weight. \nThis results from an imbalance of energy consumed (too much) and energy \nexpended (too little). Children with this condition have an increased risk of \nnon-communicable diseases, such as high blood pressure and diabetes and is \nassociated with increased risk of being obese and overweight in adulthood.\n\n(b) **Stunting**: This is low height-for-age. It is a measure of growth \nfaltering and may result from a deficient growth environment, recurrent \ninfections, chronic diseases and other causes. It is associated with \nimpaired brain development and reduced academic achievement in childhood \nand lower economic potential as an adult.\n\n(c) **Wasting**: Is low weight-for-height and is a measure of acute weight loss. \nThis may result from inadequate food intake or from illness or infection. With \nthis condition children are more susceptible for disease and have a \nhigher risk of death.\n\n(d) **Underweight**: Is low weight-for-age and is a measure of weight \nrelative to a child's age. It reflects children who are stunted \nor wasted or both.\n\nThe World Health Organisation (WHO) child growth standards provide a single \ninternational standard that describes the physiological growth for all children \nfrom birth to age 5. The measurements of individual children are compared \nto the WHO Child Growth Standards to assess each child's growth. The WHO \nGrowth Standards for children use a distribution that is expressed in units of \nstandard deviations from the mean, also called z-scores. It is a distribution \nwith a mean 0 and standard deviation 1. Within 1 standard deviation are \nabout 67 % of all values, within 2 95 % and within 3 standard deviations 99.7%. \nHeight for age, weight for height, and weight for age of children are expressed \nin these units.\n\nStatistical cutoffs are used to measure malnutrition. If a child's height \nfor age is below minus two standard deviations from the \nmean (expressed in z-scores), the child is considered stunted. If it is \nbelow minus three standard deviations from the mean it is considered severely \nstunted. If the weight-for-height z-score is below minus 2 standard \ndeviations from the mean the child is considered wasted, if it is below \nthree standard deviations from the mean it is considered severely wasted. \nIf the weight for height z-score is above two standard deviations from \nthe mean the child is considered overweight, if three standard deviations \nabove it is considered obese. If the weight for height z-score of a child \nis below two standard deviations from the mean it is considered underweight, if \nthree, severely underweight.\n\nThe WHO uses the following prevalence thresholds to assess the problems of \nstunting and wasting in a population of children:\n\n\n\n|Classification|stunting         |wasting and overweight |\n|:-------------|:----------------|:----------------------|\n|very high     | 30% or more\t   | 15 % or more          |\n|high\t         | 20 - 29 %\t     | 10 - 14 %             |\n|medium\t       | 10 - 19 %\t     |  5 - 9 %              |\n|low\t         |  2.5 - 9 %\t     |  2.5 - 4 %            |\n|very low\t     | Less than 2.5 % | Less than 2.5 %       |\n\n: WHO Prevalence thresholds for height and weight {.striped .hover}\n\nPrevalence is the proportion of a population who have a specific characteristic \nin a given time period. In a survey prevalence is measured by the number of \npeople in a sample who have the characteristic divided by all people in the sample.\n\nBefore we go on with learning the tools to summarize and communicate lots of data let us\ngain more skills for handling the tool that will actually enable you to handle\nlarge data sets yourself by making use of R. Building on what we learned in the last unit, \nlets now push your knowledge of R bit further.\n\n### Reading data in R {#sec-readingdata}\n\nBefore we can do anything with data, we need first to learn how  to load data into R and how\nto save them. We will discuss now how to do this for the case of *comma separated text files* or \nso called *csv* files. R provides functions \nfor reading and writing from almost any other format, like data stored in\nExcel files and many more data formats that are used today.\nSince in all those different formats are read by R following the same principles \nas in the csv case, it is sufficient if we discuss here the the case of csv files only.\n\nWe have discussed a data set of per capita primary energy use in countries around the globe to\nproduce a histogram of these data for the year 2019. How did I get these data into R?\n\nFirst of all I could access these data because helpful people at Oxford University in the UK who maintain\nand run the website \"Our world in data\", which we have encountered before, store these data on their\nwebsite. In the concrete case of the energy data, they can currently be found at\nhttps://ourworldindata.org/grapher/per-capita-energy-use\nwhere you can download the datafile from the webpage and save it locally somewhere on your\ncomputer. \n\nI have taken a screenshot here\n\n![Our World in Data website energy](pictures/our_world_in_data_energy.png)\n\nIn the lower right corner you see a box called Data and a download button. This button allows you to \ndownload the dataset to your machine. The file is called `per-capita-energy-use.csv`. From the extension\nof the file `csv`, you can see that it is a comma separated text file. This is a plain text file following\ncertain formatting rules. In particular individual data points are separated by a comma^[The standard format for csv can be looked up here https://www.ietf.org/rfc/rfc4180.txt. Despite this standardization\nit can occur that different files use different conventions for the notation of the decimal comma sign. In the most common specification this symbols is a dot (.) and in others it is a comma (,). For such speical cases R provides special functions, which we will explain in the text.]\n\nI have stored\nthe file in a sub-folder to the directory in which I am writing these lecture notes. If you decide to\ndownload this file, you will save it somewhere on your machine where you find it appropriate. Perhaps\nyou have a folder for this course and in this folder you have a sub-folder where you store all the\ndata sets we are using in the course.\n\nTo read a csv file, R provides the function `read.csv()`. If the csv file\ncomes with a European instead of an US decimal format (`,` instead of `.` for the\ndecimal sign.) you need to use the function `read.csv2()`. Please check out the documentation\nof these functions by typing `?read.csv` at the R prompt.\n\nIn the simplest form you read the data and store them in an object\nyou can work with in R. How to store data in an R object, we have already learned\nin the previous lecture. You invent a name and assign the values to this name using the \nassignment operator `<-`.\n\nLet's call the\nobject in which we save our data `energy_consumption`, then by calling the function `read.csv()` with the\npath to your file as an argument will read the data from your local folder and store them in the\nobject we have created. This allows us to refer to the data for doing further computations.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nenergy_consumption <- read.csv(\"data/energy_use_per_capita/per-capita-energy-use.csv\")\n```\n:::\n\nThe function needs as an argument the file name. If the file is in a sub-folder\nof  the current directory you need to also specify the path.\nTo specify the correct path to the file\nyou need to know in which part of your directory tree you are currently working.\n\nIn my\ncase I am working in the project folder for my lecture notes, which has a sub-folder called\n`data`. The data sub-folder has a further sub-folder called `energy_use_per_capita` in my case\nand thus I specify the path relative to this location. \n\nTo find out what is your current\nR working directory, R provides the function `getwd()`. If I type this in my case, I will get\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ngetwd()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"/home/martinsummer/R/Statistics_JWL\"\n```\n:::\n:::\n\nthe path of my project folder for this lecture notes. So if I type the string\n`\"data/energy_use_per_capita/per-capita-energy-use.csv\"` this specifies the path relative to my working directory.\n\nIf you read the file on your computer, you need to specify the path appropriately from where\nyou are working in R at the moment to where you have stored the csv file.\n\nNow `read.csv()` has many additional arguments, which provide you with lots of\nflexibility. I encourage you to check it out and play with it using the help function and\nthe examples given therein by typing `?read.csv` at the prompt.\n\nWe have now read the primary energy consumption data and written it to the \nR-object `energy_consumption`. Lets\ninspect the object a bit to see what we've got. \n\nI use the function `head()` with\nthe parameter value `n = 10`. This will show me the first 10 rows of the data-file.\nSo the value I give to the argument `n` contros how many rows will be displayed.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\noptions(width = 120)\nhead(energy_consumption, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Entity Code Year Primary.energy.consumption.per.capita..kWh.person.\n1  Afghanistan  AFG 1980                                           583.2944\n2  Afghanistan  AFG 1981                                           666.3782\n3  Afghanistan  AFG 1982                                           725.6599\n4  Afghanistan  AFG 1983                                           912.1396\n5  Afghanistan  AFG 1984                                           941.3926\n6  Afghanistan  AFG 1985                                           939.6124\n7  Afghanistan  AFG 1986                                           976.6691\n8  Afghanistan  AFG 1987                                          1592.7023\n9  Afghanistan  AFG 1988                                          2805.6096\n10 Afghanistan  AFG 1989                                          2700.4739\n```\n:::\n:::\n\nThis gives you an idea what the data look like. There are four variables, called `Entity`, `Code`, `Year` and `Primary.energy.consumption.per.capita..kWh.person.` The last variable name is very informative\nbut also very long and unpractical. We will learn how to change variable names soon. Because of the\nlong name, I had to use the function `options()` before `heads()` to tell R to use a sufficiently wide\ndisplay. Don't worry for this detail at the moment.\n\n### R objects {#sec-robjects}\n\nThe most basic type of R objects are **atomic vectors**. Objects in R are built\nfrom atomic vectors. \n\nThe energy consumption data-file we have just loaded is an example of such\na more complex structure built from atomic vectors. We have already encountered a few of\nthose in our previous lecture.\n\n#### Atomic vectors {#sec-atomic}\n\nAn atomic vector is just a simple vector of data. For example remember when we typed the\ninfant mortality data for eight European countries for 1860 we typed\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmr_1860 <- c(0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)\n```\n:::\n\nIn this case `mr_1860` is an atomic vector. \n\nR has a function, which allows you to check whether an object is an atomic vector or not.\nThis function is called `is.vector()`. It takes the object name as an argument and returns `TRUE`\nif the object is an atomic vector and `FALSE` if it is not. \n\nFor example:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nis.vector(mr_1860)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\ndoes indeed return `TRUE`.\n\nEach atomic vector stores values in a one-dimensional vector, and each atomic vector can\nonly store one type of data. The length of the atomic vector can be determined by the\nfunction `length()` This function takes an R object, which is an atomic vector, as an\nargument and returns the number of elements in this vector. Here is the example of the die\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlength(mr_1860)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8\n```\n:::\n:::\n\nwhich is 8 as it should be. An atomic vector could also have only one element, in which case\n`lenght()`would return 1.\n\n#### Data types {#sec-datatypes}\n\nNow altogether R has implemented *six basic types of atomic vectors*:\n\n1. double\n\n2. integers\n\n3. characters\n\n4. logical\n\n5. complex\n\n6. raw\n\nWe will not encounter complex and raw data-types in this course, so let us skip those and discuss\nonly the first 4 types, double, integer, character and logical.\n\nIf yo u go back to our energy consumption data and look at the first three lines\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(width = 120)\nhead(energy_consumption, n=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Entity Code Year Primary.energy.consumption.per.capita..kWh.person.\n1 Afghanistan  AFG 1980                                           583.2944\n2 Afghanistan  AFG 1981                                           666.3782\n3 Afghanistan  AFG 1982                                           725.6599\n```\n:::\n:::\n\n\nyou will see the different variables in the object `energy_consumption`. Note that I had to fudge a bit\nwith the display because of the unwieldy and long name of the last variable. We are soon going to fix this.\n\nThe variables `Entity`  and `Code` are both of type *character*.  A character vector stores \nstrings of text, which have to be put between quotation marks `\"\"`. Strings are the \nindividual elements of a character vector.\n\nNote that a string can be more than just letters. If you type, for instance the number `1` with\nquotation marks, like `\"1\"` R would interpret the value as a string not as a number. Sometimes\none can get confused in R because both objects and characters appear as text in R code. Object\nnames are without quotation marks strings always are between quotation marks.\n\nCharacter is the natural data type for country names - here \"Afghanistan\" and the international abbreviation \"AFG\" also called an ISO-country code^[ISO is the short name for International Organization for Standardization. International Organization for Standardization came into existence in the year 1946 in London. This organization was formed after a delegation of 65 members from 25 countries, met to discuss the future of International Standardization. In 1947, ISO was officially formed with 67 technical committees consisting of a group of experts focusing on a specific subject. ISO founders decided to give it an acronym ISO, which was based on the Greek word ‚Äòisos‚Äô, which means ‚Äòequal‚Äô.] \n\nThe variable `Year` encodes the year of a particular record or observation. \nIts type is `integer`, since years are integer values like 1980 or\n2022. If yo want to specify a number explicitly as integer in R you have to type it as, say, `1980L`, the number followed without space by a big `L`.\n\nThe variable with the very long name `Primary.energy.consumption.per.capita..kWh.person.` is of type\ndouble. This is the data type in R for encoding numbers that are decimal fractions, like `12.451`\n\nNow why should we care for distinguishing integers from doubles? This has to do with the\nway a computer does computations. Sometimes a difference in precision can have surprising effects.\n\nIn your computer 64bits of memory^[A bit is a binary digit, the smallest increment of data on a computer. A bit can hold only one of two values: 0 or 1, corresponding to the electrical values of off or on, respectively. So 64 bits are sequences of 0 or 1 with a length of 64] are allocated \nfor each double in an R program. While this\nallows for a very precise representation of numbers not all numbers can be exactly represented\nwith 64-bits. The famous candidates are $\\pi$, which has an infinite sequence of digits and must\ntherefore be rounded by the computer. Usually the rounding error introduced into your\ncomputations will go unnoticed but sometimes surprises can occur. \n\nTake for instance:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsqrt(2)^2 - 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.440892e-16\n```\n:::\n:::\n\nWhy is that? The square root of 2 can not be expresses precisely because, as already\nthe old Greeks\nknew, it is not a rational number.^[Let me invite you to a short digression into the history of science and the history of ideas. The discovery that $\\sqrt{2}$ can not be rational was a shock discovery to the ancient Greeks. The Greek mathematician Pythagoras and his followers were fascinated by and devoted to whole numbers. They detected the fundamental role played by ratios of whole numbers for musical harmony. For example dividing a vibrating string in two half raises the pitch by an octave, dividing the string in three raises the pitch by one fifths and so on. This discovery gave them the clue that the physical world as a whole might have an underlying mathematical structure governed by whole-number patterns. It was thus quite a shock when they\nfound out that one of their foundational discoveries the Pythagorean theorem logically implied that there\nwere ratios of lengths that were incommensurable, that is, not measurable as integer multiples of the\nsame unit. The ratio between such lengths is therefore not a ratio of whole numbers. This is why the Greeks called these numbers irrational. Some of you will remember from school that the Pythagorean theorem says that in a right angled triangle with lengths $a$, $b$ and $c$, where the sides with length $a$ and $b$ from a right angle must fulfill the equation $a^2 + b^2 = c^2$ or expressed in a picture\n![The Pythagorean theorem](pictures/pythagoras.png){#fig-pyth}\nThe incommensurable lengths disovered by one member of the pythagorean school was the side and the diagonal of the unit square. By the Pythagorean theorem in a square with side length of 1 it must be the case that\n$\\text{(lenght of diagonal)}^2 = 1 + 1 = 2$. Thus the length of the diagonal must be\n$\\sqrt{2}$.\n![Length of diagonal in a unit square](pictures/unit_square.png){#fig-usq}\nHence if the diagonal and side are in the ratio $m/n$ (where $m$ and $n$ can be assumed to have no common divisor), we have\n\\begin{equation}\nm^2/n^2 = 2\n\\end{equation}\nthus\n\\begin{equation}\nm^2 = 2 n^2\n\\end{equation}\nThis equation implies that $m^2$ must be an even number. So $m$ must be even too, say $m = 2 p$. But if\n\\begin{equation}\nm = 2 p\n\\end{equation}\nthen we have\n\\begin{equation}\n2 n^2 = m^2 = 4 p^2\n\\end{equation}\nhence\n\\begin{equation}\nn^2 = 2 p^2\n\\end{equation}\nwhich similarly implies that $n$ is even. But we began the chain of deductions \nby the assumption that $m$ and $n$ have no common divisor. So if both $m$ and $n$ were even\nthey would have a common divisor, namely 2. We have arrived at a contradiction. This means the length\nof the diagonal of the unit square can not be a rational number. Legend has it that th first Pythagorean to make the result public was drowned at sea. But even if the Pythagoreans could not accept that $\\sqrt{2}$\nwas a number, no one could deny that it was the length of the diagonal of the unit square.] \n\nAnd you have a small rounding error. Let me explain to those of you who are puzzled by the\nmeaning of this output. R displays the result of its computation in scientific notation. `4.440892e-16`\nmeans $4.44089 \\times 10^{-16}$. \n\nFor those of you who forgot how exponential notation works, let me remind you that we write $10^{-1}$ for $1/10$ and $10^{-2}$ means $1/10^2$ or $1/100$. Thus $10^{-16}$ means\n$1/10.000.000.000.000.000$, a very small number but still different from 0. This is the error introduced\nby rounding $\\sqrt{2}$.\nSuch errors\nare called *floating point errors* in computer science lingo and computing with such numbers is\ncalled *floating-point-arithmetic*.\n\nWith integers floating point errors are avoided, but for many applications this is not an\noption. Luckily for most cases floating-point arithmetic provides sufficient precision\nfor most of the applications we encounter in practice.\n\nThe last data type in the list, we want to discuss here, are: **Logicals**. \nLogical vectors store `TRUE` and `FALSE`; logical values. They\nare extremely useful for doing comparisons and - as we will see shortly - also for\nselecting values from a data set.\n\nHow logical data types work can best be understood by an example. If you type, for instance:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n0 > 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\nR tells you that this statement is false, by printing the logical value `FALSE` \nas an output.\n\nWhenever you type `TRUE` of `FALSE` without quotation marks, R will treat the\ninput as logical data. Note, as an aside, if you typed `\"TRUE\"`and `\"FALSE\"` in quotation marks, R \nwould\ntreat this input as a string co characters, a different data type. When communicating\nwith a computer, you need to be very precise or the machine will not understand what\nyou want to tell it do.\n\nFor instance, the following statement yields:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| code-fold: false\nlogic <- c(TRUE, FALSE, TRUE)\nlogic\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE FALSE  TRUE\n```\n:::\n:::\n\n\n\n#### Attributes {#sec-attributes}\n\nOne important R-fact which you need to know about atomic vectors is that atomic vectors\ncan have **attributes**. Attributes won't affect the values of an object but can hold\nand store object metadata. \n\nNormally we do not look at these metadata, but many R functions\ncheck for attributes and then do special things with the object depending on these\nattributes. Attributes can be checked with the function `attributes()` using an R object as an argument.\nThis will show you all the attributes that are attached to an R object.\n\nThe most common attributes for atomic vectors as well as R objects built from\natomic vectors are **names**, **dimensions** and **classes**.\nEach of these attributes has its own helper function that you can use to also assign attributes to\nthe object. For what we need now we discuss `names` and `dimensions` and discuss `classes`, which is\na more advanced topic later.\n\nLet us look how this works with the `enegrgy_consumption`data and check whether \nthey have a names attribute:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nnames(energy_consumption)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Entity\"                                             \"Code\"                                              \n[3] \"Year\"                                               \"Primary.energy.consumption.per.capita..kWh.person.\"\n```\n:::\n:::\n\n\nWe can now see the variable names. We can use the function `names()`also as a\nhelper function to assign other names to our variables. This is a tool that could\nhelp us to get rid of the very long and\nunwieldy name `\"Primary.energy.consumption.per.capita..kWh.person.\"`. For example:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nnames(energy_consumption) <- c(\"Entity\", \"Code\", \"Year\", \"Cons\")\n```\n:::\n\nwill overwrite the name attribute by this new vector of names. When you now look for the\nnames attribute, you will see\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(energy_consumption)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Entity\" \"Code\"   \"Year\"   \"Cons\"  \n```\n:::\n:::\n\n\nOne very important attribute, we will encounter all the time is **dimension**, with the\nhelper function `dim()`. For example we can look at our data object `energy_consumption` again\nto get:\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(energy_consumption)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10215     4\n```\n:::\n:::\n\nwhich returns two numbers, which mean that the object has 10215 rows and 4 columns.\n\n#### Factors\n\nR stores categorical data, such as nationality, sex etc. by using aspeical data type\ncalled **factors**. If\nyou take for instance, sex, it can have only two values - male or female - and these\nvalues may have their idiosyncratic order, for example that females go always first.\n\nTo make a factor in R you have to pass an atomic vector to the `factor()` function.\nThis function works by recoding the values in the vector as integers and store the\nresults in an integer vector. R also adds a `level` attribute which contains the set\nof labels and their order and a class attribute that says the vector is\na factor. Example:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsex <- factor(c(\"m\", \"f\", \"f\", \"m\"))\ntypeof(sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"integer\"\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nattributes(sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$levels\n[1] \"f\" \"m\"\n\n$class\n[1] \"factor\"\n```\n:::\n:::\n\nFactors can be confusing since they look like characters but behave like integers.\n\nNote that R will often try to convert character strings to factors when you load and create data.\nI recommend that you do not allow R to make factors unless you explicitly ask for it. This\ncan usually be controlled by an argument to whatever the data reader function is. For instance\nyou can give the `read.csv()` function the argument `stringsAsFactors = FALSE`.\n\nR has an internal **coercion** behavior for data types, which you should know\nabout if you work with R. With this knowledge you can do many useful things.\n\nIf a character string is present in an atomic vector, R will automatically convert every\nother component in this vector to a character string. If a vector contains only logicals\nand numbers, R will convert the logicals to numbers. In this case every `TRUE` becomes a 1 and\nevery `FALSE` becomes a 0.\n\nR also uses the coercion rules, when we do math with logicals, like for example\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsum(c(TRUE, TRUE, FALSE, FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\nWhat happens here is that R coerces the vector `c(TRUE, TRUE, FALSE, FALSE)` to the\nvector `c(1, 1, 0, 0)` and sums the components.\n\n### Data frames and R lists {#sec-data_frame}\n\nGoing back to our data set on the enegry consumption data, we see that this data set stores\nvalues of different types, characters, integers and doubles. How does R achieve this?\n\nThe answer is that this is achieved by a data structure called a **list**. List are\nlike atomic vectors, because the group data into a one-dimensional set. However, lists do\nnot group together individual values. List group together R objects, such as atomic\nvectors or even other lists.\n\nFor example, you can create a list, which contains a numeric vector of length 31 in its\nfirst element, a character vector of length 1 in its second element and a new list of length 2 in its third. This is done by using the `list()`function of R, like this:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlist_example <- list(100:130, \"R\", list(TRUE, FALSE))\nlist_example\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n[30] 129 130\n\n[[2]]\n[1] \"R\"\n\n[[3]]\n[[3]][[1]]\n[1] TRUE\n\n[[3]][[2]]\n[1] FALSE\n```\n:::\n:::\n\nThe double bracketed indices tell you which element of the list is being displayed. The\nsingle bracketed indices tell you which sub-element of the list is being displayed. For example\n100 is the first sub element of the first element in the list. \"R\" is the first sub element of the\nsecond list element.\n\nThere is lots to say about lists. But this is an advanced topics. We mentioned it here to\nintroduce one of the most important data structres for our course the R dataframe.\n\n### Data Frames\n\n**Data Frames** are the two dimensional version of a list. They are by far the most\nuseful storage structure for data analysis. Indeed, our dataset on energy\nconsumption data we have loaded before is an instance of a data frame. Data frames group vectors\ntogether in a two dimensional table. As a consequence each variable can have a different type, \ni.e. each column of the\ndata frame can contain a different data type. Within a column, however, we can have only\none data type. The energy consumption data are a typical example of a dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(energy_consumption, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Entity Code Year      Cons\n1  Afghanistan  AFG 1980  583.2944\n2  Afghanistan  AFG 1981  666.3782\n3  Afghanistan  AFG 1982  725.6599\n4  Afghanistan  AFG 1983  912.1396\n5  Afghanistan  AFG 1984  941.3926\n6  Afghanistan  AFG 1985  939.6124\n7  Afghanistan  AFG 1986  976.6691\n8  Afghanistan  AFG 1987 1592.7023\n9  Afghanistan  AFG 1988 2805.6096\n10 Afghanistan  AFG 1989 2700.4739\n```\n:::\n:::\n\n\nEvery column in this tabular array of data can be considered as a vector. \n\n\n### Selecting data from R objects: A toolbox.\n\nAsking questions about a dataframe and in particular about our energy data requires that \nwe are able to adress particular values in the dataframe. We now learn the most \nimportant techniques to do so.\n\nR has a notation system to address individual values. You write the object name first, followed by a \npair of had brackets `[]`. Between the brackets goes a `,` separating row and column indices. The\nnotation is thus like `energy_consumption[,]`.\n\nWhen it comes to writing the indices you have six different ways to do this, all of them very simple. You can use:\n\n1. Positive integers\n2. Negative integers\n3. Zero\n4. Blank spaces\n5. Logical values\n6. Names\n\nThe simplest are positive integers. When you want to extract the energy consumption in kwh per person \nin one year - the value of the variable Cons - you would adress for instance the 3rd value in the\nCons column, which is in our case the 4th column as\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nenergy_consumption[3,4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 725.6599\n```\n:::\n:::\n\n\nYou can - of course extract more than one value. If you write for instance\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nenergy_consumption[1:5,4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 583.2944 666.3782 725.6599 912.1396 941.3926\n```\n:::\n:::\n\nyou will get the first 5 values of the consumption numbers in the dataframe. The colon\noperator `:` used here is a very useful R operator. It creates sequences of whole\nnumbers. Thus if you create a vector with the name, say `n10` containing the sequence\nof the first 10 whole numbers, you would write\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nn10 <- 1:10\n```\n:::\n\n\nNow clearly the indexing rules work in the same way as with dataframes, only that now you have \nonly 1 dimension. Say you want to extract the first three numbers from `n10` you would write\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nn10[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 3\n```\n:::\n:::\n\n\nNote that R‚Äôs notation system is not limited to data frames. The same syntax can be used to select values from any R object, provided you supply an index for each dimension of the object. Two things have to be kept in mind. In R indexing begins at 1. In some other programming languages indexing begins at 0. The indexing convention in R is just like in linear algebra. The second thing to note is that if you select two or more columns from a data frame, R will return a new data frame, like in\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nenergy_consumption[1:5, 3:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Year     Cons\n1 1980 583.2944\n2 1981 666.3782\n3 1982 725.6599\n4 1983 912.1396\n5 1984 941.3926\n```\n:::\n:::\n\n\n R will always return a dataframe. \n \nHowever, if you select a single column, R will return a vector:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nenergy_consumption[1:5, 4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 583.2944 666.3782 725.6599 912.1396 941.3926\n```\n:::\n:::\n\n\nif you prefer to get returned a data frame in this case, you have to add the argument drop = FALSE, like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nenergy_consumption[1:5, 4, drop = FALSE]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Cons\n1 583.2944\n2 666.3782\n3 725.6599\n4 912.1396\n5 941.3926\n```\n:::\n:::\n\n\n\n**Negative integers** work exactly opposite to positive integers. If you type:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhead(energy_consumption[-1,4], n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  666.3782  725.6599  912.1396  941.3926  939.6124  976.6691 1592.7023 2805.6096 2700.4739 2557.5864\n```\n:::\n:::\n\nR will return the fourth column of the data frame *except* the first row. We just display the\nfirst 10 values using the `head()` function.\n\nIf you try to pair a negative and a positive integer in an index, R will return an error.\nHowever, you can use both negative and positive integers if you use them in *different*\nindices.\n\n**Zero** is neither positive nor negative, If you use 0 as an index, R will return nothing\nfrom a dimension with index 0. The following syntax for instance creates an empty object\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nenergy_consumption[0,0]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndata frame with 0 columns and 0 rows\n```\n:::\n:::\n\n\n**Blank spaces** are used if you want to ask R to select *every* value in a dimension. So for\ninstance, if you type:\n\n::: {.cell}\n\n```{.r .cell-code}\n#|code-fold: false\nsel <- energy_consumption[ , 4]\n```\n:::\n\nR will select the entire column of energy consumption. You can check that the length of this\nvector is\n\n::: {.cell}\n\n```{.r .cell-code}\n#|code-fold: false\nlength(sel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10215\n```\n:::\n:::\n\nas expected.\n\n**Logical Values** can also be used for subsetting. If you type for instance\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nenergy_consumption[1, c(F,F,F,T)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 583.2944\n```\n:::\n:::\n\nR will select the energy consumption value of the first\nobservation. Note that here we used the R convention that `TRUE`and `T`\nas well as `FALSE`and `F` have an equivalent meaning.\n\nFinally, you can ask for the elements, you want by name. On our case, you could select the first\nenergy_consumption value by the syntax \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nenergy_consumption[1, \"Cons\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 583.2944\n```\n:::\n:::\n\n\nFinally, note that two types of object in R obey an optional second system of notation. You can\nextract values from data frames and lists with th `$`syntax. \n\nIt works as follows: For example\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntest <- energy_consumption$Cons\ntest[1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 583.2944 666.3782 725.6599 912.1396\n```\n:::\n:::\n\nwould select the column of energy consumption numbers in our dataframe, save them in an object\nnames `test`and then subsequently extract the first four values from the object.\n\n### Application: Reproduce our histogram of primary per capita energy consumption around the globe in 2019 by a worked example.\n\nThis was a lot of abstract and dry instruction about some data extraction tools. It will\nrequire lost of practice and concrete examples before you get some natural acquaintance with\nthese techniques which belong to the everday routines of data analysis.\n\nTo see the concept in action, let me take you step by step through an example by showing you how \nyou can produce a histogram, like we discussed in the first section of this chapter \nstarting from the raw data.\n\nWe have already read the data from  the csv file and stored it in an object we have called\n`energy_consumption`. We would like to plot a histogram of the annual per capita energy consumption\nin different countries around the globe for the year 2019.\n\nLet us look again at the first 10 observations:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhead(energy_consumption, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Entity Code Year      Cons\n1  Afghanistan  AFG 1980  583.2944\n2  Afghanistan  AFG 1981  666.3782\n3  Afghanistan  AFG 1982  725.6599\n4  Afghanistan  AFG 1983  912.1396\n5  Afghanistan  AFG 1984  941.3926\n6  Afghanistan  AFG 1985  939.6124\n7  Afghanistan  AFG 1986  976.6691\n8  Afghanistan  AFG 1987 1592.7023\n9  Afghanistan  AFG 1988 2805.6096\n10 Afghanistan  AFG 1989 2700.4739\n```\n:::\n:::\n\n\nHow would we extract the observations that refer to the year 2019 only? Here is the\nfirst practical use case of logical data types, which will immediately show you \ntheir power. Let me suggest the following code and then explain step by step what it does.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndat <- energy_consumption[energy_consumption$Year == 2019, ]\nhead(dat, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Entity Code Year       Cons\n40          Afghanistan  AFG 2019   945.6454\n95               Africa      2019  4245.7188\n137             Albania  ALB 2019 11266.2578\n192             Algeria  DZA 2019 16140.1963\n234      American Samoa  ASM 2019 26023.6699\n274              Angola  AGO 2019  3429.5928\n314 Antigua and Barbuda  ATG 2019 31384.8633\n369           Argentina  ARG 2019 20785.9141\n399             Armenia  ARM 2019 15538.2012\n433               Aruba  ABW 2019 51178.5859\n```\n:::\n:::\n\n\nLet me explain what is going on here: We have used the indexing rules and logical\nsubsetting. In the first index we have written a logical condition, namely \n`energy_consumption$Year == 2019`. This means that R selects all the rows for which the year variable of\nenergy_consumption is equal to 2019. The logical sign for identical in R is `==`. So what R does then\nis checking for each value of the Year variable whether it is identical to 2019 - in which case the\nlogical comparison results in `TRUE` or not, in which case the result would be `FALSE`. Then\nby logical subsetting all rows for which the comparison vector is `TRUE` will be selected. Note that\nwe had to tell R that it needs to compare the Year variable. Year is selected in a data frame by\n`energy_consumption$Year`. We have then stored all the data in a vector I have called dat and then\nlooked at the first 10 rows. You see that only the observations where the Year is 2019 have been\nkept, as we had intended.\n\nNow we see from the output that the `Entity` variable apparently not only contains individual countries\nbut also whole regions or continents, like `Africa` in our example. It seems that the regions have\nno ISO code but instead an empty character.\n\nLet's try to apply the logical subsetting logic and our knowledge of character types to filter out\nEntities which have no value in the `Code`variable. One way to achieve this would be, for\nexample the code:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndat_countries <- dat[dat$Code!=\"\", ]\nhead(dat_countries, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Entity Code Year       Cons\n40          Afghanistan  AFG 2019   945.6454\n137             Albania  ALB 2019 11266.2578\n192             Algeria  DZA 2019 16140.1963\n234      American Samoa  ASM 2019 26023.6699\n274              Angola  AGO 2019  3429.5928\n314 Antigua and Barbuda  ATG 2019 31384.8633\n369           Argentina  ARG 2019 20785.9141\n399             Armenia  ARM 2019 15538.2012\n433               Aruba  ABW 2019 51178.5859\n545           Australia  AUS 2019 66614.8828\n```\n:::\n:::\n\nNow Africa is out. Of course this is no proof that I have now only entities with ISO-codes left but\nit is an indication. We can check this. But let me first expalin what is going on here.\n\nI now work with the object `dat`, the object we have created before by filtering for the observations\nin which the `Year`variable is equal to 2019. Now we tell R compare the entries in the `Code`variable, i.e.\nin `dat$code` with the condition that they are not equal to an empty string. The symbol for `not identical`\nin R is `!=`. We know that the variable `Code` is of type character hence all codes are strings within\nquotation marks. An empty string is thus written as `\" \"`.\n\nHow could we check - by the way that the `Code` variable does not contain any empty string anymore?\nHere is how:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsum(dat_countries$Code == \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nHere I have used two rules we have dicussed before, logical data types and coercion. First we ask\nR to check whether any Code variable in `dat_counties`still has any empty string. This will create\na vector of logicals like (showing only  the first five entries)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhead(dat_countries$Code == \"\", n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE FALSE FALSE FALSE FALSE\n```\n:::\n:::\n\n\nNow I use coercion rules by summing over this vector. The function `sum()` in R is a function for summation. For example, if I have the vector `c(1,2,3,4,5,6,7,8,9,10)` and type\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsum(c(1,2,3,4,5,6,7,8,9,10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 55\n```\n:::\n:::\n\n R will add up the numbers.\n \nNow if I have a vector of logicals and apply a numerical function to it, R will by its coercion\nrules, force `TRUE` into 1 and `FALSE` into 0. The result that `sum(dat_countries$Code == \"\")` equals\n0 means that the vector for our logical test contained `FALSE` only. So indeed there is no empty \n`Code` variable anymore.\n\nNow we are ready to do a histogram of the per capita primary annual energy consumption by\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhist(dat_countries$Cons, \n     xlab = \"Primary energy consumption in kilowatt-hours per person per year.\",\n     main = \"Primary energy Consuption per Capita 2019\")\n```\n\n::: {.cell-output-display}\n![](summarizing_and_communicating_lots_of_data_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\nI have here used the `xlab`and `main`arguments with a character value to have a nicer description\nof the histogram. Otherwise the x axis would have been labeled `dat_countries$Cons` and the title\nwould have been `Histogram of dat_countries$Cons`. Note that we have here a histogram in the absolute frequency scale.\n\n\n## Exercises: Histograms\n\n### Exercises: Now you try\n\n1. A histogram of monthly wages for part-time employees is shown below (relative frequencies\nare marked in\nparenthesis). Nobody earned more than $1000 a month. The block over the class interval from 200\nto 300 is missing. How tall must it be?\n\n![Wages](pictures/wages.png)\n2. Three people plot histograms for the weights of subjects in a study, using the relative\nfrequency scale. Only one is right. Which one and why?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| echo = false\n\nlibrary(JWL)\n\ndat <- height_weight\n\ndata <- dat[dat$state == 1 & dat$sex == 1 & dat$age > 18, ]\n\nhist_inf <- hist(data$height, plot = FALSE)         # Store output of hist function\nhist_inf$density <- hist_inf$counts /    # Compute density values\n  sum(hist_inf$counts) * 100\n\npng(file=\"pictures/hight_version1.png\")\nplot(hist_inf, freq = FALSE, xlab = \" \", ylab = \" \", main = \" \")\n\n\npng(file=\"pictures/hight_version2.png\")\nplot(hist_inf, freq = FALSE, xlab = \"hight (cm) \", ylab = \"Percent per 5 cm \", main = \" \")\n\n\npng(file=\"pictures/hight_version3.png\")\nplot(hist_inf, freq = FALSE, xlab = \"hight (cm) \", ylab = \"5 cm per percent\", main = \" \")\n```\n:::\n\n\n::: {#fig-height_hist layout-ncol=3}\n![Version 1](pictures/hight_version1.png){#fig-hight-hist-version1}\n\n![Version 2](pictures/hight_version2.png){#fig-hight-hist-version2}\n\n![Version 3](pictures/hight_version3.png){#fig-hight-hist-version3}\n\nThree versions of a hight histogram of males over age 18\n:::\n\n3. An investigator draws a histogram for some height data, using the metric system. She is working in\ncentimeters (cm). She wants to draw the histogram in a so called density scale, i.e. in a scale that the area of all the bars sum to 1. The vertical axes shows relative frequency\nand the top of the vertical axes is 10 percent per cm. Now she wants to convert to millimeter (mm). There are 10 millimeter to the centimeter. On the \nhorizontal axis, she has to change 175 cm to ? mm, 200 cam to ? mm. On the vertical axis she has to change 10 percent per cm to ? percent per mm, and 5 percent per cm to ? percent per mm.\n\n4. In a Public Health Service study, a histogram was plotted showing the number of \ncigarettes per day smoked by each subject (male current smokers), as shown in the \nhistogram below. The density is marked in\nparentheses. The class interval include the right endpoint, not the left.\n\n   (a) The percentage who smoked 10 cigarettes or less per day is around:\n\n        1.5%    15%   30%   50%\n\n   (b) The percentage who smoked more than a pack a day, but not more than 2 packs, is around\n\n        1.5%    15 %    30%   50%\n        (There are 20 cigarettes in a pack)\n\n   (c) The percentage who smoked more than a pack a day is around\n\n        1.5%,   15%,   30%,   50%\n\n   (d) The percent who smoked more than 3 packs a day is around\n\n        0.25 of 1%,    0.5 of 1%,     10 %\n\n   (e) The percent who smoked 15 cigarettes per day is around\n\n        0.35 of 1%,    0.5 of 1%,   1.5%,    3.5%,    10%\n        \n![Number of cigarettes](pictures/cigarettes.png){#fig-hight-cigarettes}\n\n### Exercises for the Average\n\nExercise 1:\n\n(a) The number 3 and 5 are marked by crosses on  the horizontal line below. Find the average of \n    these two numbers and mark it by an arrow.\n    \n    ![](pictures/average_a.png)\n    \n(b) Repeat (a) for the list 3,5,5\n\n    ![](pictures/average_b.png)\n    \n(c) Two numbers are shown below by crosses on a horizontal axis. Draw an arrow\n    pointing to their average.\n    \n    ![](pictures/average_c.png)\n    \nExercise 2: \n\nA list has 10 entries. Each entry is either 1, 2 or 3. What must the list be of the average is\n1? if the average is 3? Can the average be 4?\n\nExercise 3:\n\nWhich of the following lists has a bigger average? Or are they the same? try \nto answer without doing arithmetic.\n\n(i) 10, 7, 8, 3, 5, 9   (ii) 10, 7, 8, 3, 5, 9, 11\n\nExercise 4:\n\nTen people in a room have an average height of 1.69 m. An 11th person is 1.96 enters\nthe room. Find the average height of all 11 people.\n\nExercise 5:\n\nTwenty-one people in a room have an average height of 1.68. A 22nd person who is 1.96\nenters the room. Find the average height of all 22 people. Compare with exercise 4.\n\nExercise 6:\n\nTwenty-one people in a room have a height of 1.68. A 22nd person enters the room. \nHow tall would he have to be to raise the average height by 1 inch.\n\nExercise 7:\n\nIf you go back to figure @fig-ex-hist-bm, where in the Histogram would be mountains?\nWhere would you find planes? Where would the trenches in the sea floor show?\n\nExercise 8: \n\nDiastolic blood pressure is considered a better indicator of heart trouble than systolic\npressure. The figure below shows age-specific average diastolic blood pressure for\nmen age 20 and over in a health survey from the US (HANES5 (2003-04)). True or fales: The\ndata show that as men age, their diastolic blood pressure increases until age 45\nor so. and then decreases. If false, how do you explain the pattern in  the graph?\n(Blood pressure is measured in \"mm\" that is millimeter of mercury)\n\n![](pictures/blood_pressure.png)\n\n\nExercise 9:\n\nAverage hourly earnings in the US are computed each month by the Bureau fo Labor statistics using \npayroll data from commercial establishments. The Bureau figures the total \nwages paid out to non-supervisory personnel, and divides\nby the total hours worked. During recessions, average hourly earnings typically go up. \nWhen the recession ends, average hourly earnings often start going down. How can this be?\n\n### Exercises for the standard deviation\n\nExercise 1:\n\nThe `socr_height_weight` about the height and weight of 18 year old humans, we had used in the\nlecture before the average height in inches was about 173 cm and the standard deviation was about\n5 cm.\n\n(i) One individual was 188 cm. He was above average by how many standard deviations?\n(ii) Another individual was 174.66 cm. She was above average by how many standard deviations?\n(iii) A third individual was 1.5 standard deviations below the average height. He was how many cm?\n(iv) If an individual was within 2.25 standard deviations of average height, the shortest height for this\nindividual would be how many cm? The highest height would be how many cm?\n\nExercise 2:\n\n(a) Here are the heights of 4 individuals. 150 cm, 130 cm, 180 cm, 172 cm. Match \nthe heights with the description. A description may be used twice.\n\nunusually short, about average, unusually tall\n\n(b) About what percentage of individuals in the data had heights between 170.2 and 173.2 ? Between\n    165.5 and 179.2?\n    \nExercise 3: \n\nEach of the following lists has an average of 50. For which one is the spread of\nthe numbers around the average biggest? Smallest?\n\n(i)     0, 20, 40, 50, 60, 80, 100\n(ii)    0, 48, 49, 50, 51, 52, 100\n(iii)   0,1,2,50,98, 99, 100\n\nExercise 4:\n\nEach of the following lists has an average of 50. For each one, guess whether the standard deviation is around 1, 2, or 10. (This does not require any arithmetic.)\n\n(a)   49, 51, 49, 51, 49, 51, 49, 51, 49, 51\n(b)   48, 52, 48, 52, 48, 52, 48, 52, 48, 52\n(c)   48, 51, 49, 52, 47, 52, 46, 51, 53, 51\n(d)   54, 49, 46, 49, 51, 53, 50, 50, 49, 49\n(e)   60, 36, 31, 50, 48, 50, 54, 56, 62, 53\n\nExercise 5:\n\nBelow are three sketches of three stylized histograms \n(we call them \"sketches\" because they display a schematic\nshape of a hypothetical histogram. This is why these sketches do not look like real histograms.) \nMatch the sketch with the description. Some descriptions will be left over. \nGive your reasoning in each case. The\nsymbol $\\approx$ is the mathematical notation for *approximately*.\n\n(i)   ave $\\approx$ 3.5, sd $\\approx$ 1       (iv) ave $\\approx$ 2.5, sd $\\approx$ 1\n(ii)  ave $\\approx$ 3.5, sd $\\approx$ 0.5     (v)  ave $\\approx$ 2.5, sd $\\approx$ 0.5\n(iii) ave $\\approx$ 3.4, sd $\\approx$ 2       (vi) ave $\\approx$ 4.5, sd $\\approx$ 0.5 \n\n::: {#fig-histograms layout-ncol=3}\n\n![Histogram 1](pictures/hist_ex_1.png){#fig-hist_ex_1}\n\n![Histogram 2](pictures/hist_ex_2.png){#fig-hist_ex_2}\n\n![Histogram 3](pictures/hist_ex_3.png){#fig-hist_ex_3}\n\nThree stylized histograms\n:::\n\nExercise 6:\n\nOne investigator takes a sample of 100 men age 18 - 24 in a certain town. Another one takes a sample\nof 1000 such men.\n\n(a)   Which investigator will get a bigger average for the heights of the men in his sample? Or should the average be about the same?\n\n(b) Which investigator will get a bigger standard deviation for the heights of the men in his sample? or should the standard deviation be about the same for both investigators?\n\n(c) Which investigator is likely to get the tallest of the sample men? Or are the chances about the same\nfor both investigators?\n\n(d) Which investigator is likely to get the shortest of the sample men? Or are the chances about the same\nfor both investigators?\n\n\n\n## Computer exercises\n\nOpen the notebook `exercises_summarizing_and_communicating_lots_of_data.ipynb` \nand work on the exercises. Write your solutions into the notebook. You will\nneed the computer to do this work.\n\n\n\n<!-- ## Contents  -->\n\n<!-- Statistics usually involves lots of data and we need ways to communicate and summarize these data. This -->\n<!-- chapter introduces the most important concepts. -->\n\n\n<!-- 1. The empirical distribution of data points -->\n<!-- 2. Measures of location and spread. -->\n<!-- 3. Skewed data distributions are common and some summary statistics are very sensitive to outlying values.  -->\n<!-- 4. Summaries always hide some detail.  -->\n<!-- 5. How to summarize sets of numbers graphically (histograms and box plots) -->\n<!-- 6. Useful transformations to reveal patterns -->\n<!-- 7. Looking at pairs of numbers, scatter plots, time series as line graphs.  -->\n<!-- 8. The primary aim in data exploration is to get an idea of the overall variation. -->\n\n<!-- ## Next steps in R: -->\n\n<!-- - vectors and indices -->\n<!-- - extracting subsets from vectors -->\n<!-- - creating vectors using c() -->\n<!-- - subsetting vectors wit logicals -->\n<!-- - data frames -->\n<!-- - factor class -->\n<!-- - extracting data from data frames -->\n<!-- - tapply -->\n\n\n\n<!-- ## Outcome  -->\n\n<!-- Understand these concepts and work through may examples showing how to apply these summary measures to data on the -->\n<!-- computer. -->",
    "supporting": [
      "summarizing_and_communicating_lots_of_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}