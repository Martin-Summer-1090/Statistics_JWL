# What causes what?

<!-- ## Contents -->

<!-- In this lecture we discuss what causation means in a statistical sense, why we need be careful to distinguish between causation and correlation. -->

<!-- 1. Causation in a statistical sense means that when we intervene, the chances of different outcomes vary systematically. -->
<!-- 2. Causation is difficult to establish statistically, but well designed randomized trials are the best available framework. -->
<!-- 3.Principles that helped clinical trials to identify effects. -->
<!-- 4, Observational data may have background factors influencing the apparent relationships between exposure and outcome which may be either observed confounders or lurking factors. -->
<!-- 5. Statistical methods do not suspend judgment which is always required for the confidence with which causation can be claimed. -->

<!-- ## Outcome -->

<!-- This is a conceptually difficult topic. It is, however, not technically difficult but it will need lots of examples. Fortunately there are many good (and bad) real world examples that I hope will stick with the students as reference examples after the course. Students should understand the idea of randomized trials and observational studies as well as the general ideas of comparison in statistical analysis. -->

The last two centuries brought enormous advances in the expansion of education around the
world. Quantitative measures such as literacy rates, school enrollment, the share of 
education spending in GDP all point into the same direction: While access to education
had been the privilege of a very small elite throughout history, things have changed
in the direction of a very broad participation. 

This is a result of both economic
necessities of modern production as well as of political struggles for human rights and
equality. Today education is perceived as a right and it is seen as a government duty to
provide broad access to basic education. A certain amount of schooling is even compulsory in
most countries, although not all the countries have the capacity to broadly enforce such obligations.

Despite all of these developments and efforts in the 
expansion of education there are still about 8 % of children worldwide 
which are not in school.
Also progress has been heterogeneous across the globe, with literacy rates
among the youth of sub Saharan Africa - for example - still below 50 %.^[The data and an interesting analysis can be found in @Roser2021 at https://ourworldindata.org/children-not-in-school]

Education policies have thus been widely debated 
in international policy circles and 
have spurred debates as well as to 
seemingly unresolvable controversies about 
what is needed to further improve education for 
those who are still behind and precisely which policies would and
would not work towards this goal.

For example, the majority view in international policy circles has been that the focus of education policy should be to
get children into the class room and provide the
necessary teachers. This entails policies to 
make sure that children have
access to a school nearby as well as the 
free provision of basic education. Once this is
assured, according to this approach, the rest will
more or less follow automatically.

But enrollment does not automatically mean that all the children enrolled actually attend.
For example, we know from the DHS, the 
demographic and health survey you learned about
in previous lectures, that there is a gap 
between enrollment and attendance in the countries 
that have the most urgent need for
improved education policies. From the
surveys we also know that the absence is not 
necessarily driven by the fact that the
children are needed at home. 

Look at this graph compiled from DHS data provided by Our World in Data:

```{r}
library(JWL)
plot(enrol_attend_dat$Enrolment, enrol_attend_dat$Attendance, 
     main = ("Enrollment versus Attendance"), xlab = ("Enrollment rate primary school"), ylab = ("Attendance rate primary school"), pch = 16)

abline(1,1, col = "blue")
```

In the graph you see the enrolment rate in primary school at the x-axes and the
attendance rate on the y-axes. If all children who are enrolled would also attend
all the points would be on the blue line, showing all the points where the enrollment rate
is equal to the attendance rate. In the data you see many observations where the attendance 
rate is below the enrolment rate and these observations are quite heterogeneous.

Another aspect that was debated is that enrollment does not necessarily mean that children
actually learn something or that they learn at a sufficient quality. This problem of poor 
learning outcomes in many parts of the globe was demonstrated in many cases.^[For details on the evidence you can for instance look at chapter 4 in the excellent book by @BanerjeeDuflo2012]

Does this mean these policies are wrong, as their critics claim. They point out that there
is no point in all this educational supply if  there is no demand 
for it, taking lack of attendance and lack of sufficient learning 
outcomes as evidence. If there were demand, these
critics argue, supply would follow and the demand arises by itself as soon as there are
high enough returns to education.

Why can't controversies like these not simply be resolved by looking at the data?

## Correlation does not imply causation

Many data sources we have just rely on education policy outcomes somebody has observed and
recorded, so called observational studies. For instance, there is a large literature, 
mainly from before the mid 1990ies in education research, which tried to answer 
the following question: Do increased financial resources for public 
schools improve educational outcomes as measured by standard education achievement tests?

A look across countries would - for instance - give the following picture.

```{r}
plot(expenditure_outcome_dat$Expenditure, 
     expenditure_outcome_dat$Outcome, 
     main = ("Average learning outcomes by total education expenditure per capita"), 
     xlab = ("Public and private per capita expenditure on education (PPP, constant 2011-intl $)"), 
     ylab = ("Average harmonized learning outcome in 2005 - 2015"), pch = 16)

axis(1, at = seq(500, 3500, by = 500))
```


Indeed a study by @Hedgesetal1994 confirmed patterns like these. There is strong
evidence that education spending and educational outcomes are positively correlated.
Remember our discussion of measuring the relation between two variables in section 3.5?

But does this evidence support the conclusion that there is a causal relation between 
increased educational spending and student outcomes? There is a straightforward reason why
this can not be the case. 

We compare very different students or schools across countries in
the scatter plot we produced before. In the study by @Hedgesetal1994 the variables that were
related were students from different households across different schools. In the
plot we show here it is students from different households accross different counties. For
the general point we want to make here, this is immaterial. What matters is that simply comparing outcomes of students across countries or students across schools with different
levels of spending does not tell us whether the different spending levels are causal for the
variation in outcomes we observe. There may be many other differences between countries and
students that are shaping this relationship.^[There are whole websites documenting examples where two variables are highly correlated but it is obvious that there can not be a causal
relation. These examples are quite funny. One site you can find for example here:
https://tylervigen.com/spurious-correlations]

Statisticians have summarized this observation in the mantra 
that **"correlation does not imply causation"**. If we want to 
establish a causal relation from data we need to work harder.

## What does it mean that a variable is "causal"?

When you ask a doctor today about the risks of smoking, he will usually give
you a warning and advise against it. One of the arguments will be that the evidence
shows that smoking causes lung cancer.

But assume you have been a smoker and now come into the unfortunate 
situation that you are diagnosed with
lung cancer. How do you know that your lung cancer occurred, because you
were smoking and you would not have got lung cancer anyway, perhaps for an
entirely different reason?

It took the medical profession decades to accept the causality of smoking of lung cancer.
The many studies that were undertaken on this subject since the 1930es usually compared
people who smoke with non-smokers and tried to take conclusions about the effect from this
comparison. In this comparisons the smokers came out badly, showing among many other things
higher incidence of lung cancer. So many studies together established a strong correlation
between smoking and lung cancer. If smoking cause lung cancer, this would be an explanation
of the association between smoking and lung cancer. But this evidence is only circumstantial.
There could be some hidden factor - so called **confounding factor** - which makes 
people smoke and also makes them get lung
cancer. In this case there would be no point in quitting smoking.

The famous statistician Sir R.A. Fisher doubted the causal role of cigarettes for lung cancer 
and suggested possible confounding factors. Epidemiologists conducted careful studies that
accumulated evidence that these confounding factors were not plausible. Over time the
evidence taken together convinced the medical community that smoking is indeed causal for
lung cancer. But causal here has a meaning that takes into account the unavoidable variability that underlies real life.

The statistical notion of causation or causality is not deterministic. If statisticians say
there is evidence that smoking is causal for lung cancer it does not mean that every smoker will
end up with lung cancer. It does also not mean that lung cancer can occur 
only if you are a smoker. It rather means that among the group of smokers lung cancer will occur
more often. Thus we can not use statistical methods to establish causation in 
a specific case. The only thing we can possibly establish by data is that 
smoking increases the proportion of times lung cancer will occur in a population.

This has two important consequences for what needs to be done if we want to find out what
causes what. We need to intervene, that is consciously change a variable, and 
perform experiments. But since we are in a situation of uncertainty we need to intervene
more than once, to establish strong enough evidence.

For this reason much of research in medicine and epidemiology today relies on 
evidence gained from so called controlled experiments. Let us explain next what
the ideas behind such an approach is. These ideas have also been extended to other
field, like the research on what works in policies attempting to improve
education and to other fields.

## Controlled experiments

### Field trials for early polio vaccines in the US in the 1950ies

Let us take an example from medicine first. Suppose a new drug is found and we
would like to introduce it as a medication that can be prescribed by doctors to their
patients.

The basic idea of testing the effectiveness of the medicine is **comparison**. An
experiment is designed in which the medicine is given to a **treatment group**. But there
are other subjects that are not treated and form another group, the **control group**.
Then the responses of the two groups are compared. The assignemt of subjects to both 
groups should be **random** and  the experiment should be run **double blind**: Neither the subjects
nor the researchers who measure the response should know who was in the control group.

The key of randomization in this protocol is that it keeps the two groups equal except for
the treatment. Then, if you measure different outcomes you can conclude with some
confidence that the observed difference must be the result of the treatment. This would then
establish causality.

One of the first epidemiological studies implementing these ideas was a randomized controlled
experiment to show the effectiveness of a vaccine against polio in the US in 1954 . 
It was conducted by the Public Health Service and the National Foundation for 
Infantile Paralysis (NFPI).

The field trial was conducted in select school districts among the children in the 
age group most vulnerable. Children could, however, only be vaccinated with their 
parents consent, so only children whose parents agreed to the vaccination could 
go into the treatment group. Since parents with higher income were more 
likely to consent to treatment. This biases the design against the vaccine, since polio is
a disease of hygiene. Children in more hygienic surroundings have less exposure to mild
cases of polio in early childhood. This prevents the generation of antibodies which protect
them against severe infections at a later age.

Many experts recognized the flaw in the design. The assignment to treatment and control was
not random and thus the treatment and control group were systematically different. The 
idea, however, is that both groups should be as similar as possible except for treatment. If the
two groups differ to some factor other than treatment this factor my **confound** or get
mixed up with the effect of treatment.

To exclude the bias in assignment introduced by parental consent in the NFPI design, 
it was suggested that the
control group has to chosen from the same population as the treatment group using an impartial chance procedure. Such an approach is called **randomized controlled**.

The idea of double blindness was also already used in a new improved design. The children 
in the control group were given an injection of salt dissolved in water rather than 
the vaccine, a so called **placebo**. During the experiment therefore the subjects 
did not know whether they were in the control group or in the treatment group. Also 
the doctors were not told which group the child belonged to and neither knew 
those who evaluated the responses.

A subsequent program evaluation published in a scientific journal (@Francis1955), showed the
bias in the NFPI study in comparison with the randomized controlled double blind
experiment:

```{r}
#| tbl-cap: Results of the 1954 polio vaccine trials
#| tbl-subcap: ["Randomized Trial", "NFPI study"]
#| layout-ncol: 2

RCT <- data.frame(Group = c("Treatment", "Control", "No consent"), Size = c(200000, 200000, 350000), Rate = c(28, 71, 46))

NFIP <- data.frame(Group = c("Grade 2 (vaccine)", "Grades 1 and 2 (control)", "Grade 2 (no consent"), Size = c(225000, 725000, 125000), Rate = c(25, 54, 44))

library(knitr)

# table on the left
kable(RCT)

# table on the right
kable(NFIP)
```

The two tables show the results of the randomized trial design with the NFPI design. The size
of groups and the rates of polio cases per 100.000 in each group are shown in the columns
Size and Rate.

In the randomized controlled trial the vaccination cut the polio rate from 71 to 28 per 
hundred thousand. The NFPI study - by contrast - shows  a reduction from 54 to 25 per 
hundred thousand.
The source of the bias which accounts for the difference was confounding. The NFPI 
treatment group contained only  only children whose partents consented to the 
vaccination. However the
control group contained also children whose partents would not have consented. 
Therefore control and treatment group were not comparable.


### School subsidies for the poor: The Mexican progresa poverty program

While randomized controlled trials have their origin in medical research and
epidemiology, this is an approach that can be applied in other contexts to establish
a causal effect. In such fortunate cases, it is possible to produce actual
evidence that a certain policy works or not.

As an example consider an education policy program for the poor designed in Mexico
in the 1990 called Progresa. The problem Progresa tried to address was how a reasonable 
support program could be designed to increase school enrollment among the poor.

It was known that school enrollment among the poor was low and that one reason for this
low enrollment was that children needed to work to help supporting their families rather
than going to school. This situation could be characterized as a kind of poverty trap
because the income of the poor families is so low that they cannot afford to enroll
the children in school which prevents them to get enough training and eventually 
higher incomes which can raise them out of poverty.

The Finance minister at that time, Santiago Levy, initiated a program trying to address
this problem by designing education grants to poor mothers to support the increase of school
enrollment of their children. The Progresa program tried to avoid some known
problems of past poverty alleviation programs. If a transfer conditions on current
income. If a recipient of the transfer joins the paid labor force the transfer is lost as
earnings increase. This can lead to disincentives to attempts of joining the paid labor force.

The Progresa Program therefore tried to minimize disincentives to work by not conditioning the
transfer on current income but only on an initial assessment of household poverty information.
This approach was tried as an alternative to build more schools  and augment other 
educational resources such as teachers and teaching quality. While there was 
evidence that this approach
would increase enrollment it was known that such supply policies were not 
effective for increasing enrollment amongt the poor. So Santiago Levy took a 
household demand approach providing subsidies which can administratively 
targeted to the poor and thereby perhaps help increase the enrollment rates 
in these groups. But would this work?

Santiago Levy conducted a randomized controlled trial on a set of villages, selecting
some of them at random for receiving Progresa benefits. To provide additional
incentives for child enrollment the transfers were conditioned on whether children 
attended school regularly and the family used preventive health care. More money was given 
to children in secondary school than to children in primary school to compensate
for lost wages (from going to school rather than taking up paid work) and also more
money was given for enroling girls than for enroling boys. 

Through the selection of all potentially eligible households based on geographical and
household poverty information all of these households share similar characteristics. The
random allocation of Progresa recipients splits this population into a 
treatment group (the Progresa recipients) and a control group ( the
households which did not receive Progresa transfers). If the groups are then
compared with respect to enrolment rates we can say that they are similar in
all dimensions except treatment. Differences in enrolment rates between the groups
can then be understood as causally linked to the treatment. There is no
confounding. This is the basic idea
of a randomized controlled trial.

It could indeed be established that Progresa worked to increas school enrolment among
the poor in Mexico. Here are the numbers: For boys the enrolment increased from 73 % in the
control group to 77 % in  the Progresa group. For girls the increase was even bigger, from
67 % in the contril group to 75 % in the Progresa group.

The evidence for the success of the Progresa Program also helped that the program
survived when political power was transferred to a new government in the next election.
Although Santiago Levy was not prime minister any more, the new government continued the
program under the new name Oppordunitades.

## Observational studies

Controlled experiments are different from **observational** studies. In a controlled 
experiment the investigator decides who will be in the treatment group and who will be
in the control group. In an observational study, by contrast, the investigator can
just observe what happens and he has no control over assigning subjects to different
groups. In an observational study the subjects assign themselves to treatment and
control.

This terminology can be confusing because it uses the word control in two meanings: On the
one hand, as we discussed in the previous sections a control is a subject who did not get
the treatment. A controlled experiment is a study where the investigator decides who will be
in the treatment group and who will be in the control group.

Going back to studies on the effects of smoking, which we discussed earlier in this
lecture, we can see that such studies must necessarily be observational. We can not 
make some people smoke for a decade or more just to fulfill a statistically satisfactory
research design.

Note, however, that the basic idea of treatment and control groups is still used. The
statistician in an observational study on the effects of smoking would
compare the smokers - the treatment group with exposure to smoking - with the non-smokers,
the control group. The idea is that we can learn something about the effects from 
smoking by such a comparison.

In this comparison, in the many observational studies that have been done on smoking since
the early 20th century, smokers come out pretty badly. They suffer more frequently than
non-smokers from heart attacks, lung cancer and other diseases. If smoking would cause
these higher rates, this would explain the correlation observed in the data of the
observational studies.

However, observing this association, is incomplete as a proof for causation. There might
be a hidden factor, not included in the observational study, which makes people smoke and at the
same time makes people ill. Such hidden factors are called **confounding factors** or
**confounders** in the technical jargon of statisticians.

To think about potential problems of confounding, it is helpful to ask yourselve how
the controls were selected. The question is whether the control group is really 
similar to the treatment group apart from the exposure of interest. If there is
confounding something has to be done to deal with it. Statisticians often talk about
controlling for confounding factors in an observational study. This is yet another, a
third sense in which the word control is used in statistics.

The meaning of the term control in this case is that in the case where subjects 
differ among themselves in an observational study in crucial ways 
beside treatment, the statistician adjusts
the sample by comparing smaller and more homogemous groups.

A famous example illustrating this point and featured in many textbooks^[I follow here the
discussion in the textbook by @FrePisPur2009 pp. 18-19] is an
observational study on sex bias in admission to graduate studies at the University
of California in Berkeley in the 1975. During the study period, 8442 men and 4321 women applied for admission to graduate school. The acceptance rate among men was 44 % and that of
women 35 %. 

Assuming that men and women overall were equally well qualified this difference in the
admission rate looks like evidence of a different treatment of men and women. 

Since each subject did its own admission to graduate studies, the university should
have been able to identify the subjects which discriminated against women. But at that
point a puzzle appeared. If subjects were looked at seperately subject by subject there
didn't seem to be any bias against women.If there was a bias it was a bias against men.
How is this possible?

Over a hundred subjects were involved. However, the six largest together 
accounted for over one-third of the total number of applicants. The pattern 
for these subjects was typical of the whole campus.

These are the data for men:

| Subject | Number of Applicants| Percent admitted |
|---------|---------------------|------------------|
| A       | 825                 | 62               | 
| B       | 560                 | 63               | 
| C       | 325                 | 37               |
| D       | 417                 | 33               |
| E       | 191                 | 28               |
| F       | 373                 |  6               |


: Admissions data for men graduate programs in the six largest 
subjects at University of California, Berkeley {.striped .hover}

And these are the data for women:

| Subject | Number of Applicants| Percent admitted |
|---------|---------------------|------------------|
| A       | 108                 | 82               | 
| B       |  25                 | 68               | 
| C       | 593                 | 34               |
| D       | 375                 | 35               |
| E       | 393                 | 24               |
| F       | 341                 |  7               |

: Admissions data for women graduate programs in the six largest 
subjects at University of California, Berkeley {.striped .hover}

In each subject, the percentage of female applicants who were admitted is 
roughly equal to the percentage of male applicants. Subject A is an exception. 
It appears to discriminate against men because it admitted 82 % of the
women and only 62 % of the men. The subject that looks most biased against women is E.
But the difference here is only 4 percentage points. When we take all six
subjects together they admitted 44% of the male applicants and only 30 % of the 
females. The difference amounts to 14 percentage points.

There is, however an explanation for this seemingly paradox situation. The first
two subjects A and B were easy to get into. Over 50% of men applied to these
two subjects. The other four were much harder to get into. Over 90% of the women
applied to these subjects. 

This means that men applied to the easy subjects whereas women applied for the hard ones. 
there was a confounding effect. The effect due to the choice of 
subject was confounded with the effect due to sex. When the choice of the subject is
controlled for there is little difference between admission rates of men and women.
(Simpson paradox)


### How do randomized controlled trials differ from observational studies?

### Labor market consequences of schooling: Data from Indonesia

Use article by Esther Duflo in AER to show what is needed to establish causality in
an observational study.
