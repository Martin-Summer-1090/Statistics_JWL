# What causes what?

::: {.callout-note appearance="simple"}
## Overview

In this lecture we discuss what causation means in a statistical sense and 
why we need be careful to distinguish between causation and correlation. 
This is a conceptually difficult topic. It is, however, not technically difficult 
but it will need lots of examples. Fortunately there are many good (and bad) real 
world examples that I hope will stick with you as reference examples after the course. 
At the end of this lecture you should understand the idea of randomized trials and 
observational studies as well as the general ideas of comparison in statistical analysis.

1. Causation in a statistical sense means that when we intervene, the chances of different outcomes vary systematically.

2. Causation is difficult to establish statistically, but well designed randomized trials are the best available framework.

3. You will learn about the principles that helped clinical trials to identify effects and how you can
transfer these principles to other fields.

4. Observational data may have background factors influencing the apparent relationships between exposure and outcome which may be either observed confounders or lurking factors.

5. Statistical methods do not suspend judgment which is always required for the confidence with which causation can be claimed.

:::



The last two centuries brought enormous advances in the expansion of education around the
world. Quantitative measures such as literacy rates, school enrollment, the share of 
education spending in GDP all point into the same direction: While access to education
had been the privilege of a very small elite throughout history, things have changed
in the direction of a very broad participation. 

This is a result of both economic
necessities of modern production as well as of political struggles for human rights and
equality. Today education is perceived as a right and it is seen as a government duty to
provide broad access to basic education. A certain amount of schooling is even compulsory in
most countries, although not all the countries have the capacity to broadly enforce such obligations.

Despite all of these developments and efforts in the 
expansion of education there are still 
about 8 % of children worldwide 
which are not in school.
Also progress has been heterogeneous across the globe, with literacy rates
among the youth of sub Saharan Africa - for example - still below 50 %.^[Data on this particular topic and an interesting analysis can be found in @Roser2021 at https://ourworldindata.org/children-not-in-school]

Education policies have thus been widely debated 
in international policy circles and 
have spurred debates as well as to 
seemingly unresolvable controversies about 
what is needed to further improve education for 
those who are still behind and precisely which policies would and
would not work towards this goal.

For example, the majority view in international policy circles has been that the focus of education policy should be to
get children into the class room and provide the
necessary teachers. This entails policies to 
make sure that children have
access to a school nearby as well as the 
free provision of basic education. Once this is
assured, according to this approach, the rest will
more or less follow automatically.

But enrollment does not automatically mean that all the children enrolled actually attend.
For example, we know from the DHS, the 
demographic and health survey you learned about
in previous lectures, that there is a gap 
between enrollment and attendance in the countries 
that have the most urgent need for
improved education policies. From the
surveys we also know that the absence is not 
necessarily driven by the fact that the
children are needed at home. 

Look at this graph compiled from DHS data provided by Our World in Data:

```{r}
library(JWL)
plot(enrol_attend_dat$Enrolment, enrol_attend_dat$Attendance, 
     main = ("Enrollment versus Attendance"), xlab = ("Enrollment rate primary school"), ylab = ("Attendance rate primary school"), pch = 16)

abline(1,1, col = "blue")
```

In the graph you see the enrolment rate in primary school at the x-axes and the
attendance rate on the y-axes. Such a graph, showing the relation between two
variables is called a **scatter plot**. We will learn about the details of scatter
plots later in the course. 

To understand the graph, think of each point as measuring
the enrollment rate in primary school and the attendance rate in primary school in a 
given year for a given country. Taking many countries around the globe gives you
a scatter of such points all showing different enrollment attendance pairs. The whole
cloud of points shows a positive slope suggesting that attendance varies positively with
enrollment. 

To help you interpret the scatter plot think of the following situation:
If all children who are enrolled would also attend
all the points would be on the blue line, showing all the points where the enrollment rate
is equal to the attendance rate. In the data you see many observations where the attendance 
rate is below the enrolment rate and these observations are quite heterogeneous.

Another aspect that was debated is that enrollment does not necessarily mean that children
actually learn something or that they learn at a sufficient quality. This problem of poor 
learning outcomes in many parts of the globe was demonstrated in many cases.^[For details on the evidence you can for instance look at chapter 4 in the excellent book by @BanerjeeDuflo2012]

Does this mean these policies are wrong, as their critics claim? They point out that there
is no point in all this educational supply if  there is no demand 
for it, taking lack of attendance and lack of sufficient learning 
outcomes as evidence. If there were demand, these
critics argue, supply would follow and the demand arises by itself as soon as there are
high enough returns to education.

Why can't controversies like these not simply be resolved by looking at the data?

## Correlation does not imply causation

Many data sources we have, just rely on education policy outcomes somebody has observed and
recorded, so called **observational studies**. For instance, there is a large literature, 
mainly from before the mid 1990ies in education research, which tried to answer 
the following question: Do increased financial resources for public 
schools improve educational outcomes as measured by standard education achievement tests?

A look across countries would - for instance - give the following picture.

```{r}
plot(expenditure_outcome_dat$Expenditure, 
     expenditure_outcome_dat$Outcome, 
     main = ("Average learning outcomes by total education expenditure per capita"), 
     xlab = ("Public and private per capita expenditure on education (PPP, constant 2011-intl $)"), 
     ylab = ("Average harmonized learning outcome in 2005 - 2015"), pch = 16)

axis(1, at = seq(500, 3500, by = 500))
```


Indeed a study by @Hedgesetal1994 confirmed patterns like these. There is strong
evidence that education spending and educational outcomes are **positively correlated**.
This is the statistics jargon for saying that the variables covary positively. We will
learn in detain about correlation in the next chapter.

But does this evidence support the conclusion that there is a causal relation between 
increased educational spending and student outcomes? There is a straightforward reason why
this can not be the case. 

We compare very different students or schools across countries in
the scatter plot we produced before. In the study by @Hedgesetal1994 the variables that were
related were students from different households across different schools. In the
plot we show here it is students from different households across different countries. For
the general point we want to make here, this is immaterial. What matters is that simply comparing outcomes of students across countries or students across schools with different
levels of spending does not tell us whether the different spending levels are causal for the
variation in outcomes we observe. There may be many other differences between countries and
students that are shaping this relationship.^[There are whole websites documenting examples where two variables are highly correlated but it is obvious that there can not be a causal
relation. These examples are quite funny. One site you can find for example here:
https://tylervigen.com/spurious-correlations Try it out!]

Statisticians have summarized this observation in the mantra 
that **"correlation does not imply causation"**. If we want to 
establish a causal relation from data we need to work harder.

## What does it mean that a variable is "causal"?

When you ask a doctor today about the risks of smoking, he will usually give
you a warning and advise against it. One of the arguments will be that the evidence
shows that smoking causes lung cancer.

But assume you have been a smoker and now come into the unfortunate 
situation that you are diagnosed with
lung cancer. How do you know that your lung cancer occurred, because you
were smoking and you would not have got lung cancer anyway, perhaps for an
entirely different reason?

It took the medical profession decades to accept the causality of smoking of lung cancer.
The many studies that were undertaken on this subject since the 1930es usually compared
people who smoke with non-smokers and tried to take conclusions about the effect from this
comparison. In this comparisons the smokers came out badly, showing among many other things
higher incidence of lung cancer. So many studies together established a strong correlation
between smoking and lung cancer. If smoking cause lung cancer, this would be an explanation
of the association between smoking and lung cancer. But this evidence is only circumstantial.
There could be some hidden factor - so called **confounding factor** - which makes 
people smoke and also makes them get lung
cancer. In this case there would be no point in quitting smoking.

The famous statistician Sir R.A. Fisher doubted the causal role of cigarettes for lung cancer 
and suggested possible confounding factors. Epidemiologists conducted careful studies that
accumulated evidence that these confounding factors were not plausible. Over time the
evidence taken together convinced the medical community that smoking is indeed causal for
lung cancer. But causal here has a meaning that takes into account 
the unavoidable variability that underlies real life.

The statistical notion of causation or causality is not deterministic. If statisticians say
there is evidence that smoking is causal for lung cancer it does not mean that every smoker will
end up with lung cancer. It does also not mean that lung cancer can occur 
only if you are a smoker. It rather means that among the group of smokers lung cancer will occur
more often. Thus we can not use statistical methods to establish causation in 
a specific case. The only thing we can possibly establish by data is that 
smoking increases the proportion of times lung cancer will occur in a population.

This has two important consequences for what needs to be done if we want to find out what
causes what. We need to intervene, that is consciously change a variable, and 
perform experiments. But since we are in a situation of uncertainty we need to intervene
more than once, to establish strong enough evidence.

For this reason much of research in medicine and epidemiology today relies on 
evidence gained from so called controlled experiments. Let us explain next what
the ideas behind such an approach is. These ideas have also been extended to other
fields, like the research on what works in policies attempting to improve
education and to other fields.

## Controlled experiments

### Field trials for early polio vaccines in the US in the 1950ies

Let us take an example from medicine first. Suppose a new drug is found and we
would like to introduce it as a medication that can be prescribed by doctors to their
patients.^[This discussion follows @FrePisPur2009]

The basic idea of testing the effectiveness of the medicine is **comparison**. An
experiment is designed in which the medicine is given to a **treatment group**. But there
are other subjects that are not treated and form another group, the **control group**.
Then the responses of the two groups are compared. The assignment of subjects to both 
groups should be **random** and  the experiment should be run **double blind**: Neither the subjects
nor the researchers who measure the response should know who was in the control group.

The key of randomization in this protocol is that it keeps the two groups equal except for
the treatment. Then, if you measure different outcomes you can conclude with some
confidence that the observed difference must be the result of the treatment. This would then
establish causality.

One of the first epidemiological studies implementing these ideas was a randomized controlled
experiment to show the effectiveness of a vaccine against polio in the US in 1954 . 
It was conducted by the Public Health Service and the National Foundation for 
Infantile Paralysis (NFPI).

The field trial was conducted in select school districts throughout the country where the
risk of polio was high. Two million children were involved and half a million were
vaccinated. A million were deliberately left unvaccinated as controls, half a million refused 
vaccination.

Children could, however, only be vaccinated with their 
parents consent, so only children whose parents agreed to the vaccination could 
go into the treatment group. Parents with higher income were more 
likely to consent to treatment. This biases the design against the vaccine, since polio is
a disease of hygiene. Children in more hygienic surroundings have less exposure to mild
cases of polio in early childhood. This prevents the generation of antibodies which protect
them against severe infections at a later age.

Many experts recognized the flaw in the design. The assignment to treatment and control was
not random and thus the treatment and control group were systematically different. The 
idea, however, is that both groups should be as similar as possible except for treatment. If the
two groups differ to some factor other than treatment this factor my **confound** or get
mixed up with the effect of treatment.

To exclude the bias in assignment introduced by parental consent in the NFPI design, 
it was suggested that the
control group has to chosen from the same population as the treatment group using an impartial chance procedure. Such an approach is called **randomized controlled**.

The idea of double blindness was also already used in a new improved design. The children 
in the control group were given an injection of salt dissolved in water rather than 
the vaccine, a so called **placebo**. During the experiment therefore the subjects 
did not know whether they were in the control group or in the treatment group. Also 
the doctors were not told which group the child belonged to and neither knew 
those who evaluated the responses.

A subsequent program evaluation published in a scientific journal (@Francis1955), showed the
bias in the NFPI study in comparison with the randomized controlled double blind
experiment:

```{r}
#| tbl-cap: Results of the 1954 polio vaccine trials
#| tbl-subcap: ["Randomized Trial", "NFPI study"]
#| layout-ncol: 2
#| label: tbl-salk-vaccine-field-trials

RCT <- data.frame(Group = c("Treatment", "Control", "No consent"), Size = c(200000, 200000, 350000), Rate = c(28, 71, 46))

NFIP <- data.frame(Group = c("Grade 2 (vaccine)", "Grades 1 and 2 (control)", "Grade 2 (no consent)"), Size = c(225000, 725000, 125000), Rate = c(25, 54, 44))

library(knitr)

# table on the left
kable(RCT)

# table on the right
kable(NFIP)
```

The two tables show the results of the randomized trial design with the NFPI design. The size
of groups and the rates of polio cases per 100.000 in each group are shown in the columns
Size and Rate.

In the randomized controlled trial the vaccination cut the polio rate from 71 to 28 per 
hundred thousand. The NFPI study - by contrast - shows  a reduction from 54 to 25 per 
hundred thousand.
The source of the bias which accounts for the difference was confounding. The NFPI 
treatment group contained only  only children whose parents consented to the 
vaccination. However the
control group contained also children whose parents would not have consented. 
Therefore control and treatment group were not comparable.


### School subsidies for the poor: The Mexican progresa poverty program

While randomized controlled trials have their origin in medical research and
epidemiology, this is an approach that can be applied in other contexts to establish
a causal effect. In such fortunate cases, it is possible to produce actual
evidence that a certain policy works or not.

As an example consider an education policy program for the poor designed in Mexico
in the 1990 called Progresa. The problem Progresa tried to address was how a reasonable 
support program could be designed to increase school enrollment among the poor.

It was known that school enrollment among the poor was low and that one reason for this
low enrollment was that children needed to work to help supporting their families rather
than going to school. This situation could be characterized as a kind of poverty trap
because the income of the poor families is so low that they cannot afford to enroll
the children in school which prevents them to get enough training and eventually 
higher incomes which can raise them out of poverty.

The Finance minister at that time, Santiago Levy, initiated a program trying to address
this problem by designing education grants to poor mothers to support the increase of school
enrollment of their children. The Progresa program tried to avoid some known
problems of past poverty alleviation programs. If a transfer conditions on current
income, this can for instance create an incentive against taking up a job. This 
can happen because if a 
recipient of the transfer joins the paid labor force the transfer is lost as
earnings increase. This can lead to disincentives to attempts of joining the paid labor force.

The Progresa Program therefore tried to minimize disincentives to work by not conditioning the
transfer on current income but only on an initial assessment of household poverty information.
This approach was tried as an alternative to build more schools  and augment other 
educational resources such as teachers and teaching quality. While there was 
evidence that this approach
would increase enrollment it was known that such supply policies were not 
effective for increasing enrollment amongt the poor. So Santiago Levy took a 
household demand approach providing subsidies which can administratively 
be targeted to the poor and thereby perhaps help increase the enrollment rates 
in these groups. But would this work?

To find out the ministry of Finance 
conducted a randomized controlled trial on a set of villages, selecting
some of them at random for receiving Progresa benefits. To provide additional
incentives for child enrollment the transfers were conditioned on whether children 
attended school regularly and the family used preventive health care. More money was given 
to children in secondary school than to children in primary school to compensate
for lost wages (from going to school rather than taking up paid work) and also more
money was given for enroling girls than for enroling boys. 

Through the selection of all potentially eligible households based on geographical and
household poverty information all of these households share similar characteristics. The
random allocation of Progresa recipients splits this population into a 
treatment group (the Progresa recipients) and a control group ( the
households which did not receive Progresa transfers). If the groups are then
compared with respect to enrollment rates we can say that they are similar in
all dimensions except treatment. Differences in enrollment rates between the groups
can then be understood as causally linked to the treatment. There is no
confounding. This is the basic idea
of a randomized controlled trial.

It could indeed be established that Progresa worked to increase school enrollment among
the poor in Mexico. Here are the numbers: For boys the enrollment increased from 73 % in the
control group to 77 % in  the Progresa group. For girls the increase was even bigger, from
67 % in the control group to 75 % in the Progresa group.

The evidence for the success of the Progresa Program also helped that the program
survived when political power was transferred to a new government in the next election.
Although Santiago Levy was not prime minister any more, the new government continued the
program under the new name Oppordunitades.


## Observational studies

Controlled experiments are different from **observational** studies. In a controlled 
experiment the investigator decides who will be in the treatment group and who will be
in the control group. In an observational study, by contrast, the investigator can
just observe what happens and he has no control over assigning subjects to different
groups. In an observational study the subjects assign themselves to treatment and
control.

This terminology can be confusing because it uses the word control in two meanings: On the
one hand, as we discussed in the previous sections a control is a subject who did not get
the treatment. A controlled experiment is a study where the investigator decides who will be
in the treatment group and who will be in the control group.

Going back to studies on the effects of smoking, which we discussed earlier in this
lecture, we can see that such studies must necessarily be observational. We can not 
make some people smoke for a decade or more just to fulfill a statistically satisfactory
research design.

Note, however, that the basic idea of treatment and control groups is still used. The
statistician in an observational study on the effects of smoking would
compare the smokers - the treatment group with exposure to smoking - with the non-smokers,
the control group. The idea is that we can learn something about the effects from 
smoking by such a comparison.

In this comparison, in the many observational studies that have been done on smoking since
the early 20th century, smokers come out pretty badly. They suffer more frequently than
non-smokers from heart attacks, lung cancer and other diseases. If smoking would cause
these higher rates, this would explain the correlation observed in the data of the
observational studies.

However, observing this association, is incomplete as a proof for causation. There might
be a hidden factor, not included in the observational study, which makes people smoke and at the
same time makes people ill. Such hidden factors are called **confounding factors** or
**confounders** in the technical jargon of statisticians.

To think about potential problems of confounding, it is helpful to ask yourselve how
the controls were selected. The question is whether the control group is really 
similar to the treatment group apart from the exposure of interest. If there is
confounding something has to be done to deal with it. Statisticians often talk about
controlling for confounding factors in an observational study. This is yet another, a
third sense in which the word control is used in statistics.

The meaning of the term control in this case is that in the case where subjects 
differ among themselves in an observational study in crucial ways 
beside treatment, the statistician adjusts
the sample by comparing smaller and more homogemous groups.

A famous example illustrating this point and featured in many textbooks^[I follow here the
discussion in the textbook by @FrePisPur2009 pp. 18-19] is an
observational study on sex bias in admission to graduate studies at the University
of California in Berkeley in the 1975. During the study period, 8442 men and 4321 women applied for admission to graduate school. The acceptance rate among men was 44 % and that of
women 35 %. 

Assuming that men and women overall were equally well qualified this difference in the
admission rate looks like evidence of a different treatment of men and women. 

Since each subject did its own admission to graduate studies, the university should
have been able to identify the subjects which discriminated against women. But at that
point a puzzle appeared. If subjects were looked at seperately subject by subject there
didn't seem to be any bias against women.If there was a bias it was a bias against men.
How is this possible?

Over a hundred subjects were involved. However, the six largest together 
accounted for over one-third of the total number of applicants. The pattern 
for these subjects was typical of the whole campus.

These are the data for men:

| Subject | Number of Applicants| Percent admitted |
|---------|---------------------|------------------|
| A       | 825                 | 62               | 
| B       | 560                 | 63               | 
| C       | 325                 | 37               |
| D       | 417                 | 33               |
| E       | 191                 | 28               |
| F       | 373                 |  6               |


: Admissions data for men graduate programs in the six largest 
subjects at University of California, Berkeley {.striped .hover}

And these are the data for women:

| Subject | Number of Applicants| Percent admitted |
|---------|---------------------|------------------|
| A       | 108                 | 82               | 
| B       |  25                 | 68               | 
| C       | 593                 | 34               |
| D       | 375                 | 35               |
| E       | 393                 | 24               |
| F       | 341                 |  7               |

: Admissions data for women graduate programs in the six largest 
subjects at University of California, Berkeley {.striped .hover}

In each subject, the percentage of female applicants who were admitted is 
roughly equal to the percentage of male applicants. Subject A is an exception. 
It appears to discriminate against men because it admitted 82 % of the
women and only 62 % of the men. The subject that looks most biased against women is E.
But the difference here is only 4 percentage points. When we take all six
subjects together they admitted 44% of the male applicants and only 30 % of the 
females. The difference amounts to 14 percentage points.

There is, however an explanation for this seemingly paradox situation. The first
two subjects A and B were easy to get into. Over 50% of men applied to these
two subjects. The other four were much harder to get into. Over 90% of the women
applied to these subjects. 

This means that men applied to the easy subjects whereas women applied for the hard ones. 
there was a confounding effect. The effect due to the choice of 
subject was confounded with the effect due to sex. When the choice of the subject is
controlled for there is little difference between admission rates of men and women.

This is known as **Simpson's paradox** which occurs when the apparent direction 
of an association is reversed by adjusting for a confounding factor.

## Can we ever conclude causation from observational data ?

@Spi2019 discusses a list of criteria first suggested by the British applied 
statistician Austin Bradford Hill in 1965. These criteria should be considered before concluding
that an observed link between an outcome and an exposure was causal. These criteria 
were much debated and we show here a version of the list presented in @Spi2019 and which is
due to Jermey Howick - a clinical epidemiologist form the University of Oxford 
in the UK - and his colleagues.

These criteria are separated in three categories: Direct, mechanistic and parallel evidence.
I reproduce the list from the book of @Spi2019 here:

* Direct evidence
    + The size of the effect is so large that it can not be explained by plausible       confounding.
    + There is an appropriate temporal and/or spatial proximity, in that cause precedes effect and effect occurs after a plausible interval and/or cause occurs at the same site as the
    effect.
    + The effect increases as the exposure increases and the effect reduces upon reduction
    in exposure (dose responsiveness and reversibility).
* Mechanistic evidence
    + There is a plausible mechanism of action which could be biological, chemical or mechanical with external evidence for a causal chain.
* Parallel evidence
    + The effect fits with what is known already
    + The effect is also found when the study is replicated
    + The effect is found in similar, but not identical studies.
    
The appropriate handling of causation is still a contested field of statistics.
There are various techniques that are based on the idea to use certain techniques to
bring an observational study as close as possible to the situation of a randomized 
controlled experiment. All these techniques adjust in more or less sophisticated ways for 
potential confounders. Most of these techniques are based on regression, a topic we
study later in this course. Whatever sophisticated technique is used judgement is always required if causation is claimed.

## Analyzing and producing tables in R: The tapply function.

In this section we discussed examples of randomized controlled experiments and observational
studies to explain key ideas of examining data and in particular how to compare treatment
and control groups. In terms of R this gives us an opportunity to learn in this context about
a very powerful R function which would help us analyze datasets that result from such experiments and
help us to summarize large datasets.

use the arthritis table.

## Exercises:

### Exercises

::: {.callout-note}

## Exercise 1: The polio vaccine study 

Data from the polio vaccine studies discussed in the text suggest that in 1954, the
   school districts in the NFIP trial and in the randomized controlled experiment had
   similar exposure to the polio virus.
     (a) The data also show that children in the two vaccine groups (for the randomized    controlled experiment and the NFPI design) came from families with similar incomes and educational backgrounds. Which two numbers in table @tbl-salk-vaccine-field-trials confirm
     this finding?
     (b) The data show that children in the two no-consent groups had similar family backgrounds. WHich pair of numbers in the table confirm this finding?
     (c) The data show that children in the two control groups had different family backgrounds. Which pair of numbers in the table confirm this finding?
     (d) In the NGPI study, neither the control group nor the no-consent group got the vaccine. Yet the no-consent group had a lower rate of polio. Why?
     (e) To show that the vaccine works, someone wants to compare the 44/100.000 in the NFPI study with 25/100.000 in the vaccine group. What's wrong with these data?
     
:::

::: {.callout-note}

## Exercise 2: Did the vaccine field trial cause children to get polio?
     
The polio vaccine studies were conducted only in certain experimental areas (school districts) selected by the Public Health Service in consultation with local officials. In these areas there were about 3 million children in grades 1,2, or 3; and there were about 11 million children in those grades in the United States. In the experimental areas, the incidence of
polio was about 25% higher than in the rest of the country. Did the vaccine field trial cause children to get polio instead of preventing it? Answer yes or no and explain briefly.

:::

::: {.callout-note}

## Exercise 3: The effects of smoking on health

The Public Health Service studies the effects of smoking on health, in a large sample of
representative households. For men and for women in each age group, those who had never
smoked were on average somewhat healthier than the current smokers, but the current
smokers were  on average much healthier that those who had recently stopped smoking.

(a) Why did they study men and women in the different age groups separately?
(b) The lesson seems to be that you shouldn't start smoking, but once you've started don't stop. 
Comment briefly.

:::

::: {.callout-note}

## Exercise 4: Evaluating a prisoners rehabilitation program

A program to rehabilitate prisoners before their release is evaluated. The object
of the program is to reduce the recidivism rate - the percentage who will
be back in prison within two years of release. The program involves
several months of "boot camp" - military-style basic training with very strict
discipline. Admission to the program is voluntary. According to a prison
spokesman, those who complete the boot camp are less likely to return
to prision than other inmates.

(a) What is the treatment group in the prison spokesman's comparison? What is the
control group?
(b) Is the prison spokesman's comparison based on an observational study or 
a randomized controlled experiment?
(c) True of false: The data show that the bootcamp worked.

Explain your answers.

:::