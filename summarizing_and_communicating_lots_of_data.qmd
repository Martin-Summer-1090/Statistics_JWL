# Summarizing and communicating lots of data

When we analyze data, we usually have to look at lots of them. An example might be
income data gained from household surveys. Such a survey will contain a huge number of
data points, in the order of magnitude of ten thousands of data. These data need to be
summarized, to understand their main characteristics. 

In this unit you will learn the
most important tools for summarizing and communicating lost of data. You are going to
learn the principles how data summaries are constructed, what are the properties of
these summaries and what needs to be carefully considered. 

When we need to deal
with really large data sets, and most modern data sets are too large to be handled manually,
we will need the computer. We already did some first steps in R. In this unit we will build on these
first steps but enlarge them in a way that will enable you to deal and manipulate large
datasets on the computer. 

## Understanding variation in a single variable using histograms

### Constructing a Histogram

To summarize data, statisticians often use a graph which is called a **histogram**. In this section
we will discuss all you have to know about histograms and how to use them. Let us start by an example,
where we have about 100 data points, which is a lot but not that large that we can not handle them 
by hand.

The data we want to look at come from measurements of the annual flow of the river Nile at 
Aswan (formerly) Assuan in Egypt from 1871 to 1970. The units of these measurement in which the
annual flow is recorded are 100 millions of cubic meters, i.e. $10^8 m^3$. 

This is one of the data sets that is bundled with the R distribution and is available to all users of R. 
They are stored in an R object called `Nile`.^[When you type `data()` at the R console, you get a list
of all datasets that are available with the current distribution of R.]

This is how the data look like, when we print them to the R console using the R command
`print()`. The R function `options()` with the argument `width`just controls how the
numbers are printed. Here I made sure that they will fit in the width of the page.

```{r}
#| code-fold: false
options(width = 70)
print(Nile)
```

We start the construction of a histogram by choosing for the horizontal axes ranges of numerical
values - in our case of the river flow data - which are called *bins* or *classes*. There is no
fixed rule as to how to choose the size of these ranges. These ranges should neither be too 
fine, nor too
coarse. While there are lists of mechanical rules, which you can for example find on Wikipedia^[See https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width], it is usually best to use your domain
knowledge and some experimentation to find out the bin size that works best for your data.

For this example, assume we had chosen a bin size of 100^[Note that this will mean $100\times 10^8 m^3$ per year.]. When you study the list, you will find that the lowest value is at 456 while the highest value
is at 1370. This is already quite tedious to find out by eyeballing the numbers with the small 
number of values we have chosen for this example. It is impossible to do for really large data sets.

Now lets make a distribution table like this:

| Flow-bin   | Frequency |
|-----------:|----------:|
| 400 -  500 |          1|
| 500 -  600 |          0|
| 600 -  700 |          5|
| 700 -  800 |         20|
| 800 -  900 |         25|
| 900 - 1000 |         19|
|1000 - 1100 |         12|
|1100 - 1200 |         11|
|1200 - 1300 |          6|
|1300 - 1400 |          1|

In the column Flow-bin we have recorded the bins in steps of 100 and in the right column, Frequency, we
have recorded the count of values that are in this bin. 

When we make such a tabulation we have to agree on an endpoint convention. This is important, since 
when a flow value would for instance be measured as exactly 500, in which bin should it be counted:
400-500 or 500-600? You, the constructor of the histogram, has to take this decision. Let us
agree on the convention that when a value falls exactly at the endpoint of the bin, we
put it in the next bin. In practice you
will usually do a histogram by computer. The code of the computer program has to specify an endpoint
convention, so the computer knows what to do when a value coincides with an endpoint.

On the Frequency axes you put the frequency scale: Counts of values. Then for each bin, you plot
a bar, which has the width of the bin and the height of the frequency. 

Do this for all the bins
you have tabulated and you are ready.

The histogram provides a certain aggregation of the data because it sorts the 100 data points into 10
bins, in our example. While loosing some local information on individual data points the global information
conveyed by the summary gives us a pretty good idea of the overall pattern of variation on the Nile river
flow data. 

We can see, for instance, that the most frequent flow is between 800 and 900 and that the
variation is fairly symmetric around this bin. In the extremes this most frequent value can half or almost
double, so there is quite some spread in the data.

![Constructing the river flow histogram](pictures/nile_hist.png)

If we had just plotted all individual data points, we also got a picture, though you probably agree
that it is not particularly useful.

```{r}
plot(as.numeric(Nile), xlab = "Observation", ylab = "Annual Flow", pch = 16)
```

Histograms are such a common tool in statistics to explore the variation in one variable and the shape, how
it is roughly distributed that every statistical software has functions to produce histograms. In R, the
language we use in this course, there is also such a function. The function name is called `hist()` and it
takes the data as an argument. This is the second graphic function of R you encounter in this course
after we played with the `barplot()`function in the last lecture. 

To produce a histogram from the river flow data, we type at the console

```{r}
#| code-fold: false

hist(Nile)
```

:::{.callout-note icon=false}

## Now you try

Let us check your understanding of histograms by a little quiz now. The histogram below
shows the distribution of the final score in a certain class.

(a) Which block represents the people who scored between 60 and 80?
(b) Ten percent scored between 20 and 40 about what percentage scored between 40 and 60?
(c) About what percentage scored over 60?

![Final Score](pictures/final_score.png)
:::

### The density scale: Absolute versus relative frequency

Sometimes it might be useful, to choose a different scale for the y axes of your histogram. Instead
of absolute frequencies (or counts) it might be useful to show relative frequencies, the proportion
of occurrences in each bin. The type of scale you choose will depend on what kind of 
comparisons you want to emphasize about your data.

Let us look at this issue by an example. The numbers we want to look at report energy consumption per
capita in kwh per person per year for different countries around the world. The energy numbers 
refer to primary energy â€“ the energy input before the transformation to forms of energy for 
end-use (such as electricity or petrol for transport).Let us look at the
year 2019.

```{r}
library(JWL)
library(ggplot2)

dat <- with(energy_consumption_per_capita, energy_consumption_per_capita[Year == 2019, ])

hist_info <- hist(dat$Cons, plot = FALSE)         # Store output of hist function
hist_info$density <- hist_info$counts /    # Compute density values
  sum(hist_info$counts) * 100
plot(hist_info, freq = FALSE, xlab = "Primary energy consumption in kilowatt-hours per person per year.", ylab = "Percent", main = "Energy Consuption per Capita 2019")              # Plot histogram with percentages
```
In this histogram you see the distribution of per capita primary energy consumption for the year
2019 for countries around the globe. But now the y axes shows relative frequencies instead of counts.
For example, you see from the graph that roughly 55 % of countries have a primary energy consumption 
smaller that 20.000 kilowatt hours per person in this year. The next larger bucket contains already 
roughly half or 24 %. The biggest buckets are then a very small fraction of countries in the world. The
distribution is skewed. The countries with a really huge primary energy consumption per capita are a small
fraction of the countries in the world. 

When you have a histogram with a density or relative frequency scale, the total area sums to 100 (or to 1
depending on how you express the percentages, i.e. whether you express them as 10 % or as 0.1.)


Best practices for histograms
common misuses you should avoid
displaying unknown data

## Contents 

Statistics usually involves lots of data and we need ways to communicate and summarize these data. This
chapter introduces the most important concepts.


1. The empirical distribution of data points
2. Measures of location and spread.
3. Skewed data distributions are common and some summary statistics are very sensitive to outlying values. 
4. Summaries always hide some detail. 
5. How to summarize sets of numbers graphically (histograms and box plots)
6. Useful transformations to reveal patterns
7. Looking at pairs of numbers, scatter plots, time series as line graphs. 
8. The primary aim in data exploration is to get an idea of the overall variation.

## Next steps in R:

- vectors and indices
- extracting subsets from vectors
- creating vectors using c()
- subsetting vectors wit logicals
- data frames
- factor class
- extracting data from data frames
- tapply



<!-- ## Outcome  -->

<!-- Understand these concepts and work through may examples showing how to apply these summary measures to data on the -->
<!-- computer. -->