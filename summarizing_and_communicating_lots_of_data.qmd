# Summarizing and communicating lots of data

::: {.callout-note appearance="simple"}
## Overview

1. In this chapter we will learn how to summarize and communicate lots of data. Most
   datasets you will be working with in statistics are large and can contain thousands or
   even millions of observations. To understand these data and their pattern of
   variation you need tools to summarize them in a meaningful and informative way. In this
   chapter you will learn the tools to do so.

2. We first discuss a graphical tool to summarize the variation in a single variable by
   organizing the data in a way that shows us a the distribution of these
   data. This tool is called a **histogram** and you will encounter it very often across
   all applied data work and across all of statistics. To get a good understanding how to
   construct and work with a histogram, we first work with an example you can in principle
   do without a computer. This is useful to understand the basic principles of constructing
   and reading a histogram.
   
3. You will also learn to summarize large datasets by single numbers that provide information
   about the central tendency of the values of a variable. The **mean** is such a number, used
   over and over in statistics. You will learn what a mean is, what it tells you and when
   it gives a meaningful summary about the center of the distribution of a variable.

4. For a summary of a large dataset by numbers, you need not only an idea of at which values
   of a variable the data are centered but also how much they are spread out around this value.
   Such a measure of spread is the **standard deviation**. Like the mean understanding and 
   working with standard deviations is fundamental and will be encountered over and over
   again throughout the large field of statistics and applied work with data. You need
   to gain a good understanding of what the standard deviation is and how to compute it.

5. For larger datasets, which will be the usual case in practice, you will need to learn
   how to summarize and communicate the information about variables with lost of different
   values using the computer. This means in particular, that you need to understand how
   to read large datafiles from a given source, how to select and modify particular variables.
   You will thus learn some powerful tools from the R language that will help you to
   do so.

When you have worked through this unit you will be able to summarize large datasets
graphically and numerically and understand what these measures tell you about the
variation in the data, how they look like, what are the most typical values of the variables
and how much they are spread out around these values. In this process you will power up
your abilities to read, select and manipulate large data files and single variables using the
R language. This will enlarge your abilities to work with data by a huge and important step.

:::

When we analyze data, we usually have to look at lots of them. As a leading example,
let us take the case of anthropometric data. These are data measuring height and weight
of humans in a population.^[Anthropometry (This is a word with roots from ancient greek. 
In ancient greek anthropos means 'human', and m√©tron means 'measure'. Anthropometry thus 
refers to the measurement of the human individual. An early tool of physical 
anthropology, it has been used for identification, for the 
purposes of understanding human physical variation]. These data are usually gathered via
large population survey like the Demographic and Health Survey, abbreviated DHS^[The DHS 
program, a US based program on demographics and health in various parts of the 
developing world. The data are used to monitor demographic and health 
developments to decide where policy action might be needed. The DHS website 
can be found here: https://dhsprogram.com/ ] 

What's the point of recording the height and
weight of humans? 
First of all, human growth depends on good nutrition and the 
availability of medical services to 
effectively treat illness. Thus human height in a population has for a long time been
a subject of study. The average human height in a population varies with the general 
living standards. This fact makes data on human height especially interesting
for historians who study the history of living conditions. 
Because humans tend to get taller when
they have good living conditions, human height can reveal some information 
on living conditions, height
data can be an indirect measure of living conditions. This is especially interesting for
periods when very little or no data were collected and recorded.

But also tody information about the distribution of height and weight in a 
population enables countries 
to make data-driven decisions and to monitor their progress in improving 
nutritional status and achieving the United Nation's Sustainable Development Goals.^[
The Sustainable Development Goals (SDGs) or Global Goals are a collection 
of seventeen interlinked objectives designed to serve as a "shared blueprint for peace 
and prosperity for people and the planet, now and into the future." 
The goals were set and decided by the 
United Nations General Assembly and are monitored through various data collection 
efforts, including the DHS. The short titles of the 17 SDGs are: No poverty (SDG 1), 
Zero hunger (SDG 2), Good health and well-being (SDG 3),
Quality education (SDG 4), Gender equality (SDG 5), 
Clean water and sanitation (SDG 6), 
Affordable and clean energy (SDG 7), 
Decent work and economic growth (SDG 8), 
Industry, innovation and infrastructure (SDG 9), 
Reduced inequalities (SDG 10), Sustainable cities and communities (SDG 11), 
Responsible consumption and production (SDG 12), 
Climate action (SDG 13), Life below water (SDG 14), Life on land (SDG 15), 
Peace, justice, and strong institutions (SDG 16), Partnerships for the goals (SDG 17).
See also the website: https://sdgs.un.org/goals]

The measurement of human height and weight are also important in the history of statistics.
Especially important concepts describing the statistical relation between two variables,
were detected in this context. We will learn about these concepts later, when we study the
statistical tool of regression.

We will use a dataset from the DHS later in this chapter, when we discuss how to
use R to deal with large datasets. The number of observations in this case is 2559. This
is not very big but certainly too big to reasonably handle by hand. With a 
computer this becomes realistic. 

But before we do so, lets look at a smaller example of human height data.

## The histogram

To summarize data, statisticians often use a graph which is called a **histogram**. 
In this section we will discuss all you have to know about histograms and how to use them.

### Constructing a histogram

Since real world examples in this exposition come from anthropometry, let us 
look at a sample of 100 observations from a set of real world data about human adults
of age 18 and above, where the data record their height in cm.^[These data are collected from
the internet. For details you can look at the source at http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights] I have
stored these values in an R object which I called `heights`. So in this discussion I refer
to the 100 vaues shown below as `heights`.
```{r}
library(JWL)
dat <- socr_height_weight
dat$Height <- dat$Height*2.54

set.seed(1)
heights <- sample(x = dat$Height, size = 100) # randomly select 100 values without replacement.
heights
```
This is how R prints out the 100 height values. You already learned what the numbers in
brackets on the left side mean. They are counters or observations. In total we have
100 values.

We start the construction of a histogram by choosing for the horizontal 
axes ranges of numerical
values - in our case of the height data - which are called *bins* or *classes*. There is no
fixed rule as to how to choose the size of these ranges. These ranges should neither be too 
fine, nor too
coarse. While there are lists of mechanical rules, which you 
can for example find on Wikipedia^[See https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width], it is 
usually best to use your domain
knowledge and some experimentation to find out the bin size that works best for your data.

For this example, assume we had chosen a bin size of 5 cm. When you study 
the list, you will find that the lowest value is at 160.12 cm  while the highest value
is at 185.16. This is already quite tedious to find out by 
eyeballing the numbers with the small 
number of values we have chosen for this example. 
It is impossible to do for really large data sets.

Now lets make a distribution table like this:

| Height-bin | Frequency |
|-----------:|----------:|
| 160 -  165 |          7|
| 165 -  170 |         20|
| 170 -  175 |         39|
| 175 -  180 |         28|
| 180 -  185 |          5|
| 185 -  190 |          1|


In the column Height-bin we have recorded the bins in steps of 5 and in the right 
column, Frequency, we have recorded the count of values that are in this bin. 

When we make such a tabulation we have to agree on an endpoint convention. 
This is important, since 
when a height value would for instance be measured as exactly 165, in which bin 
should it be counted:
160-165 or 165-170? You, the constructor of the histogram, has to take this decision. Let us
agree on the convention that when a value falls exactly at the endpoint of the bin, we
put it in the next bin. In practice you
will usually do a histogram by computer. 
The code of the computer program has to specify an endpoint
convention, so the computer knows what to do when a value coincides with an endpoint.

On the Frequency axes you put the frequency scale: Counts of values. 
Then for each bin, you plot
a bar, which has the width of the bin and the height of the frequency. 
Do this for all the bins
you have tabulated and you are ready.

The histogram provides a certain aggregation of the data because it sorts 
the 100 data points into 6
bins, in our example. While loosing some local information on 
individual data points the global information
conveyed by the summary gives us a pretty good idea of the overall 
pattern of variation on the human height data.

:::{.callout-caution collapse="true"}
## Seitwerk: Expand for reading comment.
Reda, I think it would be great if you could do some animation that reproduces the 
steps: Plot a height axis with bins from 160 to 190 in bin sizes of 5, then plot a frequency
axes from 0 to 40 in steps of 10. Them from the list of 100 numbers, collect all that are
recorded in the first bin and then make the height proportional to the count, then the next
etc. The idea would be to realize something like my previous sketch for the Nile data
![Constructing the river flow histogram](pictures/nile_hist.png)
:::

We can see, for instance, that the most frequent height is between 170 and 175 and that the
variation is fairly symmetric around this bin. In the extremes this most frequent 
value can half or shrink even more, so there is quite some spread in the data.

If we had just plotted all individual data points, we would have got a picture like this.

```{r}
plot(as.numeric(heights), xlab = "Observation", ylab = "Height in cm", pch = 16)
```

You will agree that as a summary of the data it is not particularly useful and not
as informative as the histogram.

Histograms are such a common tool in statistics to explore the variation 
in one variable and the shape, how
it is roughly distributed that every statistical software has functions 
to produce histograms. 

In R, the
language we use in this course, there is also such a function. 
The function name is called `hist()` and it
takes the data as an argument. This is the second graphic function 
of R you encounter in this course
after we played with the `barplot()`function in the last lecture. 

To produce a histogram from the height data with R, we type, assuming that we have stored the
100 heights data we printed above in a variable called `heights`.

```{r}
#| code-fold: false

hist(heights)
```

:::{.callout-note icon=false}

### Now you try

Let us check your understanding of histograms by a little quiz now. The histogram below
shows the distribution of the final score in a certain class.

(a) Which block represents the people who scored between 60 and 80?
(b) Ten percent scored between 20 and 40 about what percentage scored between 40 and 60?
(c) About what percentage scored over 60?

![Final Score](pictures/final_score.png)
:::


### The relative frequency scale: Absolute versus relative frequency

Sometimes it might be useful, to choose a different scale for the y axes of 
your histogram. Instead
of absolute frequencies (or counts) it might be useful to show relative 
frequencies, the proportion
of occurrences in each bin. The type of scale you choose will depend on what kind of 
comparisons you want to emphasize about your data.

Assume you would want to draw your histogram such that on the y-axis you do not
see the absolute counts of how many individuals in your data fell into a certain
hight bin but you want to see instead these numbers as a percentage of the whole
dataset. 

This is sometimes convenient and in some cases more informative. This choice of
a different scale is called the **relative-frequency-scale**. The construction
of the histogram follows the same principles as we have already discussed but now
we just quantify the counts in the bins differently as shares of all observations.

Let's do the histogram for the heights data in a relative frequency scale. In
our table, which we used to construct the histogram before we had in total
100 observations. A count of 7 is therefore 7 % or 0.07 in the relative frequency
scale etc. Thus our table would now be:

| Height-bin | Frequency |
|-----------:|----------:|
| 160 -  165 |       0.07|
| 165 -  170 |       0.20|
| 170 -  175 |       0.39|
| 175 -  180 |       0.28|
| 180 -  185 |       0.05|
| 185 -  190 |       0.01|

and the histogram in relative frequency scale would look like this:
```{r}
h<-hist(heights, plot=F)
h$counts <- h$counts / sum(h$counts)
plot(h, freq=TRUE, ylab="Relative Frequency")
axis(2, at = c(0,0.1,0.2,0.3,0.4), labels = c(0,0.1,0.2,0.3,0.4))
```

Unfortunately R has no standard argument to its `hist()`function to draw a
histogram with a relative frequency scale. To produce a histogram like shown
here we would need some more advanced coding than we know at the moment.^[The `hist()` function in R has an argument, called `freq`. It can take the value `TRUE` or `FALSE`. If the
argument is set to `FALSE` the histogram is shown such that the y axis has yet another
scale, the so called density scale. This scale is chosen such that the total area of
the histogram is 1.] I will come back to explain how to do a histogram with relative
frequency scale in R as soon as we have learned the appropriate syntax to implement this.

### Best practices for histograms

When you summarize lots of data by a histogram there are some things you should 
consider carefully. Let us go through the most important best practice 
principles for histograms.

#### Bin size

When doing exploratory data work it is usually a good idea not to look at a single 
histogram but at several histograms of the same data by changing the bin size. There
is no clear rule about the optimal bin size. It often depends on context and field 
knowledge. 

If the bins are to fine, then the data will be be very noisy and give no overview because
they show too many individual points. On the other hand if the bins are too wide, they
will not show you the overall variability in the data very well and you fail to
get a good idea about the distribution. 

Let us illustrate this point using our heights data. 

In the first case we have chosen 100 bins, which is too fine. There is almost one bar
for every single data point. In this way we have a lot of spurious peaks and throughs and can
not see the variation pattern in the data very clearly

```{r}
hist(heights, breaks = seq(min(heights), max(heights), length.out = 100))
```
Now here we have the other extreme, lets assume we have only 3 bins. This would give us
a pattern like this.

```{r}
hist(heights, breaks = seq(min(heights), max(heights), length.out = 3))
```
Here the histogram is too coarse and we do not see the variation pattern either. 

The computer usually has a built in rule of thumb for the histogram 
which will work well in most
of the cases. Still for individual datasets it is sometimes better 
to choose a different bin size
that more adequately mirrors the variation in the data.

#### Choose boundaries that can be clearly interpreted

Tick marks and labels should fall on the bin boundaries. As in the examples discussed
so far, they need not be there for every tick but it is enough if they are there between
every few bars. Bin labels should also have not many significant digits, so they are easy to
read. So bin sized which divide 10 and 20 evenly are easier to read than bin sizes that do not.
So always take caution not to arbitrarily split bin sizes. Otherwise you can end up with
off bin boundaries.

For example, if we just took the maximum and the minimum of the heights data and
arbitrarily divided them into 7 bins, we would get the difficult to read bin boundaries

```{r}
seq(min(heights), max(heights), length.out = 7)
```

instead of the more easily readable boundaries

```{r}
seq(160,190,5)
```

#### What's the difference between a histogram and a bar chart?

A histogram depicts the frequency distribution of a continuous, quantitative variable, such 
as height, weight, time, energy consumption etc. These are variables that can take on any
value and these values can be ordered from smallest to highest.

When we have a categorical variable, like we encountered them in section 2, 
we need to use a bar chart.
The bars of the bar chart typically will have a small gap between the bars, emphasizing 
the discrete
nature of the variable. The categories in a bar chart usually have no natural ordering. 
As we discussed
in section 2, we have even to be conscious how we display the categories to 
avoid suggesting an
order that is in fact not there in the data.

## The average and the standard deviation

With a histogram we can summarize a large amount of data and get some insights about the
variation in the data. Often we can summarize data much more drastically by just one number
describing the center of the histogram and the spread around this center. When I write center
an spread here, these are just ordinary words with no special technical meaning.

When we do statistics we need 
precise definitions and we will study and learn these definitions in this section. 
The **average** is
often used to find the center. The **standard** deviation measures spread around 
the average.
^[When the distribution is not symmetric around the mean it is better to take other
maesures of center and spread. These alternative measures are called 
the **median**.  The **interquartile range** is another alternative measure of 
spreads. We will learn  the details about these measures later in the course.]

Before we go into these definitions, let me show for a start 
two histograms, both have the same center, but the second one is more spread
out, there are more observations farther away from the center.

```{r}
#| include: false

set.seed(1)
dat1 <- rnorm(1000, mean = 0, sd = 2)
dat2 <- rnorm(1000, mean = 0, sd = 6)

png("pictures/example_histogram1.png")
hist(dat1, xlab = "", main = "", xlim = c(-20, 20), breaks = 30)
abline(v = mean(dat1),                       # Add line for mean
       col = "red",
       lwd = 2)
abline(v = (mean(dat1) + sd(dat1)),
      col = "blue",
      lwd = 2)
abline(v = (mean(dat1) - sd(dat1)),
      col = "blue",
      lwd = 2)
dev.off()

png("pictures/example_histogram2.png")
hist(dat2, xlab = "", main = "", xlim = c(-20, 20), breaks = 30)
abline(v = mean(dat2),                       # Add line for mean
       col = "red",
       lwd = 2)
abline(v = (mean(dat2)+ sd(dat2)),
      col = "blue",
      lwd = 2)
abline(v = (mean(dat2) - sd(dat2)),
      col = "blue",
      lwd = 2)
dev.off()
```

::: {#fig-example-hist layout-ncol=2}

![Histogram 1](pictures/example_histogram1.png){#fig-ex-hist-1}


![Histogram 2](pictures/example_histogram2.png){#fig-ex-hist-2}

Histogram 1 and Histogram 2 have the same center but Histogram 2 is more spread out
:::

These distributions can be summarized by the center and the spread. But what about a situation like this?
```{r}
#| include: false 

mu1 <- log(0.03)   
mu2 <- log(50)
sig1 <- log(3)
sig2 <- log(2)
cpct <- 0.4   

bimodalDistFunc <- function (n,cpct, mu1, mu2, sig1, sig2) {
  y0 <- rlnorm(n,mean=mu1, sd = sig1)
  y1 <- rlnorm(n,mean=mu2, sd = sig2)

  flag <- rbinom(n,size=1,prob=cpct)
  y <- y0*(1 - flag) + y1*flag 
}

bimodalData <- bimodalDistFunc(n=100000,cpct,mu1,mu2, sig1,sig2)

png("pictures/bimodal.png")
hist(log(bimodalData), breaks = 50, xlab = "", main = "")
abline(v = mean(log(bimodalData)),                       # Add line for mean
       col = "red",
       lwd = 2)
abline(v = (mean(log(bimodalData))+ sd(log(bimodalData))),
      col = "blue",
      lwd = 2)
abline(v = (mean(log(bimodalData)) - sd(log(bimodalData))),
      col = "blue",
      lwd = 2)
dev.off()
```


![A bimodal distribution](pictures/bimodal.png){#fig-ex-hist-bm}

In some cases such distribution can occur naturally. 
Think, for example, about the distribution of the elevation data of surface area of the earth.
Most of the surface area of the earth is taken by the sea floor at about three 
miles below the sea level 
or the continental planes around sea level. If we would report only the center an the spread
of this histogram, plotted in the picture as the red and blue vertical lines, we would miss these peaks.

### The average

Let us come back to the height data, we have analyzed in a histogram before as the
context for which we study the concept of an average.

::: {.callout-note icon=false}

## Average

The average of a list of numbers is defined as their sum, divided by how
many numbers their are in the list.

:::

For example the average of the list $L$

```{r}
#| code-fold: false
L <- c(9,1,2,2,0)
```

would be computed as:

\begin{equation}
\frac{(9+1+2+2+0)}{5} = \frac{14}{5} = 2.8
\end{equation}

By now it will be no surprise for you that R provides a function for computing averages.
This function is known by the alternative name for the average, called the mean.

Here is how you would compute the mean using R.

```{r}
#| code-fold: false

mean(L)
```

which is indeed what we should get as a result.

Let's go back to the issue of human height data and take our data set of
100 observations of the height of adult humans. Let us recall
our histogram and give it some nice title and a precise label for the
x-axis:
```{r}
#| code-fold: false

hist(heights, xlab = "Height in cm", main = "Height of 18 year old humans")
```

The distribution looks symmetric. If we summarize these data by taking an average in one number
we will capture the center of the distribution fairly well. 
Let us compute the average, using the
mean function. 

```{r}
#| code-fold: false
average_height <- mean(heights)
average_height
```

The average height in this dataset is about `r average_height`  cm. 
The average is a very powerful way of
communicating data by compressing many observations into one single
number, the mean. 

This compression is, however, only achieved by loosing some 
information on individual differences.
For example in our dataset the average height is `r average_height` cm. 

But there are about `r mean(heights > 180)*100` % who are larger than
180 cm and there are also about `r mean(heights < 165)*100` % who are 
smaller than 165 cm. With 100 individuals these are
individuals who are beyond these thresholds. This diversity 
is hidden in the aggregation.

::: {.callout-tip}
## How to compute percentages of observations that fulfill a condition.

This is a good opportunity to show you a cool feature or trick in R how we could in one line
compute such percentages. Assume we want to compute the percentage of individuals in our data
who are larger than the mean of `r average_height`. How can we do this?

Logicals are a powerful data type to make such a computation. A logical is
a special data type, similar to the data type numerical, used for representing numbers, or
the data type character, representing strings. A logical can 
either be `TRUE` or `FALSE` and will be the output of an R operation testing a condition.

Say we have 10 numbers, given by

```{r}
#| code-fold: false
x <- c(4,5,1,2,4,2,0,10,11,6)
```

Now we could ask R which entries of x are larger than 2 by typing

```{r}
#| code-fold: false
x > 2
```

The output is a vector of logicals where R compares 
each entry in `x`with 2 and if it is larger it 
returns `TRUE` and otherwise `FALSE` (this includes all entries which are exactly 2). 

Note that, when you apply the function `mean()` to a vector of data of 
type logical R performs a computation by coercing `TRUE`to the number 1 and `FALSE` to the
number 0. We will learn about the details of these R the coercion rules later in more detail.
If you apply `mean()` to `x` you will get the proportion of values fulfilling a certain
condition. So if you take the mean of
```{r}
x > 2
```
R would firts transfrom the vector

`TRUE`,  `TRUE`, `FALSE`, `FALSE`, `TRUE`,  `FALSE`,  `FALSE`,  `TRUE`,  `TRUE`,  `TRUE`

into a vector

1,1,0,0,1,0,0,1,1,1.

When you take the mean
\begin{equation}
\frac{1+1+0+0+1+0+0+1+1+1}{10}
\end{equation}
you just get the proportion of 1, i.e. cases where the condition $x > 2$ is `TRUE`, because
the 0-es do not contribute to the value of the sum, only the 1-s.

This insight can be used to compute the percentage of values that 
fulfill a certain condition in R.
Here the proportions of heights larger and smaller than the mean.

```{r}
#| code-fold: false

mean(heights > mean(heights))
mean(heights < mean(heights))
```
Here you see the symmetry of the distribution numerically. About 50 % are larger and also 
smaller than the mean.
:::


### The standard deviation

When summarizing and communicating lots of data, it is useful not only to report 
at which value the center of the distribution is. It is often helpful to also 
think about the way how the values spread around the average. The quantity which
measures this spread is called the standard deviation. It can be interpreted as 
an average deviation. 

Le us go back to our data on human height. There were 100 observations
in our sample. The average height was `r round(mean(heights),2)` cm.

This average tells us that most of the humans measured in the sample had a height of
around 1 m and 73 cm. But there were deviations from this average. Some humans were
taller and others were smaller. We can ask how big these deviations are? In answering 
this question we need the concept of the standard deviation.

::: {.callout-note icon=false}

## Standard deviation

The standard deviation says how far away numbers on a list are from their
average. Most entries on the list will usually be somewhat around one
standard deviation from the average. Very few will be more than two or three
standard deviations away.

:::

In R you can compute the standard deviation by using the function `sd()`. So let us
compute the standard deviation of our height data using the `sd()`function.

```{r}
#| code-fold: false

sd(heights)
```

The standard deviation is at about `r round(sd(heights),2)` cm. That means 
that many humans differed from the
average height by about 1, 2, 3, 4 or about 5 cm. Let us use R to compute the percent
of observation that are within one standard deviation from the average.

Let us compute the percentage of observations, which are within 1 standard deviation
from the mean height. Do you remember the trick with using logicals?

Let us compute the percentage of observations which are larger than the average height minus
und standard deviation and smaller than the average height plus one standard deviation. The
R symbol for and is `&`. Let us use it to formulate the condition:

```{r}
#| code-fold: false

mean(heights >= mean(heights) - sd(heights) & 
     heights <= mean(heights) + sd(heights))
```

In our case these are 71 % of all observations.

Sometimes it is convenient to subtract the mean from every observation and 
and express the units in terms of standard deviations. By construction this
is a variable with mean 0 and standard deviation 1. This is called standardization
or normalization. Let me show you how this works.

We transform our heights data to 
\begin{equation*}
z_i = \frac{x_i - \mu}{\sigma}
\end{equation*}
and compute the mean of $z_i$ and its standard deviation. We can use R to do that:
```{r}
#| code-fold: false
z <- (heights - mean(heights))/sd(heights)

mean(z)
sd(z)
```
Now you see that the mean is 0 and the standard deviation is 1. 
The mean in the R computation is not exactly 0. R says it is `2.699057e-15`. What does this
mean? This is a way of writing 0.000000000000002699057 in a special notation. Do not worry for
now how this notation works exactly. What you can see is that this is an very very small 
number, practically the same as 0. In R it is not exactly zero because of rounding
errors of the computer. For practical purposed the value is as good as zero. 
This must be the case as a 
consequence of how we changed the units. The mathematically inclined among you might
try to derive this fact more generally using the definition of the mean and the standard
deviation.

Now if we ask the same question as before. Which proportions of $z$ are within
-1 and + 1 standard deviation from the mean, we get

```{r}
mean(z >= mean(z) - sd(z) & 
     z <= mean(z) + sd(z))
```

This is exactly the same value as we got before. Our transformation was just about
changing the units in which our data are measured. This change of unit does not change
the distribution. So no matter whether we work with $x$ the original heights in cm or
with the $z$ the normalized data, the distributional properties do not change. In 
statistics if data are normalized in this way the normalized values are also often
referred to as the **z-score**.

### Computing the standard deviation

Usually we will compute the standard deviation using the 
computer and only in rare cases will we
ever compute a standard deviation by hand. Still it is important that you understand how the
computation works and what is actually been computed. Here is how.

Example 1: Find the standard deviation of the list 20, 10, 15, 15

Step 1: We first need to find the average, which is
\begin{equation}
\frac{20 + 10 + 15 + 15}{4} = 15
\end{equation}

Step 2: We next need to find the deviation from the average. In order to do so, we just subtract
the average from each entry
\begin{eqnarray}
(20-15)&=5\\
(10-15)&=&-5\\
(15-15)&=&0\\
(15-15)&=&0
\end{eqnarray}

Step 3: Now we have to square each one of these differences and take the square root, which is 
often called the root mean square

\begin{eqnarray}
sd&=&\sqrt{\frac{5^2 + (-5)^2 + 0^2 + 0^2}{4}} \\
 &=& \sqrt{\frac{25 + 25 + 0 + 0}{4}} \\
 &=& \sqrt{\frac{50}{4}}\\
 &=& \sqrt{12.4}\\
 &\approx& 3.5
\end{eqnarray}

The standard deviation has the same units as the data. 
For example, when we measure height in cm then at the squaring step the units change to $cm^2$ but the squre root returns $cm$ again. 

::: {.callout-note icon=false}

## Now you try

1. Guess which of the following two lists has the larger standard deviation. Check your
guess by computing the standard deviation for both lists.

(i) 9,9,10,10,12
(i) 7,8,10,11,11,13

2. Can the standard deviation ever be negative?

3. For a list of positive numbers can the standard deviation ever be larger than the
average?

:::

## Bigger datasets and more R {#sec-moreR}

When we use the computer to summarize and communicate lots of data we need
to get to know more of R. In particular we need to understand how to read data
and select and manipulate variables. Let me lead you through some of the
most basic concepts you need to know in order to do so. Finally we will
apply our new skills to the hand on analysis of a real world anthropometric dataset
from the DHS survey that help us to get a global picture about the nutritional
situation for children and how this information could support us in setting
policy priorities.

### Vectors and indices

Say that for some reason we would be interested in the 50th observation in our data
on human height. How would we tell R to select this observation from our data?

Individual elements can be accessed by using an *index* or a set of *indices*. The
index is specified by using brackets `[]`. For example
```{r}
#| code-fold: false

heights[5]
```
gives the 5th element of our heights data. You can go to the beginning of this
chapter to verify that we have indeed captured this element. 5 is the index in this
example. To capture the 50the element, we would type
```{r}
#| code-fold: false
heights[50]
```

A very important function in R is the `c()`function. `c` is short for concatenate and
this is what the function does. It builds a vector putting several numbers together
in one R object. In this way we can write down several indices at once and tell R
for instance to read the 4th, the 40ies and the 67th element from our heights data like
this
```{r}
#| code-fold: false
heights[c(4,40,67)]
```

### Selecting more than one observation

Sometimes we need to retrieve consecutive numbers. For this case we need another
function, the colon operator or `:`. It helps us to build sequences of conscutive 
numbers.

For example
```{r}
#| code-fold: false
heights[c(2,3,4)]
```
is the same as
```{r}
#| code-fold: false
heights[2:4]
```
Say we wanted to know the average height and the standard deviation of the first 
50 observations in our data we could tell R to compute this by
```{r}
#| code-fold: false
mean(heights[1:50])
sd(heights[1:50])
```
Another very powerful and useful function we will need often in data analysis is
the `lenght()`function of R. It gives us the number of elements in a vector. Let's
check this with `heights()`
```{r}
#| code-fold: false
length(heights)
```
which gives us the answer to be expected.

### Logical subsetting, coercion rules and recycling

As we already learned in the context of computing proportions of
values fulfilling a certain condition by using logicals and the
coercion rules of R before, we can do similar things by composing
different functions.

Assume we want to know the number of observations with a height larger than 180 cm.
This could be computed by typing
```{r}
#| code-fold: false
sum(heights > 180)
```
You already know how this works. In the parentheses is a comparison. In this comparison R
checks for each entry in the vector `heights` whether it is larger than 180 or not.
For each observation where this is the case the output is `TRUE` and otherwise `FALSE`.

But how can such a comparison work? After all heights is a vector with 100 entries and
180 is just one number. A vector can not be compared to a number. Here some feature
of R comes into place which is called *recycling*. R makes `heights`and 180 similar 
by extending 180 to a vector with 100 of the same element 180 and then compare component
by component resulting in a vector with 100 logicals (i.e. `TRUE` or `FALSE`).

When the vector of logicals is given to the `sum()`function another internal rule
of R is applied, which is called *coercion*. In this case R transforms in the background
automatically the `TRUE` values into 1 and the `FALSE`values into 0. As a result you
will get the sum of all cases where the condition is fulfilled.

A related and also very useful function is the `which()`function. With this function
you can find out the indices of the observations, fulfilling a condition
```{r}
#| code-fold: false

which(heights > 180)
```
tells you that the elements 8 26 46 54 60 73 were larger than 180. And what were the
values of these 6 observations? Now you know already. It is
```{r}
#| code-fold: false
heights[which(heights > 180)]
```


### Data Frames

Data Frames are the next R workhorse when we deal with data. A data frame
is a rectangular table of data where every row corresponds to one data
point.

Instead
of using just some data frame let us this time use a real world example
of anthropometric data from the DHS.
Anthropometric data can help a country to get a data driven picture on the nutritional status
of its population. These data can help identify areas where policy action is needed.

#### DHS data on the nutritional status of children under 5 years

In the analysis we are going to produce a statistical table on the nutritional 
status of children under five years.
The anthropometric data reveal information about a 
persons nutritional status. The nutritional status of children is observed to 
monitor and measure malnutrition. The nutritional 
data of children are included in the United Nations development goals indicators.
They encompass children's sex, age, height-length, and weight. The nutritional 
indicators are sex and age specific because boys and girls grow at different rates. 
Their growth rates are also age dependent.

Four indicators on nutritional status are collected in DHS surveys:

(a) **Overweight**: This indicates high weight for height and is a measure of excess weight. 
This results from an imbalance of energy consumed (too much) and energy 
expended (too little). Children with this condition have an increased risk of 
non-communicable diseases, such as high blood pressure and diabetes and is 
associated with increased risk of being obese and overweight in adulthood.

(b) **Stunting**: This is low height-for-age. It is a measure of growth 
faltering and may result from a deficient growth environment, recurrent 
infections, chronic diseases and other causes. It is associated with 
impaired brain development and reduced academic achievement in childhood 
and lower economic potential as an adult.

(c) **Wasting**: Is low weight-for-height and is a measure of acute weight loss. 
This may result from inadequate food intake or from illness or infection. With 
this condition children are more susceptible for disease and have a 
higher risk of death.

(d) **Underweight**: Is low weight-for-age and is a measure of weight 
relative to a child's age. It reflects children who are stunted 
or wasted or both.

The World Health Organisation (WHO) child growth standards provide a single 
international standard that describes the physiological growth for all children 
from birth to age 5. The measurements of individual children are compared 
to the WHO Child Growth Standards to assess each child's growth. The WHO 
Growth Standards for children use a distribution that is expressed in units of 
standard deviations from the mean, also called z-scores. It is a distribution 
with a mean 0 and standard deviation 1. Within 1 standard deviation are 
about 67 % of all values, within 2 standard deviations it is 95 % and 
within 3 standard deviations 99.7%. 
Height for age, weight for height, and weight for age of children are expressed 
in these units.

Statistical cutoffs are used to measure malnutrition. If a child's height 
for age is below minus two standard deviations from the 
mean (expressed in z-scores), the child is considered stunted. If it is 
below minus three standard deviations from the mean it is considered severely 
stunted. If the weight-for-height z-score is below minus 2 standard 
deviations from the mean the child is considered wasted, if it is below 
three standard deviations from the mean it is considered severely wasted. 
If the weight for height z-score is above two standard deviations from 
the mean the child is considered overweight, if three standard deviations 
above it is considered obese. If the weight for height z-score of a child 
is below two standard deviations from the mean it is considered underweight, if 
three, severely underweight.

The WHO uses the following prevalence thresholds to assess the problems of 
stunting and wasting in a population of children:

|Classification|stunting         |wasting and overweight |
|:-------------|:----------------|:----------------------|
|very high     | 30% or more	   | 15 % or more          |
|high	         | 20 - 29 %	     | 10 - 14 %             |
|medium	       | 10 - 19 %	     |  5 - 9 %              |
|low	         |  2.5 - 9 %	     |  2.5 - 4 %            |
|very low	     | Less than 2.5 % | Less than 2.5 %       |

: WHO Prevalence thresholds for height and weight {.striped .hover}

Prevalence is the proportion of a population who have a specific characteristic 
in a given time period. In a survey prevalence is measured by the number of 
people in a sample who have the characteristic divided by all people in the sample.

Let us go now into the actual data and learn how to use the computer and R
as a tool.

#### Reading the DHS data using the JWL-package {#sec-readingdata}

The data we use in these notes and in our course are usually real world data collected
from a public source on the internet. Since we begin from scratch with explaining
the concepts of statistics as well as of R, we develop your R knowledge step by step 
and explain only at the end of the course how to read data into R from various outside
sources, from websites and in different file formats. While this is not difficult, it is tedious and
perhaps unnecessarily confusing for a beginner. We  have therefore packed all datasets used in this
course into a so called `R package`. R-packages are libraries, which add new functionalities to R, usually new
functions, which you yourself or somebody else has written. Packages must be installed once and then
loaded to be usable for your current R section. The package with the name `JWL` contains all datasets
used in the course and it has been preinstalled by your learning facilitators. You only have to load the
package, when you want to use data from it in your current R session.

The R function, loading a package is called `library()` and it takes the package name as an argument.
We load the DHS data now from the JWL package. We first load the package to make the data
available to R. For this you type at the console of at the input cell of your Jupyter Notebook

```{r}
#| code-fold: false
library(JWL)
```

Now the data are available to you. But how do I know the names of the datasets? R contains a function
`data()` where you can look up datasets that are bundeled in a package. For instance if you type

```{r}
#| code-fold: false
data(package = "JWL")$results[ , "Item"]

```

R shows you all the datasets that are in the `JWL` package. Don't worry for the moment about
the somewhat mysterious syntax of this R statement. Given your current knowledge of
R you cannot fully understand it at this moment. So just accept it as a command whith which 
you can look up datasets in a package. 

Our dataset from the DHS is called `cildren_nutrition_data`. Let's store them in an object
with a slightly simpler name, say `dhs_data`. How to store data in an R object, we have already learned
in the previous lecture. You invent a name and assign the values to this name using the 
assignment operator `<-`.

Let's call the object in which we save our data `dhs_data`
```{r}
#| code-fold: false
dhs_data <- children_nutrition_data
```


#### Inspecting the data

A useful function to inspect a data object like the one
we have just created is the `dim()` function. It will
tell us how many rows and columns our dataframe actually has
```{r}
#| code-fold: false

dim(dhs_data)
```

This means that our data have `r dim(dhs_data)[1]` rows and `r dim(dhs_data)[2]`
columns. Quite a bit larger than the `heights` dataset. Here we really need
a computer to analyze the infortion contained in these data.

Another useful function to inspect data is the `heads()`function. It displays by
default the first 6 rows of the dataframe
```{r}
#| code-fold: false
head(dhs_data)
```

Since there are 17 variables not all of them can be displayed on a single page-width.
R stacks the information in blocks. We see that some columns are numerical, some are
characters and some are logicals. The variable names give already some clues about
what they mean. For the exact meaning we would need some form of documentation.

#### Analyzing particular variables and subsets of data

Dollar signs `$` are used to adress individual columns. For example, assume we would like
to count the number of boys (i.e. sex = male) in our sample we could use this together
with R's coercion rules by coding
```{r}
#| code-fold: false
sum(dhs_data$sex == "male")
sum(dhs_data$sex == "female")
```

Observe a few things here: Operationally we have applied the tricks we have learned
about using logicals to count up cases for which a condition is true. The variable
`sex` in our data is coded by strings of chracters. We therefore need to put these
strings into `" "` quotation marks in the comparison. Note also that the identidy sign
in R is `==`. The symbol `=` in R is an assignment operator, which we can only use to
assign values to arguments in a function.

Indices in a dataframe are pairs specifying row and column numbers. For example
```{r}
#| code-fold: false

dhs_data[4,3]
```

is the data point in the dataframe which is at row 4 and column 3. In this case
it is a string of characters.

::: {.callout-tip}
## Remember
For any subset of a data frame `d`, we can extract whatever rows and columns we want using the format

`d[the rows we want, the columns we want]`
:::

Sometimes dataframes do not have column names. Still we can apply functions to
the vector of a column using indices. For example we could compute the mean of
the variable `nt_ch_waz`as

```{r}
#| code-fold: false
mean(dhs_data[ ,17])
```
Note the expression [,17]. Since there is a 17 in the second position, we are
talking about column 17. And since the first position, before the comma, is
empty, no rows are specified ‚Äî so *all* rows are included. That boils down to:
all of column 18.

A powerful feature of R is that we can extract data from a dataframe using the
same logic as we used when extracting data from a vector.

For example the follwoing code extracts variables 1, 3 and 5 and prints the first
four values of them
```{r}
#| code-fold: false

dhs_data[1:4, c(1,3,5)]
```
Note that in a dataframe all columns must have the same length. But as in our
dataset, every variable can contain a different data type, like numbers, characters
or logicals.

Now let us analyze the prevalence of children in our dataset that are wasted. Before
we do that let us explain that the data contain a variable, which is called `wt`.
This is short for weight. It means that each row is not counted as one observation
but is weighted. The sum of the weights
```{r}
#| code-fold: false
sum(dhs_data$wt)
```
would then give us the total number of children in the sample. Let us not go into
the details of where these weights are coming from and why they are needed at this
stage. The variable that encodes stunting is `nt_ch_stunt` and the variable that encodes
wasting is `nt_ch_wast`.

Now let us do the computation using the rules we have learned so far:
```{r}
#| code-fold: false
sum(dhs_data$nt_ch_stunt*dhs_data$wt)/sum(dhs_data$wt)
sum(dhs_data$nt_ch_wast*dhs_data$wt)/sum(dhs_data$wt)
```

The proportion of stunted children in our sample is 39.5 % and the proportion
of wasted children is 10 %. Thus according to the WHO classification the prevalence in 
stunting is very high and the prevalence in wasting is high.

We could use our new knowldge to answer other questions. Assume we would
want to find out which of our regions has the highest proportion in stunting?

First, let us find which rows refer to region 1, region 2 etc.
```{r}
#| code-fold: false

r1 <- which(dhs_data$region == "region 1")
r2 <- which(dhs_data$region == "region 2")
r3 <- which(dhs_data$region == "region 3")
r4 <- which(dhs_data$region == "region 4")
```
Now with these row indices we can compute the proportions on the
relevant subset of the data.
```{r}
#| code-fold: false

data_r1 <- dhs_data[r1, ]
data_r2 <- dhs_data[r2, ]
data_r3 <- dhs_data[r3, ]
data_r4 <- dhs_data[r4, ]

sum(data_r1$nt_ch_stunt*data_r1$wt)/sum(data_r1$wt)
sum(data_r2$nt_ch_stunt*data_r2$wt)/sum(data_r2$wt)
sum(data_r3$nt_ch_stunt*data_r3$wt)/sum(data_r3$wt)
sum(data_r4$nt_ch_stunt*data_r4$wt)/sum(data_r4$wt)
```
So we see that region 2 has the highest prevalence in stunting.

Often we want to extract data that fulfill more than just one condition. For instance
when we want to filter the values of a variable that fall in a particular interval we
need the values that are larger or equal to the lower bound of this interval and lower or
equal to the upper bound of the interval.

#### Combining several conditions

R allows us to combine conditions by logical operators. Assume we want to know the
proportion of children in our data that are wasted and stunted. In this case
both conditions need to be fulfilled. This would be coded in the following way:
```{r}
#| code-fold: false
sum((dhs_data$nt_ch_stunt & dhs_data$nt_ch_wast)*dhs_data$wt)/sum(dhs_data$wt)
```
Let me explain. The condition that a child is both stunted and wasted means that 
the stunting variable evaluates to `TRUE` and the wasting variable evaluates to 
`TRUE`. If this is the case `(dhs_data$nt_ch_stunt & dhs_data$nt_ch_wast)` is `TRUE`.
If one of these variables evaluates to `FALSE` then `(dhs_data$nt_ch_stunt & dhs_data$nt_ch_wast)` is `FALSE`.

We can do this with other logical operators. We can ask for example what is the 
proportion of children who are wasted or stunted. The operator here is `|` and 
`(dhs_data$nt_ch_stunt | dhs_data$nt_ch_wast)` evaluates to `TRUE` if the first
condition is `TRUE` or if the second condition is `TRUE` or if both are `TRUE`.
Let's do the computation:
```{r}
#| code-fold: false

sum((dhs_data$nt_ch_stunt | dhs_data$nt_ch_wast)*dhs_data$wt)/sum(dhs_data$wt)
```
a proportion that is very high.

## Summary

In this chapter you have learned how to summarize and communicate lost of data.
As an application context we have chosen measures of height and weight, or 
anthropometric data.

1. You have learned how to summarize data graphically by a histogram, how to
   construct a histogram and how to display the histogram with different scales: 
   With absolute    and relative frequencies.

2. You have learned to plot histograms in R and learned about things to care about when
   doing histograms, such as the choice of the bin size and choosing the boundaries of bins.
   
3. You have learned to summarize data by the mean and the standard deviation. 
   You have learned what these numbers mean and how to compute them by hand and, when you have    lots of data, by R.

4. You have learned new concepts in R that helped us to analyze a large dataset
   from the demographic and health survey by computing proportions of children 
   suffering from certain physical conditions. In particular the new R concepts you
   learned were
     (a) Reading data from a file
     (b) Using indices to extract particular values from a vector.
     (c) Selecting more than one observation.
     (d) Logical subsetting, coercion and recycling rules of R
     (e) Data frames and how to work with them.
     

## Exercises

### Exercises:

::: {.callout-note}
## Exercise 1: Histogram of monthly wages

A histogram of monthly wages for part-time employees is shown below (relative frequencies
are marked in
parenthesis). Nobody earned more than $1000 a month. The block over the class interval from 200
to 300 is missing. How tall must it be?

![Wages](pictures/wages.png)
:::

::: {.callout-note}

## Exercise 2: Which histogram is right?

Three people plot histograms for the weights of subjects in a study, using the relative
frequency scale. Only one is right. Which one and why?

```{r}

#| echo = false

library(JWL)

dat <- height_weight

data <- dat[dat$state == 1 & dat$sex == 1 & dat$age > 18, ]

hist_inf <- hist(data$height, plot = FALSE)         # Store output of hist function
hist_inf$density <- hist_inf$counts /    # Compute density values
  sum(hist_inf$counts) * 100

png(file="pictures/hight_version1.png")
plot(hist_inf, freq = FALSE, xlab = " ", ylab = " ", main = " ")


png(file="pictures/hight_version2.png")
plot(hist_inf, freq = FALSE, xlab = "hight (cm) ", ylab = "Percent per 5 cm ", main = " ")


png(file="pictures/hight_version3.png")
plot(hist_inf, freq = FALSE, xlab = "hight (cm) ", ylab = "5 cm per percent", main = " ")

```

::: {#fig-height_hist layout-ncol=3}
![Version 1](pictures/hight_version1.png){#fig-hight-hist-version1}

![Version 2](pictures/hight_version2.png){#fig-hight-hist-version2}

![Version 3](pictures/hight_version3.png){#fig-hight-hist-version3}

Three versions of a hight histogram of males over age 18
:::

:::

::: {.callout-note}

## Exercise 3: Different scales in a histogram 

An investigator draws a histogram for some height data, using the metric system. She is working in
centimeters (cm). She wants to draw the histogram in a so called density scale, i.e. in a scale that the area of all the bars sum to 1. The vertical axes shows relative frequency
and the top of the vertical axes is 10 percent per cm. Now she wants to convert to millimeter (mm). There are 10 millimeter to the centimeter. On the 
horizontal axis, she has to change 175 cm to ? mm, 200 cam to ? mm. On the vertical axis she has to change 10 percent per cm to ? percent per mm, and 5 percent per cm to ? percent per mm.

:::

::: {.callout-note}

## Exercise 4: Find the average

(a) The number 3 and 5 are marked by crosses on  the horizontal line below. Find the average of 
    these two numbers and mark it by an arrow.
    
    ![](pictures/average_a.png)
    
(b) Repeat (a) for the list 3,5,5

    ![](pictures/average_b.png)
    
(c) Two numbers are shown below by crosses on a horizontal axis. Draw an arrow
    pointing to their average.
    
    ![](pictures/average_c.png)

:::

::: {.callout-note}

## Exercise 5: Questions about averages

A list has 10 entries. Each entry is either 1, 2 or 3. What must the list be of the average is
1? if the average is 3? Can the average be 4?

:::

::: {.callout-note}

## Exercise 6: Comparing averages without computation

Which of the following lists has a bigger average? Or are they the same? try 
to answer without doing arithmetic.

(i) 10, 7, 8, 3, 5, 9   (ii) 10, 7, 8, 3, 5, 9, 11

:::

::: {.callout-note}

## Exercise 7: Adding data and the change of averages

1. Ten people in a room have an average height of 1.69 m. An 11th person is 1.96 enters
the room. Find the average height of all 11 people.

2. Twenty-one people in a room have an average height of 1.68. A 22nd person who is 1.96
enters the room. Find the average height of all 22 people.

3. Twenty-one people in a room have a height of 1.68. A 22nd person enters the room. 
How tall would he have to be to raise the average height by 1 inch.


:::

::: {.callout-note}

## Exercise 8: True or False ?

Diastolic blood pressure is considered a better indicator of heart trouble than systolic
pressure. The figure below shows age-specific average diastolic blood pressure for
men age 20 and over in a health survey from the US (HANES5 (2003-04)). True or false: The
data show that as men age, their diastolic blood pressure increases until age 45
or so. and then decreases. If false, how do you explain the pattern in  the graph?
(Blood pressure is measured in "mm" that is millimeter of mercury)

![](pictures/blood_pressure.png)
:::

::: {.callout-note}

## Exercise 9: Can you explain this?

Average hourly earnings in the US are computed each month by the Bureau fo Labor statistics using 
payroll data from commercial establishments. The Bureau figures the total 
wages paid out to non-supervisory personnel, and divides
by the total hours worked. During recessions, average hourly earnings typically go up. 
When the recession ends, average hourly earnings often start going down. How can this be?

:::

::: {.callout-note}

## Exercise 10: Working with standard deviations

The `socr_height_weight` about the height and weight of 18 year old humans, we had used in the
lecture before the average height in inches was about 173 cm and the standard deviation was about
5 cm.

(i) One individual was 188 cm. He was above average by how many standard deviations?
(ii) Another individual was 174.66 cm. She was above average by how many standard deviations?
(iii) A third individual was 1.5 standard deviations below the average height. He was how many cm?
(iv) If an individual was within 2.25 standard deviations of average height, the shortest height for this
individual would be how many cm? The highest height would be how many cm?

:::

::: {.callout-note}

## Exercise 11:  Match the height

(a) Here are the heights of 4 individuals. 150 cm, 130 cm, 180 cm, 172 cm. Match 
the heights with the description. A description may be used twice.

unusually short, about average, unusually tall

(b) About what percentage of individuals in the data had heights between 170.2 and 173.2 ? Between
    165.5 and 179.2?
    
:::

::: {.callout-note}

## Exercise 12: Recognizing the spread of lists
    
Each of the following lists has an average of 50. For which one is the spread of
the numbers around the average biggest? Smallest?

(i)     0, 20, 40, 50, 60, 80, 100
(ii)    0, 48, 49, 50, 51, 52, 100
(iii)   0,1,2,50,98, 99, 100

:::

::: {.callout-note}

## Exercise 13: Guess the standard deviation

Each of the following lists has an average of 50. For each one, guess whether the standard deviation is around 1, 2, or 10. (This does not require any arithmetic.)

(a)   49, 51, 49, 51, 49, 51, 49, 51, 49, 51
(b)   48, 52, 48, 52, 48, 52, 48, 52, 48, 52
(c)   48, 51, 49, 52, 47, 52, 46, 51, 53, 51
(d)   54, 49, 46, 49, 51, 53, 50, 50, 49, 49
(e)   60, 36, 31, 50, 48, 50, 54, 56, 62, 53

:::

::: {.callout-note}

## Exercise 14: Stylized histograms, average and standard deviation

Below are three sketches of three stylized histograms 
(we call them "sketches" because they display a schematic
shape of a hypothetical histogram. This is why these sketches do not look like real histograms.) 
Match the sketch with the description. Some descriptions will be left over. 
Give your reasoning in each case. The
symbol $\approx$ is the mathematical notation for *approximately*.

(i)   ave $\approx$ 3.5, sd $\approx$ 1       (iv) ave $\approx$ 2.5, sd $\approx$ 1
(ii)  ave $\approx$ 3.5, sd $\approx$ 0.5     (v)  ave $\approx$ 2.5, sd $\approx$ 0.5
(iii) ave $\approx$ 3.4, sd $\approx$ 2       (vi) ave $\approx$ 4.5, sd $\approx$ 0.5 

::: {#fig-histograms layout-ncol=3}

![Histogram 1](pictures/hist_ex_1.png){#fig-hist_ex_1}

![Histogram 2](pictures/hist_ex_2.png){#fig-hist_ex_2}

![Histogram 3](pictures/hist_ex_3.png){#fig-hist_ex_3}

Three stylized histograms
:::

:::

::: {.callout-note}

## Exercise 15: Estimating the size of average and standard deviation

One investigator takes a sample of 100 men age 18 - 24 in a certain town. Another one takes a sample
of 1000 such men.

(a)   Which investigator will get a bigger average for the heights of the men in his sample? Or should the average be about the same?

(b) Which investigator will get a bigger standard deviation for the heights of the men in his sample? or should the standard deviation be about the same for both investigators?

(c) Which investigator is likely to get the tallest of the sample men? Or are the chances about the same
for both investigators?

(d) Which investigator is likely to get the shortest of the sample men? Or are the chances about the same
for both investigators?

:::

### Exercises R

::: {.callout-note}

## Exercise 15: A histogram of the Nile river flow data

Base R comes bundled with certain datasets which are available to R when you start it. One of these
data are flow measurements of the river Nile at Aswan in Egypt. This dataset is called `Nile`. 

(a) Use the R help function to learn more about these data. Describe in words the meaning of the datapoints.

(b) Plot a histogram of the Nile river flow data.

(b) Change the Main title of the histogram to "Histogram of Nile river flow data at Aswan Egypt" and change the x axes label to "Annual flow is recorded are 100 millions of cubic meters". Hint: If you don't know how to go about this either use ?hist - the R help function - to find out. Alternatively you might try a search engine like google of bing and ask something like "changing title and labels in base R histogram".

:::

::: {.callout-note}

## Exercise 16: Analyze the full height dataset

Load the `JWL`library of datasets and store the variable `socr_height_weight` in a 
variable called `dat`. Use R's help function to learn what the data are.

1. The data show heights in inches and weights in pounds. Transform both variables to metric data
   before you continue. Note that 1 inch is 2.54 cm and 1 pound is 0.4535924 kg.
2. Plot a histogram of the heights in cm. Plot a histogram of the weight in cm.
3. Compute the mean and the standard deviation of both height and weight.
4. Compute the percentage of observations between plus 1 and minus 1 standard deviations from the mean.
5. Compute the percentage of observations between plus and minus 2 standard deviations from the mean. Do the same with three standard deviations.

:::

::: {.callout-note}

## Exercise 17: Energy consumption per capita accross countries in the world

1. Load the dataset `energy_consumption_per_capita` from the JWL package and use the R help function
to learn what this dataset describes.
2. Select the energy consumption in the year 2018 and plot a histogram of these data. What do you see?
3. How many countries consume more than 50.000 kilowatt-hours-per person per year? 
4. Which countries fulfill this condition?


:::

### Exercise Project:

To be added.
