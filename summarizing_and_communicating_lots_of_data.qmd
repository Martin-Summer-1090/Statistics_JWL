# Summarizing and communicating lots of data

When we analyze data, we usually have to look at lots of them. An example might be
income data gained from household surveys. Such a survey will contain a huge number of
data points, in the order of magnitude of ten thousands of data. These data need to be
summarized, to understand their main characteristics. 

In this unit you will learn the
most important tools for summarizing and communicating lost of data. You are going to
learn the principles how data summaries are constructed, what are the properties of
these summaries and what needs to be carefully considered. 

When we need to deal
with really large data sets, and most modern data sets are too large to be handled manually,
we will need the computer. We already did some first steps in R. In this unit we will build on these
first steps but enlarge them in a way that will enable you to deal and manipulate large
datasets on the computer. 

## Understanding variation in a single variable using histograms

### Constructing a Histogram

To summarize data, statisticians often use a graph which is called a **histogram**. In this section
we will discuss all you have to know about histograms and how to use them. Let us start by an example,
where we have about 100 data points, which is a lot but not that large that we can not handle them 
by hand.

The data we want to look at come from measurements of the annual flow of the river Nile at 
Aswan (formerly) Assuan in Egypt from 1871 to 1970. The units of these measurement in which the
annual flow is recorded are 100 millions of cubic meters, i.e. $10^8 m^3$. 

This is one of the data sets that is bundled with the R distribution and is available to all users of R. 
They are stored in an R object called `Nile`.^[When you type `data()` at the R console, you get a list
of all datasets that are available with the current distribution of R.]

This is how the data look like, when we print them to the R console using the R command
`print()`. The R function `options()` with the argument `width`just controls how the
numbers are printed. Here I made sure that they will fit in the width of the page.

```{r}
#| code-fold: false
options(width = 70)
print(Nile)
```

We start the construction of a histogram by choosing for the horizontal axes ranges of numerical
values - in our case of the river flow data - which are called *bins* or *classes*. There is no
fixed rule as to how to choose the size of these ranges. These ranges should neither be too 
fine, nor too
coarse. While there are lists of mechanical rules, which you can for example find on Wikipedia^[See https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width], it is usually best to use your domain
knowledge and some experimentation to find out the bin size that works best for your data.

For this example, assume we had chosen a bin size of 100^[Note that this will mean $100\times 10^8 m^3$ per year.]. When you study the list, you will find that the lowest value is at 456 while the highest value
is at 1370. This is already quite tedious to find out by eyeballing the numbers with the small 
number of values we have chosen for this example. It is impossible to do for really large data sets.

Now lets make a distribution table like this:

| Flow-bin   | Frequency |
|-----------:|----------:|
| 400 -  500 |          1|
| 500 -  600 |          0|
| 600 -  700 |          5|
| 700 -  800 |         20|
| 800 -  900 |         25|
| 900 - 1000 |         19|
|1000 - 1100 |         12|
|1100 - 1200 |         11|
|1200 - 1300 |          6|
|1300 - 1400 |          1|

In the column Flow-bin we have recorded the bins in steps of 100 and in the right column, Frequency, we
have recorded the count of values that are in this bin. 

When we make such a tabulation we have to agree on an endpoint convention. This is important, since 
when a flow value would for instance be measured as exactly 500, in which bin should it be counted:
400-500 or 500-600? You, the constructor of the histogram, has to take this decision. Let us
agree on the convention that when a value falls exactly at the endpoint of the bin, we
put it in the next bin. In practice you
will usually do a histogram by computer. The code of the computer program has to specify an endpoint
convention, so the computer knows what to do when a value coincides with an endpoint.

On the Frequency axes you put the frequency scale: Counts of values. Then for each bin, you plot
a bar, which has the width of the bin and the height of the frequency. 

Do this for all the bins
you have tabulated and you are ready.

The histogram provides a certain aggregation of the data because it sorts the 100 data points into 10
bins, in our example. While loosing some local information on individual data points the global information
conveyed by the summary gives us a pretty good idea of the overall pattern of variation on the Nile river
flow data. 

We can see, for instance, that the most frequent flow is between 800 and 900 and that the
variation is fairly symmetric around this bin. In the extremes this most frequent value can half or almost
double, so there is quite some spread in the data.

![Constructing the river flow histogram](pictures/nile_hist.png)

If we had just plotted all individual data points, we also got a picture, though you probably agree
that it is not particularly useful.

```{r}
plot(as.numeric(Nile), xlab = "Observation", ylab = "Annual Flow", pch = 16)
```

Histograms are such a common tool in statistics to explore the variation in one variable and the shape, how
it is roughly distributed that every statistical software has functions to produce histograms. In R, the
language we use in this course, there is also such a function. The function name is called `hist()` and it
takes the data as an argument. This is the second graphic function of R you encounter in this course
after we played with the `barplot()`function in the last lecture. 

To produce a histogram from the river flow data, we type at the console

```{r}
#| code-fold: false

hist(Nile)
```

:::{.callout-note icon=false}

### Now you try

Let us check your understanding of histograms by a little quiz now. The histogram below
shows the distribution of the final score in a certain class.

(a) Which block represents the people who scored between 60 and 80?
(b) Ten percent scored between 20 and 40 about what percentage scored between 40 and 60?
(c) About what percentage scored over 60?

![Final Score](pictures/final_score.png)
:::

### The density scale: Absolute versus relative frequency

Sometimes it might be useful, to choose a different scale for the y axes of your histogram. Instead
of absolute frequencies (or counts) it might be useful to show relative frequencies, the proportion
of occurrences in each bin. The type of scale you choose will depend on what kind of 
comparisons you want to emphasize about your data.

Let us look at this issue by an example. The numbers we want to look at report energy consumption per
capita in kwh per person per year for different countries around the world.^[A kilowatt-hour, known as Kwh
is a way to measure how much energy is used. A kilowatt-hour is the amount of energy used if
a 100 watt appliance is kept running for an hour. For instance, if you turned on a 100 watt bulb 
for one hour you are using a kilowatt-hour of energy. What’s the difference between kilowatt 
vs. kilowatt-hour? A kilowatt is 1,000 watts, which is a measure of power. A kilowatt-hour is a 
measure of the amount of energy a certain machine needs to run for one hour. So, if you have 
a 1,000 watt drill, it takes 1,000 watts (or one kW) to make it work. If you run that 
drill for one hour, you’ll have used up one kilowatt of energy for that hour, or one kWh. 
Obviously, every appliance will use a different amount of power. Here are some of the 
usages for some items in a home: 50″ LED Television: around 0.016 kWh per hour,
electric water heater: 380-500 kWh per month] 
The energy numbers 
refer to primary energy – the energy input before the transformation to forms of energy for 
end-use (such as electricity or petrol for transport).

Let us look at the
year 2019.

```{r}
library(JWL)
library(ggplot2)

dat <- with(energy_consumption_per_capita, energy_consumption_per_capita[Year == 2019, ])

hist_info <- hist(dat$Cons, plot = FALSE)         # Store output of hist function
hist_info$density <- hist_info$counts /    # Compute density values
  sum(hist_info$counts) * 100
plot(hist_info, freq = FALSE, xlab = "Primary energy consumption in kilowatt-hours per person per year.", ylab = "Percent", main = "Primary energy Consuption per Capita 2019")              # Plot histogram with percentages
```
In this histogram you see the distribution of per capita primary energy consumption for the year
2019 for countries around the globe. But now the y axes shows relative frequencies instead of counts.

For example, you see from the graph that roughly 55 % of countries have a primary energy consumption 
smaller that 20.000 kilowatt hours per person in this year. The next larger bucket contains already 
roughly half or 24 %. The biggest buckets are then a very small fraction of countries in the world. This
means that there is a relatively small share of counrties, around 20 %, which have a large
primary energy consumption per capita. One says in the language of statistics that the
distribution of per capita primary energy consumption is skewed. 

When you have a histogram with a density or relative frequency scale, the total area sums to 100 (or to 1
depending on how you express the percentages, i.e. whether you express them as 10 % or as 0.1.).

### Exercises: Now you try

1. A histogram of monthly wages for part-time employees is shown below (densities are marked in
parenthesis). Nobody earned more than $1000 a month. The block over the class interval from 200
to 300 is missing. How tall must it be?

![Wages](pictures/wages.png)
2. Three people plot histograms for the weights of subjects in a study, using the density scale. Only one is right. Which one and why?

```{r}

#| echo = false

library(JWL)

dat <- height_weight

data <- dat[dat$state == 1 & dat$sex == 1 & dat$age > 18, ]

hist_inf <- hist(data$height, plot = FALSE)         # Store output of hist function
hist_inf$density <- hist_inf$counts /    # Compute density values
  sum(hist_inf$counts) * 100

png(file="pictures/hight_version1.png")
plot(hist_inf, freq = FALSE, xlab = " ", ylab = " ", main = " ")


png(file="pictures/hight_version2.png")
plot(hist_inf, freq = FALSE, xlab = "hight (cm) ", ylab = "Percent per 5 cm ", main = " ")


png(file="pictures/hight_version3.png")
plot(hist_inf, freq = FALSE, xlab = "hight (cm) ", ylab = "5 cm per percent", main = " ")

```

::: {#fig-height_hist layout-ncol=3}
![Version 1](pictures/hight_version1.png){#fig-hight-hist-version1}

![Version 2](pictures/hight_version2.png){#fig-hight-hist-version2}

![Version 3](pictures/hight_version3.png){#fig-hight-hist-version3}

Three versions of a hight histogram of males over age 18
:::

3. An investigator draws a histogram for some height data, using the metric system. She is working in
centimeters (cm). The vertical axes shows density and the top of the vertical axes is 10 percent per cem. Now she wants to convert to millimeter (mm). There are 10 millimeter to the centimeter. On the 
horizontal axis, she has to change 175 cm to ? mm, 200 cam to ? mm. On the vertical axis she has to change 10 percent per cm to ? percent per mm, and 5 percent per cm to ? percent per mm.

4. In a Public Health Service study, a histogram was plotted showing the number of cigarettes per day smoked by each subject (male current smokers), as shown in the histogram below. The density is marked in
parentheses. The class interval include the right endpoint, not the left.

   (a) The percentage who smoked 10 cigarettes or less per day is around:

        1.5%    15%   30%   50%

   (b) The percentage who smoked more than a pack a day, but not more than 2 packs, is around

        1.5%    15 %    30%   50%
        (There are 20 cigarettes in a pack)

   (c) The percentage who smoked more than a pack a day is around

        1.5%,   15%,   30%,   50%

   (d) The percent who smoked more than 3 packs a day is around

        0.25 of 1%,    0.5 of 1%,     10 %

   (e) The percent who smoked 15 cigarettes per day is around

        0.35 of 1%,    0.5 of 1%,   1.5%,    3.5%,    10%
        
![Number of cigarettes](pictures/cigarettes.png){#fig-hight-cigarettes}

### Best practices for histograms

When you summarize lots of data by a histogram there are some things you should 
consider carefully. Let us go through the most important best practice 
principles for histograms.

#### Bin size

When doing exploratory data work it is usually a good idea not to look at a single 
histogram but at several histograms of the same data by changing the bin size. There
is no clear rule about the optimal bin size. It often depends on context and field 
knowledge. 

If the bins are to fine, then the data will be be very noisy and give no overview because
they show too many individual points. On the other hand if the bins are too wide, they
will not show you the overall variability in the data very well and you fail to
get a good idea about the distribution. 

Let us illustrate this point using the river flow data of the Nile. 

In the first case we have chosen 100 bins, which is too fine. There is almost one bar
for every single data point. In this way we have a lot of spurious peaks and throughs and can
not see the variation pattern in the data very clearly

```{r}
hist(Nile, breaks = seq(min(Nile), max(Nile), length.out = 100))
```
Now here we have the other extreme, lets assume we have only 3 bins. This would give us
a pattern like this.

```{r}
hist(Nile, breaks = seq(min(Nile), max(Nile), length.out = 3))
```
Here the histogram is too coarse and we do not see the variation pattern either. 

The computer usually has a built in rule of thumb for the histgram which will work well in most
of the cases. Still for individual datasets it is sometimes better to choose a different bin size
that more adequately mirrors the variation in the data.

#### Choose boundaries that can be clearly interpreted

Tick marks and labels should fall on the bin boundaries. As in the examples discussed
so far, they need not be there for every tick but it is enough if they are there between
every few bars. Bin labels should also have not many significant digits, so they are easy to
read. So bin sized which divide 10 and 20 evenly are easier to read than bin sizes that do not.
So always take caution not to arbitrarily split bin sizes. Otherwise you can end up with
off bin boundaries.

For example, if we just took the maximum and the minimum of the Nile river flow data and
arbitrarily divided them into 7 bins, we would get the difficult to read bin boundaries

```{r}
seq(min(Nile), max(Nile), length.out = 7)
```

instead of the more easily readable boundaries

```{r}
seq(400,1200,100)
```

#### What's the difference between a histogram and a bar chart?

A histogram depicts the frequency distribution of a continuous, quantitative variable, such 
as height, weight, time, energy consumption etc. These are variables that can take on any
value and these values can be ordered from smallest to highest.

When we have a categorical variable, like we encountered them in section 2, we need to use a bar chart.
The bars of the bar chart typically will have a small gap between the bars, emphasizing the discrete
nature of the variable. The categories in a bar chart usually have no natural ordering. As we discussed
in section 2, we have even to be conscious how we display the categories to avoid suggesting an
order that is in fact not there in the data.

## Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values {#sec-moreR}

Before we go on with learning the tools to summarize and communicate lost of data let us
gain more skills for handling the tool that will actually enable us to handle
large data sets. Building on what we learned in the last unit, we now expand
our knowledge of R bit further.

### Reading data in R {#sec-readingdata}

Before we can do anything with data, we need first to learn how  to load data into R and how
to save them. We will discuss now how to do this with comma separated
text files. R provides packages for reading and writing from almost any other format, like data stored in
Excel files and many more data formats that are used today. There is an option for every data format. 
Since in all those cases the same principles apply as in the csv case, it is sufficient if we stick
to the csv case.

We have discussed a data set of per capita primary energy use in countries around the globe before to
produce a histogram of these data for the year 2019. How did I get these data into R?

First of all I could access these data because helpful people at Oxford University in the UK who maintain
and run the website "Our world in data", which we have encountered before store these data on their
website. In the concrete case of the energy data, they can currently be found at
https://ourworldindata.org/grapher/per-capita-energy-use
where you can download the data from the page. 

I have taken a screenshot here

![Our World in Data website energy](pictures/our_world_in_data_energy.png)

In the lower right corner you see a box called Data and a download button. This butto allows you to 
download the dataset to your machine. The file is called `per-capita-energy-use.csv` and I have stored
the file in a subfolder to the directory in which I am writing these lecture notes. If you decide to
download this file, you will save it somewhere on your machine where you find it appropriate. Perhaps
you have a folder for this course and in this folder you have a subfolder where you store all the
data sets we are using in the course.

To read a plain text or csv file, R provides the function `read.csv()`. If the csv file
comes with a European instead of an US decimal format (`,` instead of `.` for the
decimal sign.) you need to use the function `read.csv2()`.

In the simplest form you read the data and write them to an object
you can work with in R. For this you need to type something like this: Let's call the
object in which we save our data `energy_consumption`, then by calling the function `read.csv()` with the
path to your file as an argument will read the data from your local folder and store them in the
object we have created. This allows us to refer to the data for doing further computations.
```{r}
#| code-fold: false
energy_consumption <- read.csv("data/energy_use_per_capita/per-capita-energy-use.csv")
```
The function needs as an argument the file name. If the file is in a sub-folder
of  the current directory you need to also specify the path.
To specify the correct path to the file
you need to know in which part of your directory tree you are currently working.

In my
case I am working in the project folder for my lecture notes, which has a sub-folder called
`data`. The data subfolder has a further subfolder called `energy_use_per_capita` in my case
and thus I specify the path relative to this location. 

To find out what is your current
R working directory, R provides the function `getwd()`. If I type this in my case, I will get
```{r}
#| code-fold: false
getwd()
```
the path of my project folder for this lecture notes. So if I type the string
`"data/energy_use_per_capita/per-capita-energy-use.csv"` this specifies the path relative to my working directory.

If you read the file on your computer, you need to specify the path appropriately from where
you are working in R at the moment to where you have stored the csv file.

Now `read.csv()` has many additional arguments, which provide you with lots of
flexibility. I encourage you to check it out and play with it using the help function and
the examples given therein by typing `?read.csv` at the prompt.

We have now read the primary energy consumption data and written it to the 
R-object `energy_consumption`. Lets
inspect the object a bit to see what we've got. I use the function `head()` with
the parameter value `n = 10`. This will show me the first 10 rows of the datafile.
```{r}
#| code-fold: false
options(width = 120)
head(energy_consumption, n = 10)
```
This gives you an idea what the data look like. There are four variables, called `Entity`, `Code`, `Year` and `Primary.energy.consumption.per.capita..kWh.person.` The last variable name is very informative
but also very long and unpractical. We will learn how to change variable names soon. Because of the
long name, I had to use the function `options()` before `heads()` to tell R to use a sufficiently wide
display. Don't worry for this detail at the moment.

### R objects {#sec-robjects}

The most basic type of R objects are **atomic vectors**. Most structures in R are built
from atomic vectors. The energy consumption data-file we have just loaded is an example of such
a more complex structure built from atomic vectors. We have already encountered a few of
those in our previous lecture.

#### Atomic vectors {#sec-atomic}

An atomic vector is just a simple vector of data. For example remember when we typed the
infant mortality data for eight European countries for 1860 we typed
```{r}
#| code-fold: false
mr_1860 <- c(0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)
```
In this case `mr_1860` is an atomic vector. 


R has a function, which allows you to check whether an object is an atomic vector or not.
This function is called `is.vector()`. It takes the object name as an argument and returns `TRUE`
if the object is an atomic vector and `FALSE` if it is not. For example:
```{r}
#| code-fold: false
is.vector(mr_1860)
```
does indeed return `TRUE`.

Each atomic vector stores values in a one-dimensional vector, and each atomic vector can
only store one type of data. The length of the atomic vector can be determined by the
function `length()` This function takes an R object, which is an atomic vector, as an
argument and returns the number of elements in this vector. Here is the example of the die
```{r}
#| code-fold: false
length(mr_1860)
```
which is 8 as it should be. An atomic vector could also have only one element, in which case
`lenght()`would return 1.

Now altogether R has implemented six basic types of atomic vectors:

1. double

2. integers

3. characters

4. logical

5. complex

6. raw

We will not encounter complex and raw data-types in this course, so let us skip those and discuss
only the first 4 types.

<!-- If yo u go back to our stock data and look at the first three lines -->
<!-- ```{r apple-price-1-3} -->
<!-- head(aapl_prices, n=3) -->
<!-- ``` -->
<!-- The variables open, high, close, volume and adjusted are all stored  -->
<!-- as the first R data type: **Doubles**. A double vector stores regular numbers. It seems -->
<!-- natural to store such quantities as opening and closing prices as doubles. Doubles can be -->
<!-- positive or negative, they can have digits right of the decimal place. If not -->
<!-- explicitly told otherwise, R saves any number you give it as a double. Note that some functions -->
<!-- refer to doubles with the more intuitive term numeric. It has an equivalent meaning in R. -->

<!-- For example the number of traded shares in our apple stock price file is saved as a double. But -->
<!-- arguably the number of shares could also have been saved as an **Integer**. This is the second -->
<!-- data type in R for number which can be written without decimals. If you explicitly specify -->
<!-- number as integers, you need to append the letter `L` to the number. Say, for example,  -->
<!-- we would want to specifie the points shown on our die explicitly as integers, then we would type -->
<!-- ```{r die-int} -->
<!-- die_int <- c(1L,2L,3L,4L,5L,6L) -->
<!-- ``` -->
<!-- Now R will save the die with integer values. Before, since we did not explicitly specify it,  -->
<!-- the values were stored probably as doubles. You can check the type always  -->
<!-- with the function `typeof()`: -->
<!-- ```{r check-type} -->
<!-- typeof(die_int) -->
<!-- ``` -->
<!-- Now why should we care for distinguishing integers from doubles? This has to do with the -->
<!-- way a computer does computations. Sometimes a difference in precision can have surprising effects. -->
<!-- In your computer 64bits of memory are allocated for each double in an R program. While this  -->
<!-- allows for a very precise representation of numbers not all numbers can be exactly represented  -->
<!-- with 64-bits. The famous candidates are $\pi$, which has an infinite sequence of digits and must -->
<!-- therefore be rounded by the computer. Usually the rounding error introduced into your -->
<!-- computations will go unnoticed but sometimes surprises can occur. Take for instance: -->
<!-- ```{r sqrt-4-minus-2} -->
<!-- sqrt(2)^2 - 2 -->
<!-- ``` -->
<!-- Why is that? The square root of 2 can not be expresses precisely because, as already  -->
<!-- the old Greeks -->
<!-- knew, it is not a rational number. And voila, you have a small rounding error. Such errors -->
<!-- are called *floating point errors* in computer science lingo and computing with such numbers is -->
<!-- called *floating-point-arithmetic*.  -->

<!-- With integers floating point errors are avoided, but for many applications this is not an  -->
<!-- option. Luckily for most cases floating-point arithmetic provides sufficient precision -->
<!-- for most of the applications we encounter in practice. -->

<!-- When you look at the first column of our stock price data you see the ticker symbol of the  -->
<!-- apple stock `AAPL`. This is the symbol of the -->
<!-- third data type: **Character**. A character vector stores strings of text, which have to  -->
<!-- be put between quotation marks `""`. Strings are the individual elements of a character vector. -->
<!-- We have encountered character vectors already in this lecture, where we represented the -->
<!-- fair coin with the vector `c("H", "T")` with the string `"H"` for head and the string -->
<!-- `"T"` for tail. -->

<!-- Note that a string can be more than just letters. If you type, for instance the number `1` with -->
<!-- quotation marks, like `"1"` R would interpret the value as a string not as a number. Sometimes -->
<!-- one can get confused in R because both objects and characters appear as text in R code. Object -->
<!-- names are without quotation marks strings always are between quotation marks. -->

<!-- The next data type in the list are: **Logicals**. Logical vectors store `TRUEs` and `FALSES`. They -->
<!-- are extremely useful for doing comparisons and - as we will see shortly - also for -->
<!-- subsetting. -->

<!-- For example, if you type: -->
<!-- ```{r comp} -->
<!-- 0 > 1 -->
<!-- ``` -->
<!-- R tells you that this statement is False.  -->

<!-- Whenever you type `TRUE` of `FALSE` without quotation marks, R will treat the -->
<!-- input as logical data. For instance, the following statement yields: -->
<!-- ```{r logical-inputs} -->
<!-- logic <- c(TRUE, FALSE, TRUE) -->
<!-- logic -->
<!-- ``` -->
<!-- You can check the type: -->
<!-- ```{r check-logic} -->
<!-- typeof(logic) -->
<!-- ``` -->

<!-- One important R-fact which you need to know about atomic vectors is that atomic vectors -->
<!-- can have **attributes**. Attributes won't affect the values of an object but can hold -->
<!-- and store object metadata. Normally we do not look at these metadata, but many R functions -->
<!-- check for attributes and then do special things with the object depending on these  -->
<!-- attributes. -->

<!-- Attributes can be checked with the function `attributes()` using an R object as an argument. -->
<!-- ```{r attrib} -->
<!-- attributes(die) -->
<!-- ``` -->
<!-- Since `die`has no attributes R returns `NULL`. This value is often returned by functions whose  -->
<!-- values are undefined. -->

<!-- The most common attributes for atomic vectors are **names**, **dimensions** and **classes**. -->
<!-- Each of these attributes has its own helper function that you can use to give attributes to -->
<!-- the object.  -->

<!-- When you look up the value for the die you will get -->
<!-- ```{r names-of-die} -->
<!-- names(die) -->
<!-- ``` -->
<!-- We see that our die object has no names, but we could assign some. For example: -->
<!-- ```{r assign-names-to-die} -->
<!-- names(die) <- c("one", "two", "three", "four", "five", "six") -->
<!-- ``` -->
<!-- Now the die has a name attribute: -->
<!-- ```{r show-die-name} -->
<!-- attributes(die) -->
<!-- ``` -->
<!-- Note that the values are not affected nor will the names be affected when the  -->
<!-- values are manipulated. For instance, if you type -->
<!-- ```{r increase-values} -->
<!-- die + 1 -->
<!-- ``` -->
<!-- the names stay what they were and all values are increased by 1. To remove the names -->
<!-- you can type: -->
<!-- ```{r remove-names} -->
<!-- names(die) <- NULL -->
<!-- ``` -->

<!-- One very important attribute, we will encounter all the time is **dimension**, with the -->
<!-- helper function `dim()`. For example we can look at our data object of stock prices -->
<!-- to get: -->
<!-- ```{r dim-in-action} -->
<!-- dim(aapl_prices) -->
<!-- ``` -->
<!-- which returns two numbers, which mean that the object has 8044 rows and 8 columns.  -->

<!-- But with dim we can also transform our die into a matrix, with say two rows and three columns: -->
<!-- ```{r transform-die-to-matrix} -->
<!-- dim(die) <- c(2,3) -->
<!-- die -->
<!-- ``` -->
<!-- R will always use the first value in dim for the rows and the second for the columns. dim can -->
<!-- be generalized to higher dimensional array structures. -->

<!-- If you want to have more control on how R fills up the values, you can use a richer -->
<!-- helper function, which gives you more control. For two dimensional data arrays this -->
<!-- is provide by the function `matrix()`. Say you type -->
<!-- ```{r construct-matrix} -->
<!-- m <- matrix(die, nrow = 2) -->
<!-- m -->
<!-- ``` -->
<!-- the matrix will be filled up by column. You can change that behavior by setting the `byrow`  -->
<!-- argument to `TRUE`. -->
<!-- ```{r construct-matrix-with-byrow-true} -->
<!-- m <- matrix(die, nrow = 2, byrow = TRUE) -->
<!-- m -->
<!-- ``` -->

<!-- Notice that changing the dimensions of an object, will not change its type. But it will -->
<!-- change the object's **class** attribute: -->
<!-- ```{r class-attribute-behavior} -->
<!-- dim(die) <- c(2,3) -->
<!-- typeof(die) -->
<!-- class(die) -->
<!-- ``` -->
<!-- A class in R is a special case of an atomic vector.  -->

<!-- With the attribute system R allows you to represent more data types. R uses, for example -->
<!-- a special class to represent dates and times. The data variable in our stock data is - for -->
<!-- example - represents as a type of this kind. -->

<!-- To illustrate this we take the R-function `Sys.time()`. This function returns the -->
<!-- current time on your computer. It looks like a character string -->
<!-- when you display it but it is actually a double with class `POSIXct`, `POSIXt` (it has -->
<!-- two classes): -->
<!-- ```{r time-type} -->
<!-- now <- Sys.time() -->
<!-- now -->
<!-- typeof(now) -->
<!-- class(now) -->
<!-- ``` -->
<!-- `POSIXct` is a widely used framework for representing dates and times. But we will skip the -->
<!-- details here. -->

<!-- R stores categorical data, such as nationality, sex etc. by using **factors**. If -->
<!-- you take for instance, sex, it can have only two values - male or female - and these -->
<!-- values may have their idiosyncratic order, for example that females go always first. -->
<!-- To make a factor in R you have to pass an atomic vector to the `factor()` function. -->
<!-- This function works by recoding the values in the vector as integers and store the  -->
<!-- results in an integer vector. R also adds a `level` attribute which contains the set -->
<!-- of labels and their order and a class attribute that says the vector is -->
<!-- a factor. Example: -->
<!-- ```{r sex-factor} -->
<!-- sex <- factor(c("m", "f", "f", "m")) -->
<!-- typeof(sex) -->
<!-- attributes(sex) -->
<!-- ``` -->
<!-- Factors can be confusing since they look like characters but behave like integers. -->

<!-- Note that R will often try to convert character strings to factors when you load and create data. -->
<!-- I recommend that you do not allow R to make factors unless you explicitly ask for it. This -->
<!-- can usually be controlled by an argument to whatever the data reader function is. For instance -->
<!-- you can give the `read.csv()` function the argument `stringsAsFactors = FALSE`.  -->

<!-- R has an internal **coercion** behavior for data types, which you should know -->
<!-- about if you work with R. With this knowledge you can do many useful things. -->

<!-- If a character string is present in an atomic vector, R will automatically convert every -->
<!-- other component in this vector to a character string. If a vector contains only logicals  -->
<!-- and numbers, R will convert the logicals to numbers. In this case every `TRUE` becomes a 1 and -->
<!-- every `FALSE` becomes a 0. -->

<!-- R also uses the coercion rules, when we do math with logicals, like for example -->
<!-- ```{r math-with-logicals} -->
<!-- sum(c(TRUE, TRUE, FALSE, FALSE)) -->
<!-- ``` -->
<!-- What happens here is that R coerces the vector `c(TRUE, TRUE, FALSE, FALSE)` to the  -->
<!-- vector `c(1, 1, 0, 0)` and sums the components. -->

<!-- Going back to our data set on the apple stock price, we see that this dataset stores  -->
<!-- values of different types, characters, dates and doubles. How does R achieve this? -->

<!-- The answer is that this is achieved by a data structure called a **list**. List are -->
<!-- like atomic vectors, because the group data into a one-dimensional set. However, lists do -->
<!-- no group together individual values. List group together R objects, such as atomic -->
<!-- vectors or even other lists.  -->

<!-- For example, you can create a list, which contains a numeric vector of length 31 in its -->
<!-- first element, a character vector of length 1 in its second element and a new list of length 2 in its third. This is done by using the `list()`function of R, like this: -->
<!-- ```{r create-list} -->
<!-- list_example <- list(100:130, "R", list(TRUE, FALSE)) -->
<!-- list_example -->
<!-- ``` -->
<!-- The double bracketed indices tell you which element of the list is being displayed. The -->
<!-- single bracketed indices tell you which sub-element of the list is being displayed. For example -->
<!-- 100 is the first subelement of the first element in the list. "R" is the first subelement of the -->
<!-- second list element. -->

<!-- **Data Frames** are the two dimensional version of a list. They are by far the most -->
<!-- useful storage structure for data analysis. Indeed, our dataset on the Apple stock -->
<!-- price we have loaded before is an instance of a data frame. Data frames group vectors -->
<!-- together in a two dimensional table. As a consequence each variable can, i.e. each column of the -->
<!-- data frame can contain a different data type. Within a column, however, we can have only -->
<!-- one data type. -->

<!-- if you look at the type of a data frame, you will see that it is indeed a list and the class is  -->
<!-- a data frame. -->
<!-- ```{r type-of-data-frame} -->
<!-- typeof(aapl_prices) -->
<!-- class(aapl_prices) -->
<!-- ``` -->

<!-- Now armed with our new knowledge of R and probability lets ask a question to -->
<!-- our new data set. -->

<!-- ### Example: Will the stock price of Apple move up or down? {#apple-up-down} -->

<!-- Let us look at the first lines of our data once again: -->
<!-- ```{r fist-lines-again} -->
<!-- head(aapl_prices, n = 10) -->
<!-- ``` -->
<!-- Let us check what kind of object `aapl_prices` is. -->
<!-- ```{r} -->
<!-- typeof(aapl_prices) -->
<!-- class(aapl_prices) -->
<!-- ``` -->
<!-- As expected the object is a list. The class shows that our -->
<!-- object is a `data.frame`. Since the object is of class data-frame we can use the `dim` function -->
<!-- to see the number of daily observations in our data set. -->
<!-- ```{r dim-aapl} -->
<!-- dim(aapl_prices) -->
<!-- ``` -->
<!-- We see that our data store price information about 8044 trading days.  -->

<!-- We could now -->
<!-- approach the question about the probability of the Apple stock price moving up -->
<!-- tomorrow by taking a frequency-approach. This would require us first to count the -->
<!-- number of up-movements in our data. How do we do that? -->

<!-- Now we need to know know how we address and work with individual values in our data-set. Let's say -->
<!-- we measure the movement of the close price. R has a notation system to extract a value -->
<!-- or subset of values from an object: You write the name of the object first, followed  -->
<!-- by a pair of hard brackets. Between the brackets goes a `,`separating row and column -->
<!-- indices. The notation is thus like `aapl_prices[ , ]`. -->

<!-- When it comes to writing the indices you have six different ways to do this, all of them -->
<!-- very simple. You can use: -->

<!-- 1. Positive integers -->
<!-- 2. Negative integers -->
<!-- 3. Zero -->
<!-- 4. Blank spaces -->
<!-- 5. Logical values -->
<!-- 6. Names -->

<!-- The simplest are **positive integers**. For example if you choose -->
<!-- ```{r select-values-by-integers} -->
<!-- aapl_prices[1,6] -->
<!-- ``` -->
<!-- you see that the first value in the sixth column is selected. You thus get the first closing -->
<!-- price of the Apple stock recorded in our dataset. You can extract more than one value, for instance -->
<!-- by typing: -->
<!-- ```{r selection-from-apple} -->
<!-- aapl_prices[1:10, 6] -->
<!-- ``` -->
<!-- which gives you the first 10 closing prices. -->

<!-- Note that R's notation system is not limited to data frames. The same syntax can be used to select values from any R object, provided you supply an index for each dimension of the object. Two -->
<!-- things have to be kept in mind. In R indexing begins at 1. In some other programming languages -->
<!-- indexing begins at 0. The indexing convention in R is just like in linear algebra. The second -->
<!-- thing to note is that if you select two or more columns from a data frame, R will return a new data -->
<!-- frame, like in -->
<!-- ```{r select-block} -->
<!-- aapl_prices[1:10,1:8] -->
<!-- ``` -->
<!-- However, if you select a single column, R will return a vector: -->
<!-- ```{r select-single-column} -->
<!-- aapl_prices[1:10, 6] -->
<!-- ``` -->
<!-- If you prefer to get returned a data frame in this case, you have to add the argument -->
<!-- `drop = FALSE`, like: -->
<!-- ```{r drop-false} -->
<!-- aapl_prices[1:10, 6, drop = FALSE] -->
<!-- ``` -->

<!-- **Negative integers** work exactly opposite to positive integers. If you type: -->
<!-- ```{r negative-integers} -->
<!-- head(aapl_prices[-1,6], n = 10) -->
<!-- ``` -->
<!-- R will return the sixth column of the data frame *except* the first row. We just display the -->
<!-- first 10 values. -->

<!-- If you try to pair a negative and a positive integer in an index, R will return an error. -->
<!-- However, you can use both negative and positive integers if you use them in *different* -->
<!-- indices. -->

<!-- **Zero** is neither positive nor negative, If you use 0 as an index, R will return nothing -->
<!-- from a dimension with index 0. The following syntax for instance creates an empty object -->
<!-- ```{r empty-object} -->
<!-- aapl_prices[0,0] -->
<!-- ``` -->

<!-- **Blank spaces** are used if you want to ask R to select *every* value in a dimension. So for -->
<!-- instance, if you type: -->
<!-- ```{r select one column} -->
<!-- sel <- aapl_prices[ , 6] -->
<!-- ``` -->
<!-- R will select the entire column of closing prices. You can check that the length of this -->
<!-- vector is -->
<!-- ```{r length-sel} -->
<!-- length(sel) -->
<!-- ``` -->
<!-- as expected. -->

<!-- **Logical Values** can also be used for subsetting. If you type for instance -->
<!-- ```{r logical subsetting} -->
<!-- aapl_prices[1, c(F,F,F,F,F,T,F,F)] -->
<!-- ``` -->
<!-- R will select the first closing price. Note that here we used the R convention that `TRUE`and `T` -->
<!-- as well as `FALSE`and `F` have an equivalent meaning. -->

<!-- Finally, you can ask for the elements, you want by name. On our case, you could select the first -->
<!-- closing price by -->
<!-- ```{r first-value-by-name} -->
<!-- aapl_prices[1, "close"] -->
<!-- ``` -->

<!-- Finally, note that two types of object in R obey an optional second system of notation. You can -->
<!-- extract values from data frames and lists with th `$`syntax. It works as follows: For example -->
<!-- ```{r select-dollar-notation} -->
<!-- mean(aapl_prices$close) -->
<!-- ``` -->
<!-- would select the column of closing prices and compute their mean. -->

<!-- Now lets try to solve the counting problem for our data set. To see whether the price went up -->
<!-- we need to subtract the closing price of the first data from the second, and see whether it is -->
<!-- larger than zero. This we would have to do for every observation in the whole frame. -->

<!-- Now let us bring our indexing rules into action. We create two new objects, one ranging from -->
<!-- observation 2 of the close price until the end of the data frame, the other ranging from 1 to but the last -->
<!-- element, then we subtract and write the result in a new object. Finally we inspect -->
<!-- the first 10 elements of the result -->
<!-- ```{r differencing} -->
<!-- aux_1 <- aapl_prices[2:8044, "close"] -->
<!-- aux_2 <- aapl_prices[1:8043 , "close"] -->
<!-- diff_close <- (aux_1 - aux_2) -->
<!-- diff_close[1:10] -->
<!-- ``` -->
<!-- Now this works but it is not very elegant. Actually R has a built in function, called -->
<!-- `diff()` which will do the job for us. We can even append the result to the frame we have -->
<!-- using the `$` notation as follows: Let us call the new variable `diff`. But now we have to -->
<!-- be careful. Since we take first differences we will looe one observation. This will create a  -->
<!-- mismatch in the dimenion of the dataframe. Since the data frame starts at some date (in our case 1990-01-02) we have no difference for the first observation. R's notation for a missing -->
<!-- observation is `NA`. So we neet to put an `NA`ahead of our diff column: -->
<!-- ```{r apply-diff-function} -->
<!-- aapl_prices$diff <- c(NA, diff(aapl_prices$close, lag = 1)) -->
<!-- head(aapl_prices, n= 5) -->
<!-- ``` -->
<!-- Let's check whether we get the same thing and compare with the values in `diff_close`. They  -->
<!-- do indeed match. -->

<!-- Now we can use a really cool feature to count the up moves, which shows you the power of R -->
<!-- in a very nice example. We can test conditions by logical operators. In our case we want -->
<!-- to know whether the diff between two consecutive closing prices is positive. We add another -->
<!-- column to the dataframe which will then contain `TRUE` or `FALSE`values, when the condition -->
<!-- holds. To see what happens, let us do this and look at the first 5 rows of our new data frame: -->
<!-- ```{r logical-test} -->
<!-- aapl_prices$diff_pos <- aapl_prices$diff > 0 -->
<!-- head(aapl_prices, n = 10) -->
<!-- ``` -->
<!-- Now according to the frequency notion of probability we could attach the probability of an -->
<!-- inter day up move of the Apple stock price by computing the share of up moves (diff positive) -->
<!-- among all our change observations. -->

<!-- Now we can use the R coercion rules. If we apply the mean function to our `diff_pos` column -->
<!-- we get the share of up moves, since R will coerce the `TRUE` values to 1 and the `FALSE` values -->
<!-- to 0. This is a new opportunity to practice our newly acquired subsetting rules: -->
<!-- ```{r share-up} -->
<!-- mean(aapl_prices$diff_pos, na.rm = T) -->
<!-- ``` -->
<!-- We see that the result is that the stock price has moved up about half of the days and not up  -->
<!-- at the other days. -->

<!-- Let me explain what is going on in the syntax of our R statement. We selected the column -->
<!-- from our data frame that contained the differences. Then we apply the `mean()` function to this -->
<!-- column. But since the column contains one value which is `NA` the mean  -->
<!-- would return `NA` if we not -->
<!-- correct for this fact. This is what the argument `na.rm = T ` does. It tells R to first remove -->
<!-- `NA` form the observations and then compute the mean. -->

<!-- Now from a frequency point of view $P(U) = 0.51$ and $P(D) = 0.49$ accordingly. -->

<!-- Now **assume in addition** that the direction of a price move of the Apple stock  -->
<!-- on the current trading day is -->
<!-- **independent** of the direction of the Apple stock price move on the previous trading day for -->
<!-- all days. This means -->
<!-- that the probability of $U$-movements and of $D$-movements is unaffected by the number of -->
<!-- previous $U$ and $D$ movements. This is - of course - an assumption. You may think about -->
<!-- whether this assumption is reasonable.  -->

<!-- Postponing this important discussion, consider now a week from Monday to Friday and ask: -->
<!-- "What is the probability that the price of Apple will increase on each of the consecutive days?" -->

<!-- There are five trading days, so we need to compute -->
<!-- $P(U \cap U \cap U \cap U \cap U \cap U)$. This is by our assumption of independence equal -->
<!-- to $P(U) \cdot P(U) \cdot P(U) \cdot P(U) \cdot P(U)$. By our probability estimate from -->
<!-- relative frequency this amounts to $0.51^5$, which amounts to $0.035$. -->

<!-- "What is the probability that the stock price will decrease either on Monday,  -->
<!-- Tuesday, Wednesday, Thursday or Friday and will increase on the other four days?" -->

<!-- The probability that the $D$ movement happens, say on a Monday, is -->
<!-- $P(D \cap U \cap U \cap U \cap U)$ or $0.49*0.51^4$ which is $0.033$. -->
<!-- We have in total five mutually exclusive scenarios: -->
<!-- $P(D \cap U \cap U \cap U \cap U)$, $P(U \cap D \cap U \cap U \cap U)$, -->
<!-- $P(U \cap U \cap D \cap U \cap U)$, $P(U \cap U \cap U \cap D \cap U)$, -->
<!-- $P(U \cap U \cap U \cap U \cap D)$. Thus we have $0.033 + 0.033 + 0.033 + 0.033 + 0.033$ -->
<!-- as the final probability of this event, which is $0.132$. -->

<!-- Is this analysis any good? How could we possibly judge this? Interestingly the relative -->
<!-- frequencies of up and down moves look similar to a random experiment of a few thousand tosses -->
<!-- of a fair coin. But can we learn anything from this? Are the up and down moves independent? -->
<!-- Independence - of course - does not follow from the result we just got. -->

<!-- The idea that stock prices may fluctuate randomly was first discussed systematically by -->
<!-- Louis Bachelier (1870 - 1946), a French mathematician who studied stock price movements -->
<!-- mathematically. In 1965 the economist @Samuelson1965 published an article with the title -->
<!-- "Proof that stock prices fluctuate randomly".  -->
<!-- He argues in this paper that randomness -->
<!-- comes about through the active participation of traders seeking to maximize their wealth.  -->
<!-- A huge army of investors would aggressively use the smallest informational advantage  -->
<!-- and in doing so, they incorporate the information into market prices, which  -->
<!-- quickly eliminates this profit -->
<!-- opportunity.  -->
<!-- This lead to a cornerstone of modern Finance theory called the *random walk hypothesis* of  -->
<!-- stock price fluctuations. -->

<!-- If this theory was true, it would give an argument, why we might look at  -->
<!-- the up and down movements -->
<!-- in the stock price of apple as if it was the outcome of tossing a fair coin. In this case -->
<!-- the probability of an up or a down movement should be 1/2 and with the number of trials -->
<!-- approaching infinity the frequency of ups and downs should approach this probability. -->

<!-- The literature on stock price fluctuations which came later, however, presented evidence that -->
<!-- stock prices are predictable to a certain degree and do not fluctuate randomly. A good  -->
<!-- reference summarizing this evidence is @LoMacKinlay1999 In this -->
<!-- case our approach would perhaps produce a misleading answer. -->

<!-- We cannot give a clear cut answer here. The point of this brief discussion is that you  -->
<!-- just cannot apply a theoretical machinery mechanically without giving it further thought and -->
<!-- without maintaining a healthy amount of skepticism. It is fascinating that there are  -->
<!-- situations where abstract theories, like the theory of probability, show a robust relation -->
<!-- to real world phenomena. But the nature, the precise meaning and the robustness of this -->
<!-- relation has to be investigated for each single case. -->






<!-- ## Contents  -->

<!-- Statistics usually involves lots of data and we need ways to communicate and summarize these data. This -->
<!-- chapter introduces the most important concepts. -->


<!-- 1. The empirical distribution of data points -->
<!-- 2. Measures of location and spread. -->
<!-- 3. Skewed data distributions are common and some summary statistics are very sensitive to outlying values.  -->
<!-- 4. Summaries always hide some detail.  -->
<!-- 5. How to summarize sets of numbers graphically (histograms and box plots) -->
<!-- 6. Useful transformations to reveal patterns -->
<!-- 7. Looking at pairs of numbers, scatter plots, time series as line graphs.  -->
<!-- 8. The primary aim in data exploration is to get an idea of the overall variation. -->

<!-- ## Next steps in R: -->

<!-- - vectors and indices -->
<!-- - extracting subsets from vectors -->
<!-- - creating vectors using c() -->
<!-- - subsetting vectors wit logicals -->
<!-- - data frames -->
<!-- - factor class -->
<!-- - extracting data from data frames -->
<!-- - tapply -->



<!-- ## Outcome  -->

<!-- Understand these concepts and work through may examples showing how to apply these summary measures to data on the -->
<!-- computer. -->