# Modelling relationship: Prediction using regression

We ended the last chapter with Pearson's historical data on the
heights of fathers and sons to introduce the concept of a scatter-plot.

```{r}

library(JWL)
dat <- pearson

plot(dat, 
     pch = 19, 
     cex = 0.3, 
     xlab = "Father's height (inches)", 
     ylab = "Son's height (inches)",
     main = "Pearson's height data from 1903")
abline(0,1, lty = 2)
```

We discussed how we could use the positive association in this scatter-plot to 
predict the height of a son from the height of the father. One intuition might be
to use the straight dashed line that runs along the diagonal. If we would use this
line as a model for prediction, each adult son would be predicted to show the same 
height as his father. But we can do better than that and improve on this choice.

For any straight line we might choose, each data-point will give rise to a **residual** which
is the size of the error were we to use the line as prediction.

```{r}
plot(dat, 
     pch = 19, 
     cex = 0.3, 
     xlab = "Father's height (inches)", 
     ylab = "Son's height (inches)",
     main = "Pearson's height data from 1903")
abline(0,1, lty = 2)
abline(reg = lm(dat$Son ~ dat$Father), col = "red", lwd = 2)
points(63.1, 74.3, pch = 19, col = "red")
lines(c(63.1, 63.1), c(66.3264, 74.3), col = "red", lty = 2)
```

Suppose for instance that we took the father with a height of 63.1 inches and predicted the height
of the son according to the model described by the red line, we would have predicted the son to
have height 66.3 inches. In the data the son of the father with height 63.1 inches is in fact 74.3
inches. This is marked out as the red point in the plot. The length of the dashed line from the
red point to the red line is the residual.

The idea of prediction using models of straight lines is to make these residuals small. We
will learn a method how to do this. This approach to prediction is called regression and we
will learn how it works in this chapter.

## Correlation

In the beginning of this lecture we studied the example of infant mortality. 
Let us first study a scatter plot which plots the number of children per woman
against the infant mortality rate across countries in the world in the year 2000.

```{r}
library(JWL)
dat <- infant_mortality_data

dat_2000 <- dat[dat$Year == 2000, ]

plot(dat_2000$Children, dat_2000$Mortality,
     yaxt = "n",
     xlab = "Children per woman",
     ylab = "Infant mortality",
     pch = 19,
     cex = 0.5)
axis(2, at=pretty(dat_2000$Mortality), lab=paste0(pretty(dat_2000$Mortality) * 100, " %"), las=TRUE)
```

Notice the positive association in the data. The scatter of points is sloping upwards,
indicating that a higher number of children per woman tends to go along with
a higher infant mortality rate.

Let's look at another example, where we plot the average years of schooling for 
women on the x-axis and child mortality^[Remember from the section on counts and categorical data that child mortality is the rate of children who die before their fifth birthday. Infant mortality in contrast is the rate of children dying before their first birthday.] on the y-axis. Here we see a negative correlation. Child mortality across countries in a given year, here the year 2020, tends to be negatively
associated with the average years of schooling of women. In such cases we speak of a
negative correlation.

```{r}
data <- child_mortality_average_schooling_of_women

data_2020 <- data[data$Year == 2020, ]

plot(data_2020$YEW15, data_2020$MR,
     xlab = "Average years of schooling for women",
     ylab = "Child mortality",
     pch = 19,
     cex = 0.5)
```

The **correlation coefficient** measures how tightly a scatterplot is clustered around
a straight line. The correlation coefficient is therefore 
called a measure of linear association. It is usually expressed shorthand as
**correlation** and often abbreviated by the letter $r$.

Here are some mathematical properties of correlation which we will observe
by simulation in a next step.

::: {.callout-important}
## Mathematical properties of correlation

1. The correlation coefficient $r$ is a number between -1 and +1.
2. $r$ measures the extend to which the scatter plot clusters around a straight line.
3. $r=1$ if the scatter plot is a straight line sloping upwards and $r = -1$ if the
   scatter plot is a straight line sloping downwards
:::

### The correlation coefficient

R has a built in function for computing the correlation coefficient. This function is
called `cor()` and takes the $x$ and the $y$ values of a scatter plot as arguments. So, for
example, take the data on children per woman and infant mortality.

We read the data from the `JWL`package and store them in an object which we call `dat`.

```{r}
#| code-fold: false

library(JWL)
dat <- infant_mortality_data

```

Let's check the variable names. 

```{r}
#| code-fold: false
names(dat)
```

We filter the data for the year 2000 and store these data in a new object called `dat_2000`

```{r}
#| code-fold: false

dat_2000 <- dat[dat$Year == 2000, ]
```

Now we compute the correlation between the average number of children per
woman `Children` and the child mortality rate `Mortality`, where each data-point
corresponds to a particular country around the world.

```{r}
#| code-fold: false

cor(dat_2000$Children, dat_2000$Mortality)
```

indicating a strong positive correlation. 

Now let us use a function, which we call
`r_scatter` to explore how the correlation coefficient shapes a
scatter plot. This is an excellent opportunity to refresh our knowledge
of how to write your own functions in R.

First, remember that each function is an R object and has a name. We
already decided to call the function `r_scatter`. Then the syntax is
to write `function()` and put the function arguments within the paranthesis.

Here we have two arguments, the correlation coefficient - let us call it
according to the convention $r$ - and the number of data points we want
to consider. We call this number `n` and assign a default value to it, let's
say 1000. 

Next we have to write the function body between `{}`. This is the part
containing the actual instructions.

First we create two vectors of length $n$ of values $x$ and $z$ from a normal 
distribution with mean 0 and standard deviation 1. This is achieved by the r
function `rnorm(n)`. Note that n is a function argument and since we have set+
a default value of n, unless specified otherwise R will set n to 1000.

Next we use a mathematical fact, which we do not explain here in detail. When
$x$ and $y$ have the same standard deviation then the variable $y$ given by
\begin{equation}
y = r x + \sqrt{1 - r^2} z
\end{equation}
will have correlation coefficient $r$ with $x$. Thus we can implement this
formula in the function body, leaving $r$ unspecified. It is the function
argument of `r_scatter` and will be specified by you when you call the function
with a specific $r$.

Then we do a scatterplot of $x$ and $y$ which is the output of the function. The
function will always render the last expression in `{}` as the output.

This is how the code looks like in its entirety:

```{r}
#| code-fold: false

r_scatter <- function(r, n = 1000){
  
  x <- rnorm(n) # create n observations from a standard normal distribution
  z <- rnorm(n) # create n observations from a standard normal distribution
  
  y <- r*x + sqrt(1-r^2)*z # implement the formula discussed in text.
  
  # do a scatter plot of x and y
  
  plot(x,y,
     xlab = "x",
     ylab = "y",
     main = paste(c("Correlation coefficient is", r), collapse = " "),
     pch = 19,
     cex = 0.5)
  
}
```

Now we call `r_scatter` a few times to see what the correlation coefficient does.

```{r}
#| code-fold: false

r_scatter(0.9)
```

```{r}
#| code-fold: false

r_scatter(0.25)
```

```{r}
#| code-fold: false

r_scatter(0)
```

```{r}
#| code-fold: false

r_scatter( -0.7)
```

```{r}
#| code-fold: false

r_scatter(- 0.98)
```

### Computing r

How does `cor()` compute the correlation coefficient? We do not give a mathematical
derivation here. One way to explain the computation is to say that

::: {.callout-important}
## Formula for $r$

$r$ is the **average of the products of the two variables** when both variables are
measured in **standard units** or z-scores.
:::

Let's do a simple example in R to explain the computation. Consider the follwoing toy data
```{r}
x <- c(1,3,4,5,7)
y <- c(5,9,7,1,13)

knitr::kable(data.frame(x,y))
```

To convert into standard units we have to subtract the mean from each observation and
divide by the standard deviation. You should be able to verify for yourself that the mean
of $x$ is 4 and the standard deviation is 2 and for $y$ the corresponding  values are
7 and 4.

Doing this operation on the values in our table will give us:

```{r}
knitr::kable(data.frame(x,y, x_su = (x - mean(x))/(sd(x)*sqrt(4/5)), y_su = (y- mean(y))/(sd(y)*sqrt(4/5))))
```

Finally we have to add the product of the standardized values $x_{su}$ and $y_{su}$ 
to get
```{r}
knitr::kable(data.frame(x,y, x_su = (x - mean(x))/(sd(x)*sqrt(4/5)), y_su = (y- mean(y))/(sd(y)*sqrt(4/5)), xy_su = (x - mean(x))/(sd(x)*sqrt(4/5))*(y- mean(y))/(sd(y)*sqrt(4/5))))
```

Taking the average across these values gives $r$. Thus we get
\begin{equation}
\frac{0.75-0.25+0.00-0.75+2.25}{5} = 0.40
\end{equation}

Now check this against `cor()`:

```{r}
#| code-fold: false
cor(x,y)
```

Let's do a scatter plot of the data:

```{r}
plot(x,y)

grid(nx = 2, ny = 2,
     lty = 2, col = "gray", lwd = 2)

par(new = TRUE)
plot(x,y, 
     xlab = "x",
     ylab = "y",
     main = "Scatter plot toy example",
     pch = 19, 
     col = 4)
text(x = 4.1, y = 6.6, 
     label = "0")
text(x = 3, y = 8.4, 
     label = "-0.25")
text(x = 1, y = 4.4, 
     label = "0.75")
text(x = 5, y = 1.6, 
     label = "-0.75")
text(x = 7, y = 12.4, 
     label = "2.25")
text(x = 2.5, y = 10, 
     label = paste(c("-", "-", "-", "-")), cex = 4)
text(x = 5.5, y = 10, 
     label = "+", cex = 4)
text(x = 5.5, y = 4, 
     label = paste(c("-", "-", "-", "-")), cex = 4)
text(x = 2, y = 4, 
     label = "+", cex = 4)
```

In this plot the $x$ axis and the $y$ axis show the values of the original table
and in the plot we associate the values of the product of standardized units
in the plot. The dashed lines are plotted through the points of averages, dividing the
scatter plot into four quadrants. If a point is in the lower left quadrant both
variables are below average and are negative in standard units. So their product is
positive. In the upper right quadrant the product of two positives is positive. In
the remaining two quadrants the product of a positive and a negative is 
negative.^[For those of you who need a refresher about the algebra of multiplying positive and negative numbers, you might resort to the following intuition: $3 \times 5 = 15$: Getting
5 Dollars 3 times gives you 15 Dollars. $3 \times (-5) = -15$: Paying a 5 Dollar penalty 3 times gives you a 15 Dollar penalty. $(-3) \times 5 = -15$: Not getting 5 Dollars 3 times is not getting 15 Dollars. $(-3) \times (-5) = 15: Not paying a 5 Dollar penalty 3 times is getting 15 Dollars.] 
The
average of all these products is the correlation coefficient. 

If $r$ is positive, then
points in the two positive quadrants will predominate as for example here
```{r}
r_scatter(r = 0.93)
# Vertical grid
abline(v = 0,
       lty = 2, col = "gray")

# Horizontal grid  
abline(h = 0,
       lty = 2, col = "gray")
```

If $r$ is negative points in the
two negative quadrants will dominate as for example here.

```{r}
r_scatter(r = -0.95)
# Vertical grid
abline(v = 0,
       lty = 2, col = "gray")

# Horizontal grid  
abline(h = 0,
       lty = 2, col = "gray")
```

### Properties of r

From these observations we can observe three properties of $r$

1. $r$ is a pure number. It has no units. This is because $r$ is computed from standard units.
2. $r$ is unaffected by changing the units on either axis. This is again because $r$ is computed based on standard units.
3. $r$ is unaffected when we switch the axes. Algebraically this must be the case because when
we multiply $x$ by $y$ the value of the product is the same as when we multiply $y$ by $x$. Geometrically switching axes reflects the scatterplot along the line $x=y$ but does not change the amount of clustering along the line.

### r is a measure of clustering along a line. 

$r$ is a measure of clustering along a line. This is often referred to by calling
$r$ a measure of linear association. Note that data are often associated strongly but
not necessarily along a line. In such cases $r$ would show a weak association only.

