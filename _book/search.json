[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics",
    "section": "",
    "text": "Preface\nThese are lecture notes for my introductory statistics and data literacy course I am developing for Jesuit Worldwide Learning (JWL) together with my colleagues from Seitwerk. I would like to thank Peter Balleis, Mathias Beck, Martha Habash, Stefan Hengst and Anny Mayr for all their generous help and support in kickstarting this challenging project.\nThe goal of the notes is to develop the core contents of the course systematically and to support the production of the online units. I also write up in these notes material and instructions for teaching material, which is meant to be used in the local study centers. My vision is that after we are through with the production of the online units, the notes will be overhauled and can then when the course actually begins, be used by the students as a textbook and reading material.\nFor the time beeing it is more like a systematic notebook and a scenario. I will add questions and comments for the Seitwerk team during this phase as collapsible callout notes like this:\n\n\n\n\n\n\nComment or Remark for Seitwerk\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user.\n\n\n\nThis helps me to keep a better overview of the overall development of the text.\nI will put the developing material on our MS-teams collaboration platform as we go along. Data and graphics will be put on the platform as separate files. In addition, mainly for the interaction with my past and future self, I keep a versioning of the notes on a public github archive, which may also be accessed by colleagues, and of course by the Seitwerk team. The github repository’s address is:\nhttps://github.com/Martin-Summer-1090/Statistics_JWL"
  },
  {
    "objectID": "introduction.html#experiencing-the-world-through-data",
    "href": "introduction.html#experiencing-the-world-through-data",
    "title": "1  Introduction",
    "section": "1.1 Experiencing the world through data",
    "text": "1.1 Experiencing the world through data\n\n1.1.1 What are the major sources of electricity production in Kenya?\nWhat is the biggest source of electricity production in Kenya? Answering this question needs data, listing and recording various forms of electricity production in specific countries. An internationally acknowledged organisation, which collects and reports these data is the International Energy Agency (IEA).\n\n\n\n\n\n\nAccessing IEA data on the internet\n\n\n\nIf you have access to the internet, you can look up data and reports by the IEA on it’s website https://www.iea.org/1\n\n\n1 A screenshot I have taken in August 2022 shows the website like this. In the upperm right part of the website there is a link called data which will bring you to the data collected by IEA. Now let’s take a quiz and guess. What do you think is actually the biggest source of electricity production in Kenya?\n\nCoal\nRenewable energy\nNatural gas\n\n\n\n\n\n\n\nSeitwerk: Expand for reading comment.\n\n\n\n\n\nIt would be great to make this like an online quiz, where you can click the answer and get right or wrong. Renewable energy is the right answer.\n\n\n\nConsulting the IEA data, you will find that the correct answer is renewable energy. A very interesting website called gapminder, which analyzes the answers of many people to this question, finds that 61 % give a wrong answer to this question. Maybe they find it hard to imagine that 80 % of energy production in Kenya is already fossil free thanks to huge sources of geothermal- and hydropower.2 Even if you break the answers down by country you see that 38 % of people from Kenya get the answer wrong. Among the people from the UK, who answer this question even 72 % answer wrongly.2 Geothermal electricity generation uses the earth’s natural heating energy - geothermal energy. A country needs to be located on a geothermal hot spot to make effective use of this energy source for electricity generation. At such a hotspot there are high temperatures beneath the earth’s surface which naturally produces steam. This steam can be used to spin turbines connected to a generator. This mechanism then produces electricity. Hydropower uses the water cycle to generate electricity by using dams to alter the flow of a river. The kinetic energy of the water spins turbines connected to a generator which produces electricity.\nIf you are not familiar with the details of electricity production using geothermal- and hydropower or if you are not completely sure how to interpret a number like 38 %, don’t worry for now. The point here is that you see that one useful consequence of being able to access, read and interpret data is that it can help to establish facts about the world. In this way data can help us to perhaps correct misconceptions we might have had about these facts.\n\n\n\n\n\n\nThe gapminder webpage\n\n\n\nIf you have internet access you can reach the gapminder webpage at https://www.gapminder.org/. I encourage you to visit this page at an occasion when you have access to the internet. You will be surprised how often you might have a wrong guess about basic facts in the world.3\n\n\n3 Here is how the website looks like as of August 2022: In this course you will learn how to work with data and how to learn from these data in a systematic way.\n\n\n1.1.2 How many trees are there on the planet?\nLearning from data entails more than just establishing facts. This might not always be possible, either because you cannot access the relevant data or you cannot completely access them, since doing so would be way too expensive. When working with data you also need rigorous definitions of concepts, so that you can actually transform your experience about the world into data\nThink for a moment about the following interesting example, which I learned from a wonderful book by the British statistician David Spiegelhalter (Spiegelhalter 2019). The example shows that even just categorizing and labeling things in the world to measure them and turn them into data can be challenging. A very basic question raised in the introduction of this book is:\n\n\n\n\n\n\nQuestion:\n\n\n\nHow many trees are there on the planet?\n\n\nIt is clear that answering this question is more challenging than the task the IEA had to solve when listing energy sources by country around the world in a given time period. But before you go about to think how you might count all the trees on the planet, you have to answer an even more basic question, namely: What is a tree?\nSome of you might think this is a silly and obvious question, which every child can answer. But what some might consider a tree others will consider just a shrub. Turning experience into data requires rigorous definitions. It turns out that such definitions can be given for trees. 44 For example the forestry expert Michael Kuhns writes: “…Though no scientific definition exists to separate trees and shrubs, a useful definition for a tree is a woody plant having one erect perennial stem (trunk) at least three inches in diameter at a point 4-1/2 feet above the ground, a definitely formed crown of foliage, and a mature height of at least 13 feet. This definition works fine, though some trees may have more than one stem and young trees obviously don’t meet the size criteria. A shrub can then be defined as a woody plant with several perennial stems that may be erect or may lay close to the ground. It will usually have a height less than 13 feet and stems no more than about three inches in diameter.” (Kuhns, n.d.)\nBut even with the definition at hand you cannot just go around the planet and count every plant that meets the criteria. So this is what the researchers investigating that question did according to (Spiegelhalter 2019):\n“…They first took a series of of areas with a common type of landscape, known as a biome and counted the average number of trees per square kilometer. They then used satellite imaging to estimate the total area of the planet covered by each type of biome, carried out some complex statistical modelling, and eventually came up with an estimate of 3.04 trillion (3,040,000,000,000) trees on the planet. This sounds a lot, except that they reckoned there used to be twice this number.”\nNow imagine that if long expert discussions are needed to precisely define something so seemingly obvious as a tree, clearly more complex concepts such as unemployment or the definition of the total value of goods and services produced in a country in a given year, known as Gross Domestic Product or GDP, is even more challenging.\nThere is no automatism or mechanical receipt how we can turn experience into data and the statistics that we use and produce are constructed on the basis of judgement. It is a starting point for a deeper understanding of the world around us. This is one of the reasons why in this course statistics is not only referred to as science, which it arguably is to some degree, but also as an art.\nTo guard against the trap of mindlessly fall into mechanical thinking or automatism it is sometimes helpful to make a rough plausibility estimates about the order of magnitude you might expect as a result for a really big number, like the number of trees on the globe.\n\n\n1.1.3 What has happened to extreme poverty on the globe in the last 20 years?\nOne of the limitations of data as a source of knowledge about the world is that anything we choose to measure will differ across places, across persons or across time. When analyzing and trying to understand data we will always face the problem how we can extract meaningful insights from this apparent random variation. One challenge faced by statistics and one core topic in this course will thus be how we can distinguish in data the important relationships from the unimportant background variability.\nExploring and finding such meaningful relationships or patterns in data using the science of statistics and computational tools is one of the skills you will learn in this course.\nAn example of a pattern in data is if we can spot a trend, data values which are for example increasing or decreasing.\nConsider the following data from the World Bank, reporting the share of people in the world who are living in extreme poverty. Extreme poverty is defined by the World Bank, an international development finance organisation for low and middle income countries located in the US5 as the percentage of people in the world who have to live on less that $ 1.90 per day. Hans Rosling in his great book Factfullness (Rosling, Rosling, and Rosling-Rönnlund 2018), gives a concrete description of what it means in concrete terms to live on this income level, which Rosling calls level 1. I quote from his book:5 The Worldbank is an international financial institution founded along with the International Monetary Fund in the Bretton Wods conference in 1944. It is located in Washington D.C. and finances projects in low and middle income countries. It also collects and processes data globally to support its activities and conduct development research. The Worldbank makes its data public in print or through its website https://www.worldbank.org/en/home\n“… Your five children have to spend hours walking barefoot with your single plastic bucket, back and forth, to fetch water from a dirty mud hole and hour’s walk away. On their way home they gather firewood, and you prepare the same gray porridge that you have been eating at every meal, every day, for your whole life - except during the months when the meager soil yielded no crops and you went to bed hungry. One day your youngest daughter develops a nasty cough. Smoke from the indoor fire is weakening her lungs. You can’t afford antibiotics, and one month later she is dead. Yet you keep struggling on. If you are lucky and the yields are good, you can maybe sell some surplus crops and manage to earn more than 2 $ a day, which would move you to the next level…(Roughly 1 billion people live like this today).\nLet us have a look at a table showing the first 10 observations of this share\n\n\nCode\n# read poverty data from our project data folder\npovdat_by_country &lt;- read.csv(\"data/extreme_poverty/share-of-population-in-extreme-poverty.csv\")\n# select the years from 2000\npovdat_world &lt;- with(povdat_by_country, povdat_by_country[Entity == \"World\" & Year &gt;= 2000, ])\n# Keep only the year and the share\nplot_data &lt;- povdat_world[,c(3,4)]\n# Rename variables\nnames(plot_data) &lt;- c(\"Year\", \"Share\")\n\n# produce a table\nlibrary(knitr)\nkable(head(plot_data, n= 10), row.names = F, digits = 1)\n\n\n\n\nTable 1.1: Share of world polpulation living in extreme poverty, Source: World Bank\n\n\nYear\nShare\n\n\n\n\n2000\n27.8\n\n\n2001\n26.9\n\n\n2002\n25.7\n\n\n2003\n24.7\n\n\n2004\n22.9\n\n\n2005\n21.0\n\n\n2006\n20.3\n\n\n2007\n19.1\n\n\n2008\n18.4\n\n\n2009\n17.6\n\n\n\n\n\n\nCode\nwrite.csv(head(plot_data, n= 10), file = \"tables/table_1_1_world_poverty.csv\", row.names = FALSE)\n\n\nThe table shows that the share of people living in extreme poverty has been decreasing year after year for the first 10 years since the year 2000. This is a pattern which is called a downward trend.\nNote that we did not show the whole series of numbers. The data points in our data-set actually range until the year 2017. Printing them all in a table quickly produces very large and unwieldy number array which is awkward to read.\nExploring data and detecting patterns is usually easier when we use the power of the human visual system. Humans are very good in finding visual patterns. Looking for patterns is almost visceral for us. We can’t help but looking for patterns. This almost instinctive human urge can also be misleading and suggesting patterns to us where there are in fact none.\nIn data exploration we can make use of the power of visualization by plotting data and looking at them graphically. The modern computer has made this form of displaying data particularly easy and powerful and visualizing data in data exploration is another core skill you are going to learn in this course.\nSo let us visualize the world poverty data. In our plot we draw the year on the x-axes and the share6 of people living in extreme poverty on the y-axes. This will give us a point for each year. To facilitate the spotting of a trend, we connect the annual observations by a line.6 The share is a proportion of a whole. Here this proportion is expressed in percent. A percentage (from Latin per centum ‘by a hundred’) is a number or ratio expressed as a fraction of 100. It is often denoted using the percent sign (%), although the abbreviations pct., pct, and sometimes pc are also used. A percentage is a dimensionless number (pure number), primarily used for expressing proportions, but percent is nonetheless a unit of measurement in its orthography and usage. So for example when we say the share of extremely poor people in the world in a given year is 25 %, this expresses the fraction 25/100 or 0.25. Percentages are often used to express proportions of a whole. 25% would thus be 1/4th of all people in the world.\n\n\nCode\nlibrary(ggplot2)\n\np &lt;- ggplot(plot_data, aes(x = Year, y = Share/100)) + \n     geom_line() +\n     geom_point() +\n     xlab(\"Year\") +\n     ylab(\"Share of extremely poor people\") +\n     scale_y_continuous(labels = scales::percent)\n  \nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world.png\")\np\n\n\n\n\n\nFigure 1.1: Share of world population living in extreme poverty from 2000 - 2017\n\n\n\n\nVisualizing trends in world extreme poverty as an example of data exploration. In this case the data pattern reveals a stunning fact. Over almost two decades we can see a sharp fall in the share of extremely poor people when looked at from a global perspective.\nOf course when we drill down to the level of individual countries this trend will not look the same everywhere and there might be countries where the share has actually increased. But overall we have seen a breathtaking steady decline. This is good news.77 When you have access to the internet you can have a closer look at these data at the very interesting website “our world in data” maintained by a consortium of Oxford University and University College London. See https://ourworldindata.org/extreme-poverty. The website has many interesting visualizations and options to select individual countries, country aggregates and make other selections of the data.\nBut does this trend mean that extreme poverty must disappear some years down the road? No, because nobody can tell whether the trend of the last two decades will go on also in the future.\nStatistics can help us to think more systematically about patterns and in making systematic guesses how a pattern might continue in the future. This is another core skill you will learn in this course: Making predictions, which means using available data and information to make informed guesses about data and information we do not have.\nLet us go back to the share of people in the world living in extreme poverty. As reported in our data the last actual observation for the global share in extreme poverty from the world bank is from 2017. There are more recent data for some regions but the global data since then are by now forecasts based on statistical techniques. The basic ideas of these techniques and how to apply them to data is a core skill you will learn in this course.\nNow what does the World bank predict for the share of extreme poverty in the world? Let us look at the data again graphically to visualize the prediction.\n\n\nCode\nobsdat &lt;- data.frame(Year = plot_data$Year, Scenario = rep(\"Observed\", 18), Share = plot_data$Share)\n\nadd_dat &lt;- data.frame(Year = 2018, Scenario = \"Observed\", Share = 8.6)\n\npred_dat_precovid &lt;- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Pre-Covid-19\", 4), Share = c(8.6, 8.4, 7.9, 7.5))\n\npred_dat_covidbase &lt;- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Covid-19-Baseline\", 4), Share = c(8.6, 8.4, 9.1, 8.9))\n\npred_dat_coviddown &lt;- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Covid-19-Downside\", 4), Share = c(8.6, 8.4, 9.4, 9.4))\n\ndat &lt;- rbind(obsdat, add_dat, pred_dat_precovid, pred_dat_covidbase, pred_dat_coviddown)\n\np &lt;- ggplot(dat, aes(x = Year, y = Share, group = Scenario, color = Scenario)) + \n  geom_line(alpha = 0.5)+\n  geom_point()+\n  scale_color_manual(values=c('green', 'red', 'black', 'blue'))\nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world-forecast.png\")\np\n\n\n\n\n\nFigure 1.2: Share of world population living in extreme poverty from 2000 - 2017 with predictions until 2021\n\n\n\n\nObserve that the line showing the share of extreme poverty in the world takes different distinct future paths as we make predictions. What does this mean?\nAt the root of the predictions is an abstract model8, how the share of poverty changes over time. If the underlying data would correspond to a world before Covid the falling trend in poverty would just continue to fall, as it has done continuously from 2000 onward. In the graph you can see this scenario if you follow the black and the blue line. But taking the pandemic and the consequences into account the prediction of the World Bank is that extreme poverty after a almost two decades downward trend will rise again. This you can see by following the green and the red line. How much, this rise actually will be in the end depends on data we can not yet know.8 If you have difficulties now to imagine what this means, don’t worry. We will learn in detail what a model is and how it can be interpret. In practice a model is usually an equation which provides a low-dimensional summary of a dataset. This summary is then used to make predictions.\nWhen we make informed guesses based on observed data on information we do not yet have there is uncertainty involved. Using the theory of probability in combination with statistics we can quantify this uncertainty. Quantifying the uncertainty attached to predictions is the third basic skill you will learn in this course.\n\n\n1.1.4 Does taking your time in college pay off?\nA newspaper in Germany reported that the more semesters needed to complete an academic program at the university the greater the starting salary in the first year of a job. The report was based on a study that used a random sample9 of 24 people who had recently completed an academic program.9 We will later in the course learn in detail what a random sample is. For the moment imagine that there is a mechanism which allows to select these 24 students at random from the large population of all university students in Germany. When a sample is random, every member in the sample has the same probability of beeing chosen from the population.\nInformation was collected on the number of semesters each person in the sample needed to complete the program and the starting salary, in thousand Euros, at the beginning of the job.\nThe data are shown in the following plot\n\n\nCode\ndat &lt;- read.csv(\"data/college_years_salaries/coll_sal.csv\")\n\nlibrary(ggplot2)\n\np &lt;- ggplot(dat, aes(x=Time, y=Salary)) +\n     geom_point()\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm and the starting salary in the first year in the job.\n\n\n\n\nWhat you see in this picture is a so called scatter-plot. It takes the data and plots all pairs of time in semesters needed to complete the academic university program and the starting salary in the job, where the first value is shown on the x-axis and the second on the y-axes. The points you draw like this are “scattered” all over the place, but it seems that “by and large” there is also some trend - shown as a blue line - like this:\n\n\nCode\np &lt;- ggplot(dat, aes(x=Time, y=Salary)) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = FALSE)\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary_with_trend.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm and the starting salary in the first year in the job with a linear trend fitted to data.\n\n\n\n\nLooking at the data, does this plot support the claim of the Newspaper?\nApparently the journalist writing the article saw a pattern, shown here as the blue line, which suggests that on average the salaries are really increasing with the semesters spent at the university. But is this pattern plausible? What do you think?\nAn independent researcher, who doubted the result, received the data from the newspaper and did a new analysis by separating the data into three groups based on the major of each person.\n\n\nCode\np &lt;- ggplot(dat, aes(x=Time, y=Salary, group = Major, color = Major)) +\n     geom_point()\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary_by_major.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm by major and the starting salary in the first year in the job.\n\n\n\n\nNow, looking at this plot, describe the relation for students with a major in business. How could the newspaper report be modified to describe the data?\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nThis example works particularly well for involving students. I am not quiet sure how to build this interactive element in the online unit. maybe you have an idea.\n\n\n\nYou see in this example that looking for patterns is not as easy as it seems. Again we see that there are no automatism. You will need the skills you learn in this course to gain competence in distinguishing actual patterns from spurious ones.\nWhy did I go through all these example with you in the beginning, pinning down the share of renewable energy in the electricity production in Kenya, estimating the number of trees on the planet, long term trends in world extreme poverty, and the relation between study time and beginning salaries for students in Germany? All these examples illustrate some special skills you will learn in this course. Hopefully it also convinced most of you that statistics and working with data is an exciting field and a way to engage with real world issues.\nSo these are the three core skills you will learn in this course:"
  },
  {
    "objectID": "introduction.html#the-three-basic-skills-you-will-learn-in-this-course",
    "href": "introduction.html#the-three-basic-skills-you-will-learn-in-this-course",
    "title": "1  Introduction",
    "section": "1.2 The three basic skills you will learn in this course",
    "text": "1.2 The three basic skills you will learn in this course\n\n\n\n\n\n\nThe three basic skills you will learn in this course\n\n\n\nAfter successfully completing this course students you will have learned three core skills:\n\nData exploration or finding patterns in data and information through visualisation and computation.\nMaking predictions, which means using available data and information to make informed guesses about data and information we do not have.\nTo quantify the uncertainty we have to attach to our predictions.\n\n\n\nThe course is split into 8 units in total. Units 1, 2 and 3 will be mostly be concerned with data exploration and with making comparisons based on data. You will also learn step by step how you can use the computer for data exploration and visualization. We assume no prior knowledge and start from scratch. We also do not strive for completeness. The idea is that you acquire the practically most important skills and get maturity to drill deeper for yourself after the course either in your further studies or on the job.\nUnit 4 and unit 5 will be predominantly be concerned with models and using models for prediction. Here you will learn how to spot trends in data, how you can discern actual information in data, so called signals, from random variation, called noise.\nUnits 6, 7 and 8 will focus on how to quantify uncertainty related to prediction and inference from data. Here is the place where probability theory combines with statistics to provide analytically and practically powerful tools which ground data analysis in a firm scientific foundation. This is also the most challenging part of the course and it will require lots of practice and participation from you to acquire this important skill.\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nIn your template you suggest that in the introduction I give a minute overview of the details of topics learned in the course. I would prefer to abstain from this, because it contains lot of terms the students will learn step by step and it is probably boring at this stage. I would rather prefer the big picture approach outlined here with the three basic skills as the guiding posts. If listing the details is a must, it would look roughly like this:\n\n\n\n\n\n\nUnit 1: Overview; Categorical Data and Proportions\n\n\n\nIn unit 1 we will give an overview of the course and we begin with the analysis and understanding of binary variables, variables that can be imagined as simple yes or no questions and how they can be summarized as proportions or percentages. You will learn about how the idea of expected frequency will promote the understanding of the meaning of these shares and how this provides a basic understanding of the importance of these numbers.\n\n\n\n\n\n\n\n\nUnit 2: Summarizing and Communication lots of data; From limited data to populations\n\n\n\nIn unit 2 we learn how to deal with lots of data, the typical situation we will face when we do statistics. When there are lots of data we need instruments and tools to summarize them and to get an overview. This overview is usually also very important for communicating the data and the information they might contain. We will also learn how we can use statistics to learn properties about large populations by only making limited observations on some appropriately chosen subset of individuals from this population. The gold standard in making such inferences possible is the concept of a so called random sample. But at each step of the sampling procedure bias can crop up and invalidate our results leading to wrong conclusions. The circumstances under which inference from samples to populations can be made and how we can make sure to minimize bias we need a firm understanding of the opportunities and limits of this important technique.\n\n\n\n\n\n\n\n\nUnit 3: What causes what?\n\n\n\nIn unit 3 we discuss when data analysis allows us to say something about what causes what. We learn about the important concept of randomized trials. We will also learn what an observational study is and how it differs from a randomized trial. This is an important concept we will learn through the discussion of real world examples.\n\n\n\n\n\n\n\n\nUnit 4: Modelling relationships using regression and algorithmic predictions\n\n\n\nIn unit 4 we will learn a key concept needed to make predictions. The technical term for this concept in statistics is regression. It is a simple mathematical model describing how a set of explanatory variables varies systematically with a response variable. We will learn to construct such models and to interpret them correctly. We will also cover related techniques which have become very important recently and entered the statistical toolbox from the field of computer science. These methods are known under the notion of algorithmic prediction or machine learning.\n\n\n\n\n\n\n\n\nUnit 5: How sure can we be about what is going on: Estimates and Intervals.\n\n\n\nIn unit 5 we encounter the first time tools for quantifying uncertainty. We learn how to determine and use uncertainty intervals by using a technique which is called the bootstrap. Being able to determine such intervals is extremely important in communicating statistics and for supporting a systematic and sound answer to the question: How sure can we be about an estimate.\n\n\n\n\n\n\n\n\nUnit 6: Probability: Quantifying uncertainty and variability\n\n\n\nIn unit 6 we deepen the knowledge how to quantify uncertainty by introducing basic ideas of probability theory. Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable.\n\n\n\n\n\n\n\n\nUnit 7: Putting Probability and Statistics together\n\n\n\nIn unit 7 we put statistics and probability theory together. This allows us to both simplify ideas and techniques how to quantify uncertainty. The combination of the two field makes the tools for quantifying uncertainty at the same time more powerful. Combining statistics and probability theory is at the heart of statistics as a science. It makes all the ideas developed in unit 1 to 6 very versatile and powerful.\n\n\n\n\n\n\n\n\nUnit 8: Answering questions and claiming discoveries\n\n\n\nIn unit 8 we learn how to leverage the knowledge of this course to answer questions and to claim discoveries. You will learn how statistics is used in the sciences and how it supports to develop our knowledge of the world. It pulls many ideas of the whole course together and when you have mastered this unit you have mastered all the basic skills we want to develop in this course, data exploration, prediction and quantifying uncertainty."
  },
  {
    "objectID": "introduction.html#on-the-use-of-the-computer",
    "href": "introduction.html#on-the-use-of-the-computer",
    "title": "1  Introduction",
    "section": "1.3 On the use of the computer",
    "text": "1.3 On the use of the computer\nOur approach to teach you basic ideas of statistics and data analysis will be very much problem and activity oriented. The application of specific statistical techniques will be only one component in a whole package of activities you will need to engage in when you work with data in real world applications. Preparing data appropriately for analysis as well as communicating the conclusions for your analysis will be important elements of the whole process of statistical analysis. Today these activities involve using a computer.\nIn this course you will also learn how to use the computer. It will play an important role for developing your skills. We will make no assumptions of prior knowledge of computers and programming and will introduce the use of the computer step by step.\nUsing a computer requires a language in which we can tell the computer what to do and which the computer can understand. The language of our choice for this course is called R. It is one of the most widely used and most powerful languages for data analysis and statistics. We will introduce you to the language and its use step by step as we go along and in parallel with the statistical concepts we develop.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nRosling, Hans, Ola Rosling, and Anna Rosling-Rönnlund. 2018. Factfullness, 10 Reasons Why We Are Wrong about the World - and Why Things Are Better Than You Think. Sceptre.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "introduction.html#activities-in-the-study-center",
    "href": "introduction.html#activities-in-the-study-center",
    "title": "1  Introduction",
    "section": "1.4 Activities in the study center",
    "text": "1.4 Activities in the study center\n\n1.4.1 Visualizing the share of extremely poor people for different countries\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nHere we assume that students have a running R and R-Studio or R with Jupyter Notebook or Jupyter Lab installation on their laptops at the study center. We would give the code in a notebook with the code chunk shown in the source file, to play with.\nWhile we need not pin down all the details of the computational infrastructure yet, we need a discussion how to integrate the computer instructions into the course and the material I have to prepare for that. I would very much prefer a minimalist solution with R and some kind of notebook but not more.\n\n\n\nLets go back to figure Figure 1.1 for a moment. This figure has been created by the use of the computer.\nIn the following box you see the computer code that has read the data from a file and then plotted the share for the world and for a particular country, say China. Don’t worry if you do not (yet) understand the details of the code. Think of it as a language that tells the computer what to do with the data. You can edit the code and delete China and insert another country instead. If you click the green arrow at the upper right corner of the box the computer will run the code again and generate a new graphic.\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nThe details of this will of course depend on the kind of notebook we use. We can use in principle three options. Option 1: Have an installation of Rstudio (https://www.rstudio.com/). RStudio is the most popular (and free) IDE for development of R code. In R Studio we can open quarto notebooks (qdm) and run code interactively there. Pro: Works seamlessly with my notes and enhances production efficiency. Con: You need to explain students the IDE in addition to R, though no big deal it is an additional complication. Option 2: Jupyter Notebook with an R kernel (ipynp). Pro: If JWE decides to work more with Jupyter notebooks on a broader base, seamless integration for this strategy. Con: same as with R studio and just a tiny inch more awkward to produce for me. Option 3: Work in the R console only. Pro: No IDE or notebook. Con: perhaps a bit too difficult to use for students without Computer Science background.\n\n\n\nTry to play and experiment with the code in this way to see what happens. Soon you will know yourself how to make interesting and beautiful data visualizations yourself.\n\n\nCode\nlibrary(ggplot2)\n\ndemo_data &lt;- read.csv(\"data/extreme_poverty/share-of-population-in-extreme-poverty.csv\")\n\nnames(demo_data) &lt;- c(\"Country\", \"Code\", \"Year\", \"Share\")\n\npl_dat &lt;- with(demo_data, demo_data[Country %in% c(\"World\", \"China\") & Year &gt;= 2000, ])\n\np &lt;- ggplot(pl_dat, aes(x = Year, y = Share, color = Country)) +\n     geom_point() +\n     geom_line() +\n     xlab(\"\")\nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world-china.png\")\n\n\nSaving 7 x 5 in image\n\n\nCode\np\n\n\n\n\n\n\n\n1.4.2 Guessing ages\nThis is an exercise which needs the leadership of an instructor at the study center. It would be great to collect the data of the exercise in a readable file for later use in the course.\nWe have 10 photos of persons whose age is known to us but not to the students. We divide students into 10 groups A through J. Each group gets one of the photos. The students in each group are asked to estimate the age of the person in their photograph and write the guess into a form. Each group must come up with a single estimate.\nExplain that each group will be estimating the ages of all 10 photos and that groups are competing to get the lowest error. Each group passes its card to the next group (A to B, B to C, etc. J back to A) and estimates the age of the new photo. This is continued until each group has seen all photos.\nThe data are written in a table where the rows are groups and the columns are card numbers. We can discuss the expected accuracy of the guesses (within how many years do you think you can guess). Then start with card 1 and ask groups to give their guess, then reveal the true age and write it at the bottom margin of the first column.\nIntroduce the concept of error - guessed age minus actual age - and wrote the errors in place of the guessed ages. Then fill the whole table. Ask the students to compute the absolute average error.\nStudents get an idea about: uncertainty, empirical analysis and data display. There are many statistical ideas in this game and the data can be taken up throughout the course (variance, bias, experimental design, randomization, linear regression, two-way tables, statistical significance).\nTo illustrate the idea more precisely:\nFor each card your group is given, estimate the age of the person on the card and write your guess on the table below in the row corresponding to this numbered card. Later students are told the true ages and they can compute the error. The error is defined as estimated minus actual age.\n\nExample for group card, guessing ages\n\n\nCard\nEstimated\nActual\nError\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n5\n\n\n\n\n\n6\n\n\n\n\n\n7\n\n\n\n\n\n8\n\n\n\n\n\n9\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n1.4.3 Collect data from students.\nOne data collection exercise, which might be fun here is to collect the height and hand span from students in the class. Later when we teach regression we can compare to the data Pearson collected on university students over 100 years ago.\n\n\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nRosling, Hans, Ola Rosling, and Anna Rosling-Rönnlund. 2018. Factfullness, 10 Reasons Why We Are Wrong about the World - and Why Things Are Better Than You Think. Sceptre.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "categorical_data_and_proportions.html#communicating-counts-and-proportions",
    "href": "categorical_data_and_proportions.html#communicating-counts-and-proportions",
    "title": "2  Categorical Data and Proportions",
    "section": "2.1 Communicating Counts and Proportions",
    "text": "2.1 Communicating Counts and Proportions\nLet us follow the history of infant mortality around the globe. When we looked at the year 1860 rates of infant mortality were very high, not much better than it had been all the centuries before industrialization has set in in Europe.\nBut then about 100 years later we already see a significant reduction in the Western countries, with rates reduced still much further until now in the most affluent countries. Let us look at the country group for which we had data in 1860 already, again today in 2020, before we go to the global picture. We will use this example to discuss some important aspects of communication counts and proportions.\n\n\nCode\n# Select the country group used in the historical 1860 data\n\nrate_2020 &lt;- infant_mortality[infant_mortality$Country %in% rate_1860$Country & infant_mortality$Year == 2020, c(\"Country\", \"Continent\", \"Mortality\")]\n\n# display the table and write text file of table\n\nlibrary(knitr)\nkable(rate_2020, digits = 4, row.names = FALSE)\n\n\n\n\nTable 2.2: Infant mortality in some European countries in 2020.\n\n\nCountry\nContinent\nMortality\n\n\n\n\nAustria\nEurope\n0.0030\n\n\nBelgium\nEurope\n0.0034\n\n\nDenmark\nEurope\n0.0031\n\n\nFrance\nEurope\n0.0034\n\n\nGermany\nEurope\n0.0031\n\n\nNorway\nEurope\n0.0018\n\n\nSpain\nEurope\n0.0027\n\n\nSweden\nEurope\n0.0021\n\n\n\n\n\n\nCode\nwrite.csv(rate_2020, file = \"tables/table_2_2_infant_mortality_2020.csv\", row.names = FALSE)\n\n\nThis is a spectacular improvement for this group of countries. By 2020 we have much more data, covering more regions and we will follow the global story later. But let us stick for the moment with this group of European Countries, which we followed over a time span of 160 year.\nWithin a bit more than a century the mortality rate has been reduced from 30 % to below 0.3 %. This amounts to a reduction by a factor of about 100. Isn’t this a stunning achievement?\nIn a recent book (Smil 2020), the Canadian researcher Vaclav Smil has pointed out that such low rates are impossible without the combination of a number of critical conditions, such as good healthcare in general, appropriate prenatal, perinatal and neonatal5 care, proper maternal and infant nutrition, adequate sanitary living conditions as well as access to social support for disadvantages families. All of these factors require relevant government and private spending and on infrastructures that can be universally used and accessed. Infant mortality is thus a very powerful indicator of quality of life in a country.5 prenatal is a word rooted in ancient Latin and means before birth. Perinatal means around the time of birth and neonatal means the first month after birth of a child.\nWhen data, such as counts and proportions are reported in a table we should make careful considerations how the data are precisely presented.\nFor instance in the table about infant mortality in some European countries in 1860, we could have reported the same information by presenting survival rates instead of mortality rates. Such a choice in reporting is generally known as framing.\nFraming can have effects on the impact of communication. Depending on how you frame the communication of data, the same information might affect and engage your audience differently. The same table with survival rates instead of mortality rates would the look like this.\n\n\nCode\nsrate_1860 &lt;- rate_1860\nsrate_1860$Survival &lt;- 1 - rate_1860$Mortality\n\n# table for 1860 survival rates\n\nsrate_1860 &lt;- with(srate_1860, srate_1860[ , c(\"Country\", \"Continent\", \"Survival\")])\n\nlibrary(knitr)\nkable(srate_1860, digits = 4, row.names = FALSE)\n\n\n\n\nTable 2.3: Infant survival rates in some European countries in 1860\n\n\nCountry\nContinent\nSurvival\n\n\n\n\nAustria\nEurope\n0.763\n\n\nBelgium\nEurope\n0.861\n\n\nDenmark\nEurope\n0.864\n\n\nFrance\nEurope\n0.850\n\n\nGermany\nEurope\n0.740\n\n\nNorway\nEurope\n0.898\n\n\nSpain\nEurope\n0.826\n\n\nSweden\nEurope\n0.876\n\n\n\n\n\n\nCode\nwrite.csv(srate_1860, file = \"tables/table_2_3_infant_survival_1860.csv\", row.names = FALSE)\n\n\nTake for instance the data for Germany 1860. If we had reported a survival rate of 74 % it might sound to many better than if we had reported the equivalent information of a mortality rate of 26 %. So whenever you report counts or proportions be mindful of framing effects.\nRisk impression can often be made more clear if we report expected frequencies as well\nas percentages. In this way the risk can be imagined as an actual crowd of people. For example we could visualize the infant mortality rate in Germany by creating a picture of the mortality rate, the actual number of cases per 1000 life births. If there are relatively few cases, as in 2020 Germany, this arbitrary normalisation leads to an artificial number such as 3.1 deaths per 1000 life births. Of course this is an artifact of the normalisation because there is nothing as an event with 0.1 deaths. If we had normalized to 10000 this would amount to 31 in 10000 which sounds more like an actual count. So for the visualization we take an infant mortality rate as infant deaths per 1000 life births to be 3.\nSuch a visualization could then look for example like this:\n\n\nCode\n#library(extrafont)\nlibrary(emojifont)\nlibrary(ggplot2)\n\nmy_waffle &lt;- function(x, rows = 5, use_glyph = 'square', glyph_size = 6,\n                      title = 'Waffle chart') {\n  \n  len &lt;- sum(x)\n  waffles &lt;- seq(len) - 1\n  nms &lt;- if(is.null(names(x))) seq_along(x) else names(x)\n  df &lt;- data.frame(xvals = waffles %/% rows,\n                   yvals = 1 - (waffles %% rows),\n                   fill = factor(rep(nms, times = x)))\n  \n  ggplot(df, aes(xvals, yvals, color = fill)) +\n    geom_text(label = fontawesome(paste('fa', use_glyph, sep = '-')), \n              family = 'fontawesome-webfont', size = glyph_size, show.legend = F) +\n    coord_equal(expand = TRUE) +\n    lims(x  = c(min(df$xvals) - 1, max(df$xvals) + 1),\n         y  = c(min(df$yvals) - 1, max(df$yvals) + 1)) + \n    theme_void(base_size = 16) +\n    labs(title = title, color = NULL) +\n    theme(plot.title = element_text(),\n          plot.margin = margin(4, 4, 4, 4)) \n} \n\nmy_waffle(c(\"Death\" = 260, \"Survived\" = 740), rows = 20, use_glyph = \"child\", glyph_size = 4, title = \" \")\n\n\n\n\n\nIn 1860 in Germany among 1000 newborns 260 infants died in the first year of their life.\n\n\n\n\nHere you visualize the numbers as differently colored crowds. There are 20 rows with 50 people symbols each. This multiplies to a crowd of 1000 people. It makes the magnitude of the mortality rate (as well as the survival rate) tangible. The share of infant death per 1000 life births in Germany in 1860 becomes now more tangible than reporting just a percentage in a table.\nWhen you plot the same graph for the data of 2020 the enormous improvement that took place within a time span of one and a half centuries in Germany become perhaps more obvious than just looking at the rates alone. The same kind of visualization for the 2020 data would then look like this\n\n\nCode\nmy_waffle(c(\"Death\" = 3, \"Survived\" = 997), rows = 20, use_glyph = \"child\", glyph_size = 4, title = \" \")\n\n\n\n\n\nIn 2020 in Germany among 1000 newborns 3 infants died in the first year of their life.\n\n\n\n\nFrom this perspective the improvement in infant mortality rates in Germany are truly spectacular.\nStill even at the low mortality rate this can mean a huge amount of individual tragedies. In 2020 Germany had 760.378 births. 0.31 % of this amounts to 2357 individual tragedies. Even if this rate could be brought down further, say to 0.28 % which sounds like a tiny improvement, it would mean 228 infant lives that could be saved.\n\n\n\n\n\n\nBe mindful of framing\n\n\n\nA standard we should strive for when reporting data is providing impartial information. For this we should think carefully about our framing and should perhaps provide both positive and negative frames.\n\n\nBut even if we achieve this, we should consider other things, like the ordering of the rows. Let us discuss this aspect in the next session together with your first steps in R."
  },
  {
    "objectID": "categorical_data_and_proportions.html#a-first-acquaintance-with-r-visualizing-infant-mortality-rates",
    "href": "categorical_data_and_proportions.html#a-first-acquaintance-with-r-visualizing-infant-mortality-rates",
    "title": "2  Categorical Data and Proportions",
    "section": "2.2 A first acquaintance with R: Visualizing infant mortality rates",
    "text": "2.2 A first acquaintance with R: Visualizing infant mortality rates\nWhile a table might be the medium of choice for displaying data it is often more powerful to convey information visually. The example of how imagining actual crowds might give us sometimes a more tangible picture of the same information, was one illustration of this.\nToday when we visualize data as one important tool of data exploration, the tool of choice is a computer and an appropriate language that enables us to tell the computer what to do.\nFor us in this course this will be the R language and now we start to learn the first steps in this language.\n\n2.2.1 Starting an quitting R\nWe assume that the computer you work with has a recent version of R installed. R will work with the most common operating systems such as Windows, OSX or Linux.66 Installing R yourself is not complicated. To install R yourself you need a computer where you have the privileges to install software and you need an internet connection. You get the newest version of R at the website https://cran.r-project.org/. A screenshot of the website is here \nWe will use R in two ways: First from the R command line and then also by working with Jupyter Notebooks.77 Another very powerful and popular program to work with R and R code is RStudio, which you can find here https://www.rstudio.com/. RStudio is a so called IDE, an integrated development environment. If you have access to a computer with internet connection and the permissions to install software yourself, feel free to experiment also with this tool. The basic version is free of charge. Again you need a computer where you have the privileges to install software and you need an internet connection.\nThe notebooks will not only allow to write R code and send this code to R for execution. Notebooks also allows you to store commands, to comment them and make them available as files for later use. This is especially useful, once you work on longer and more complicated tasks. It is then essential that you can reproduce what you did in the past days, that you can write easily readable comments and that you can collaborate in a team, where you can share your work with others. This circle of collaborators includes your future and past self.\nLet us start the R command line or the R-console, as it is called, introducing the use of R via the notebooks a bit later.\nYou start R by either typing R into the terminal or by clicking the R icon on your computer. The R console shows a prompt, a symbol that looks like this &gt;. When you see the prompt in the R console, R is ready to receive commands.\nTo end your R session write quit() at the prompt &gt; of the console. Congratulations. Now that you can start and quit R we are ready to go.\n\n\n2.2.2 First steps\nHere is an easy command you can send to R. Just try to type 1 + 1 at the prompt\n\n1 + 1\n\n[1] 2\n\n\nSure enough, R gives you the result of the addition, which is 2. But what is [1]? This is just a row label. If there were more outputs to your command, then they would be labelled [2], [3], [4], and more. We will go into this aspect of R’s output display later in more detail.\nSo R can do all the usual computations. For instance if you knew that in Germany in 1860 there were 270 infant death per 1000 life birth you could compute the mortality rate in a decimal format by dividing the counts by 1000, because the convention is to report mortality as cases per 1000 life briths. The R command for division is /. So by typing\n\n270/1000\n\n[1] 0.27\n\n\nyou will get the mortality rate 0.27 or 27 %.\nYou can do all the usual arithmetic operations, like with a calculator in R. For instance subtraction\n\n3-2\n\n[1] 1\n\n\nOr multiplication\n\n10*10\n\n[1] 100\n\n\nYou can raise a number to the power of another, like\n\n3^2\n\n[1] 9\n\n\nand of course you can combine all of these operations:\n\n(1+3)^2 - 5*4 + 12^3 - (13/2)\n\n[1] 1717.5\n\n\nThe comma is represented in R as a dot .. So the above output reads 1717and one half or 0.5.\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance:\n5*\nIn this case R will show the expression 5* followed by a + instead of showing a new prompt. This means that the expression is incomplete. When R shows + after entering a command instead of the output and a new prompt, it means that it expects more input. If we complete the expression, the expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Errors are usually printed in red, and it instinctively might create a feeling of alarm. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n\n5%3\n\nError: &lt;text&gt;:1:2: unexpected input\n1: 5%3\n     ^\n\n\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, which is particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity.\nWith this knowledge we can already do a first example, by continuing our discussion on infant mortality data we started in this lecture. On the way we learn a few more things about R and the R language.\n\n\n2.2.3 Storing and reusing results\nWhen our operations become just a bit more complex than just typing in a simple arithmetic operation, it becomes useful if we can store answers and use these answers, which might be an intermediate result of some transformed data or something else. In R this problem is solved very easily. We assign the answer to a name we choose ourselves. Here is an example.\n\na &lt;- 1+1\n\nThe symbol &lt;- tells R pleas assign to the name a the result of the computation 1+1.88 The assignment operator is used so often that it is useful to type it using a keyboard shortcut instead of typing first &lt;and then -. The same result can be achieved by pressing the ALTkey followed by the - key. \nNow see why we have said that the assignement has stored your result. Enter on your keyboard the name you have just chosen:\n\na\n\n[1] 2\n\n\nAnd you can use the stored value to do further computations with it, like\n\na^2\n\n[1] 4\n\n\nWhen you assign a new value to the old name a, the old value will be overwritten by the new value.\nWhat names should you use? You could uses actually anything but you have to follow a few rules. A name in R must for example not begin with a number, a dot . or an underscore _. So for example var_1, var.1, var1 and VAR1 and myVar1 are all allowed names but 1var, .var and _var1 are not"
  },
  {
    "objectID": "categorical_data_and_proportions.html#categorical-variables-causes-of-infant-mortality",
    "href": "categorical_data_and_proportions.html#categorical-variables-causes-of-infant-mortality",
    "title": "2  Categorical Data and Proportions",
    "section": "2.4 Categorical variables: Causes of infant mortality",
    "text": "2.4 Categorical variables: Causes of infant mortality\nWe have discussed binary data. These are data that can take two values, like death or alive, in the examples of infant mortality data we have studied so far. A generalization of binary variables are called categorical variables in statistics. Categorical variables are measures that can take two or more values, which can be either unordered, such as eye color, countries or study center locations at which JWL courses take place. They can also be ordered, like positions in a hierarchy.\nAn example of categorical data arises if we study the issue of infant mortality further and ask for the causes. Why do infants die in the first year of their life?\nHere are the causes that have been registered by the Global Burden of Disease Study in 2019 by the Institute for Health Metrics and Evaluation as well as the share of each cause in the overall cases.1010 See https://www.healthdata.org/gbd/2019\n\n\nCode\n# read data from JWL package\n\nlibrary(JWL)\ncauses &lt;- infant_mortality_causes\n# order by share\ncauses_ordered &lt;- causes[order(-causes$Share) , ]\n\ndat &lt;- causes_ordered[, c(\"Entity\", \"Share\")]\nnames(dat) &lt;- c(\"Cause\", \"Share\")\n\n\n\n\nCode\nlibrary(knitr)\nkable(dat, digits = 3, row.names = FALSE)\n\n\n\n\nTable 2.4: Causes of infant mortality\n\n\nCause\nShare\n\n\n\n\nPreterm birth\n0.211\n\n\nEncephalopathy due to birth asphyxia and trauma\n0.180\n\n\nLower respiratory infections\n0.161\n\n\nBirth defects\n0.128\n\n\nDiarrheal diseases\n0.092\n\n\nHeart anomalies\n0.048\n\n\nMalaria\n0.044\n\n\nSyphilis\n0.026\n\n\nMeningitis\n0.020\n\n\nWhooping cough\n0.016\n\n\nNutritional deficiencies\n0.015\n\n\nDigestive anomalies\n0.013\n\n\nSudden infant death syndrome\n0.009\n\n\nTuberculosis\n0.008\n\n\nMeasles\n0.006\n\n\nHIV/AIDS\n0.006\n\n\nDigestive diseases\n0.005\n\n\nTetanus\n0.005\n\n\nEncephalitis\n0.003\n\n\nAcute hepatitis\n0.002\n\n\nDiabetes and kidney diseases\n0.002\n\n\n\n\n\n\nCode\n#write.csv(dat, file = \"tables/table_2_4_infant_mortality_causes.csv\", row.names = FALSE)\n\n\n\n\n\n\n\n\nThis is how an answer can look like.\n\n\n\n\n\nSince students have not yet learned subsetting, they need to use the table and retype the values and the causes manually. This is intended because it will familiarize them with the c() function, the difference between numerical and character data type and is an excellent opportunity to review what we did on simple R bar-charts. In the example cause below I use - of course - the convenience of subsetting\n\n\nCode\nbarplot(dat$Share, \n        names.arg = dat$Cause, horiz = TRUE, las = 1, cex.names = 0.3)\n\n\n\n\n\n\n\n\nIn the news but also in many publications you will often find proportions and how they add up to a total represented in so called pie charts. A pie chart for the causes of infant mortality visualizing our data will look like this\n\n\nCode\npie(dat$Share, labels = dat$Cause, radius = 1, cex = 0.5)\n\n\n\n\n\nNow compare this to a display of the same information using the bar chart.\n\n\nCode\nbarplot(dat$Share, \n        names.arg = dat$Cause, horiz = TRUE, las = 1, cex.names = 0.5)\n\n\n\n\n\nApart from some imperfections of displaying the labels in a easily readable way, why does the bar chart work much better for comparing the proportions of causes than the pie chart?\nThe reason is that the bar chart is supported better by by our visual perception capacity. In the case of the bar chart we have to compare lengths, which our visual system can do well. For decoding the pie we have to compare areas of slices or angles formed by the slices at the circle center. But our visual system does not work well for such tasks. It works even worse, if the display is a three dimensioal pie chart, as offered by various apps. In this case the pie would look like this:\n\n\nCode\nlibrary(plotrix)\npie3D(dat$Share, labels = dat$Cause, radius = 1, labelcex = 0.5)\n\n\n\n\n\nNow it is even worse, because our visual system now has to additionally decode perspective on top of area, something it is naturally also not good at.\nThere are ways to visually display data that work because they are naturally supported by our ability of visual perception and cognition, while others are not.\n\n\n\n\n\n\nDon’t use pie charts to visually compare proportions\n\n\n\nWhen you display proportions choose bar charts and avoid pie charts. Bar charts are naturally supported by the ability of our visual system to compare lengths, while pie charts require the comparison of areas and angles, a task our visual system is not so good at."
  },
  {
    "objectID": "categorical_data_and_proportions.html#compairing-pairs-of-proportions",
    "href": "categorical_data_and_proportions.html#compairing-pairs-of-proportions",
    "title": "2  Categorical Data and Proportions",
    "section": "2.5 Compairing pairs of proportions",
    "text": "2.5 Compairing pairs of proportions\nOften proportions or percentages are used to communicate risks by comparing pairs of proportions.\nWhen pairs of proportions are compared, we need to understand how such comparisons can be made in a meaningful way. Especially in the media it is popular to often run spectacular headlines of how specific behaviors affect your likelihood of developing a disease.\nMuch of the spectacle in the headlines is due to the exclusive reporting of relative risks. It is important to understand that such statements need to be put into context by also reporting baseline of absolute risk.\nLet me illustrate the issue using the example of consuming processed meat and the risk of developing bowel cancer.\nFrom epidemiological research it is known that the chance of a person of developing bowel cancer is based on factors such as meat consumption, the level of physical activity, the body weight or income.\nThe relative risk is the risk of developing bowel cancer in a group of people compared to another group of people with different behaviors, different physical conditions of different environments.\nFor instance when we compare meat eaters versus vegetarians, a statement about relative risk would be that the consumption of processed meat increases the risk of developing bowel cancer by 18 %. This sounds spectacular but it does not tell the full story.\nTo see this go back to our previous example of imagining crowds of people instead of percentages.\n\n\nCode\nmy_waffle(c(2, 8), rows = 2, use_glyph = \"male\", glyph_size = 16, title = \" \")\n\n\n\n\n\nHere we have an absolute risk of two out of 10 persons developing bowel cancer. If the relative risk would increase by 50 % this would mean in terms of absolute risk that now we have\n\n\nCode\nmy_waffle(c(2, 1, 7), rows = 2, use_glyph = \"male\", glyph_size = 16, title = \" \")\n\n\n\n\n\nThe risk increases now to three out of 10.\nThese examples illustrate that you need baseline or absolute risks as a vital information to understand what an 18 % increase in risk really means.\nGoing back to the example of processed meat consumption and bowel cancer risk, the estimated lifetime risk of developing bowel cancer is 5.6 %. Expressed as a relative frequency this is about 6 out of 100 or in a chart\n\n\nCode\nmy_waffle(c(6, 94), rows = 5, use_glyph = \"male\", glyph_size = 13, title = \"\")\n\n\n\n\n\nThe estimated life time risk of developing bowel cancer if you eat 50 g of processed meat per day increases your relative risk by 18 %. At this heightened risk level the new absolute risk is now 6.6 % or about 7 out of 100\n\n\nCode\nmy_waffle(c(6, 1, 93), rows = 5, use_glyph = \"male\", glyph_size = 13, title = \"\")\n\n\n\n\n\nIn the research literature proportions are often expressed by the odds ratio. This ratio expresses the chance of an event happening relative to the chance of an event not happening. In the example of processed meat and bowel cancer this would be \\(6/94\\), because 6 out of hundered people develop bowel cancer in their life time and 94 out of hundred do not. While odds are very common in the research literture they are not a very intuitive way to communicate the comparison of proportions. Spiegelhalter, from whom I took this example therefore recommends not to use odds ratios outside a scientific context, since it easily invites misunderstanding."
  },
  {
    "objectID": "categorical_data_and_proportions.html#activities-in-the-study-center",
    "href": "categorical_data_and_proportions.html#activities-in-the-study-center",
    "title": "2  Categorical Data and Proportions",
    "section": "2.5 Activities in the study center",
    "text": "2.5 Activities in the study center\n\n2.5.1 The story of infant mortality around the globe\nFor this activity we continue to look into the issue of infant mortality around the globe by working with a Jupyter notebook. To work on this assignment, please open the notebook infant_mortality_global.ipynb with the jupyter notebook. The notebook contains some text and some code which you can interactively execute from the notebook, as we already did it in the assignments in the Introduction, when we looked at the share of poverty in various countries. In this notebook we continue to dive deeper into the story of infant mortality around the globe. The things you learn when you work through this assignment will probably surprise you."
  },
  {
    "objectID": "categorical_data_and_proportions.html#a-data-collection-exercise-for-the-next-unit",
    "href": "categorical_data_and_proportions.html#a-data-collection-exercise-for-the-next-unit",
    "title": "2  Categorical Data and Proportions",
    "section": "2.6 A data collection exercise for the next unit",
    "text": "2.6 A data collection exercise for the next unit\nMaybe handedness score?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAntony, Volk, and Atkinson Jeremy. 2013. “Infant and Child Death in the Human Environment of Evolutionary Adaptation.” Evolution and Human Behavior 34: 182–92.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for Communication. New Riders.\n\n\nRoser, Max. 2019. “Child Mortality Is an Everyday Tragedy of Enormous Scale That Rarely Makes the Headlines.” https://ourworldindata.org/child-mortality-everyday-tragedy-no-headlines.\n\n\nSmil, Vaclav. 2020. Numbers Don’t Lie: 71 Things You Need to Know about the World. Penguin Books.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#the-histogram",
    "href": "summarizing_and_communicating_lots_of_data.html#the-histogram",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.1 The histogram",
    "text": "3.1 The histogram\nTo summarize data, statisticians often use a graph which is called a histogram. In this section we will discuss all you have to know about histograms and how to use them.\n\n3.1.1 Constructing a histogram\nSince real world examples in this exposition come from anthropometry, let us look at a sample of 100 observations from a set of real world data about human adults of age 18 and above, where the data record their height in cm.4 I have stored these values in an R object which I called heights. So in this discussion I refer to the 100 vaues shown below as heights.4 These data are collected from the internet. For details you can look at the source at http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights\n\n\nCode\nlibrary(JWL)\ndat &lt;- socr_height_weight\ndat$Height &lt;- dat$Height*2.54\n\nset.seed(1)\nheights &lt;- sample(x = dat$Height, size = 100) # randomly select 100 values without replacement.\nheights\n\n\n  [1] 171.5912 174.0172 175.5666 173.7980 172.0488 171.9966 168.0614 185.3618\n  [9] 174.4489 174.2083 174.8307 171.6123 175.5515 169.8454 174.2598 177.0208\n [17] 176.0683 178.8391 168.3490 171.3392 176.3523 171.3722 173.0217 164.0692\n [25] 171.0165 180.5622 171.1445 177.4639 167.0146 175.9097 170.9656 173.5110\n [33] 166.4426 176.2078 169.4876 176.6579 179.6563 170.2477 167.9883 176.6037\n [41] 173.6044 167.8685 164.5033 161.3102 176.2323 184.1186 175.9741 178.7940\n [49] 162.6569 175.3654 175.5658 164.5769 167.4618 180.5760 176.4807 167.1875\n [57] 170.7098 179.8085 162.4880 182.7100 173.1352 175.5756 169.5662 166.9201\n [65] 176.7185 173.8010 177.2146 164.5343 173.0048 167.5625 175.7398 174.0607\n [73] 182.0625 169.7786 171.2853 172.5116 173.4802 171.7511 172.1317 172.3667\n [81] 172.1702 169.1945 169.0105 171.7650 173.7279 165.9003 175.6967 174.5598\n [89] 177.1643 175.0406 171.9614 173.3289 169.6971 172.8287 170.3538 175.8446\n [97] 169.3031 170.4622 179.2061 167.5248\n\n\nThis is how R prints out the 100 height values. You already learned what the numbers in brackets on the left side mean. They are counters or observations. In total we have 100 values.\nWe start the construction of a histogram by choosing for the horizontal axes ranges of numerical values - in our case of the height data - which are called bins or classes. There is no fixed rule as to how to choose the size of these ranges. These ranges should neither be too fine, nor too coarse. While there are lists of mechanical rules, which you can for example find on Wikipedia5, it is usually best to use your domain knowledge and some experimentation to find out the bin size that works best for your data.5 See https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width\nFor this example, assume we had chosen a bin size of 5 cm. When you study the list, you will find that the lowest value is at 160.12 cm while the highest value is at 185.16. This is already quite tedious to find out by eyeballing the numbers with the small number of values we have chosen for this example. It is impossible to do for really large data sets.\nNow lets make a distribution table like this:\n\n\n\nHeight-bin\nFrequency\n\n\n\n\n160 - 165\n7\n\n\n165 - 170\n20\n\n\n170 - 175\n39\n\n\n175 - 180\n28\n\n\n180 - 185\n5\n\n\n185 - 190\n1\n\n\n\nIn the column Height-bin we have recorded the bins in steps of 5 and in the right column, Frequency, we have recorded the count of values that are in this bin.\nWhen we make such a tabulation we have to agree on an endpoint convention. This is important, since when a height value would for instance be measured as exactly 165, in which bin should it be counted: 160-165 or 165-170? You, the constructor of the histogram, has to take this decision. Let us agree on the convention that when a value falls exactly at the endpoint of the bin, we put it in the next bin. In practice you will usually do a histogram by computer. The code of the computer program has to specify an endpoint convention, so the computer knows what to do when a value coincides with an endpoint.\nOn the Frequency axes you put the frequency scale: Counts of values. Then for each bin, you plot a bar, which has the width of the bin and the height of the frequency. Do this for all the bins you have tabulated and you are ready.\nThe histogram provides a certain aggregation of the data because it sorts the 100 data points into 6 bins, in our example. While loosing some local information on individual data points the global information conveyed by the summary gives us a pretty good idea of the overall pattern of variation on the human height data.\n\n\n\n\n\n\nSeitwerk: Expand for reading comment.\n\n\n\n\n\nReda, I think it would be great if you could do some animation that reproduces the steps: Plot a height axis with bins from 160 to 190 in bin sizes of 5, then plot a frequency axes from 0 to 40 in steps of 10. Them from the list of 100 numbers, collect all that are recorded in the first bin and then make the height proportional to the count, then the next etc. The idea would be to realize something like my previous sketch for the Nile data \n\n\n\nWe can see, for instance, that the most frequent height is between 170 and 175 and that the variation is fairly symmetric around this bin. In the extremes this most frequent value can half or shrink even more, so there is quite some spread in the data.\nIf we had just plotted all individual data points, we would have got a picture like this.\n\n\nCode\nplot(as.numeric(heights), xlab = \"Observation\", ylab = \"Height in cm\", pch = 16)\n\n\n\n\n\nYou will agree that as a summary of the data it is not particularly useful and not as informative as the histogram.\nHistograms are such a common tool in statistics to explore the variation in one variable and the shape, how it is roughly distributed that every statistical software has functions to produce histograms.\nIn R, the language we use in this course, there is also such a function. The function name is called hist() and it takes the data as an argument. This is the second graphic function of R you encounter in this course after we played with the barplot()function in the last lecture.\nTo produce a histogram from the height data with R, we type, assuming that we have stored the 100 heights data we printed above in a variable called heights.\n\nhist(heights)\n\n\n\n\n\n\n\n\n\n\nNow you try\n\n\n\nLet us check your understanding of histograms by a little quiz now. The histogram below shows the distribution of the final score in a certain class.\n\nWhich block represents the people who scored between 60 and 80?\nTen percent scored between 20 and 40 about what percentage scored between 40 and 60?\nAbout what percentage scored over 60?\n\n\n\n\nFinal Score\n\n\n\n\n\n\n3.1.2 The relative frequency scale: Absolute versus relative frequency\nSometimes it might be useful, to choose a different scale for the y axes of your histogram. Instead of absolute frequencies (or counts) it might be useful to show relative frequencies, the proportion of occurrences in each bin. The type of scale you choose will depend on what kind of comparisons you want to emphasize about your data.\nAssume you would want to draw your histogram such that on the y-axis you do not see the absolute counts of how many individuals in your data fell into a certain hight bin but you want to see instead these numbers as a percentage of the whole dataset.\nThis is sometimes convenient and in some cases more informative. This choice of a different scale is called the relative-frequency-scale. The construction of the histogram follows the same principles as we have already discussed but now we just quantify the counts in the bins differently as shares of all observations.\nLet’s do the histogram for the heights data in a relative frequency scale. In our table, which we used to construct the histogram before we had in total 100 observations. A count of 7 is therefore 7 % or 0.07 in the relative frequency scale etc. Thus our table would now be:\n\n\n\nHeight-bin\nFrequency\n\n\n\n\n160 - 165\n0.07\n\n\n165 - 170\n0.20\n\n\n170 - 175\n0.39\n\n\n175 - 180\n0.28\n\n\n180 - 185\n0.05\n\n\n185 - 190\n0.01\n\n\n\nand the histogram in relative frequency scale would look like this:\n\n\nCode\nh&lt;-hist(heights, plot=F)\nh$counts &lt;- h$counts / sum(h$counts)\nplot(h, freq=TRUE, ylab=\"Relative Frequency\")\naxis(2, at = c(0,0.1,0.2,0.3,0.4), labels = c(0,0.1,0.2,0.3,0.4))\n\n\n\n\n\nUnfortunately R has no standard argument to its hist()function to draw a histogram with a relative frequency scale. To produce a histogram like shown here we would need some more advanced coding than we know at the moment.6 I will come back to explain how to do a histogram with relative frequency scale in R as soon as we have learned the appropriate syntax to implement this.6 The hist() function in R has an argument, called freq. It can take the value TRUE or FALSE. If the argument is set to FALSE the histogram is shown such that the y axis has yet another scale, the so called density scale. This scale is chosen such that the total area of the histogram is 1.\n\n\n3.1.3 Best practices for histograms\nWhen you summarize lots of data by a histogram there are some things you should consider carefully. Let us go through the most important best practice principles for histograms.\n\n3.1.3.1 Bin size\nWhen doing exploratory data work it is usually a good idea not to look at a single histogram but at several histograms of the same data by changing the bin size. There is no clear rule about the optimal bin size. It often depends on context and field knowledge.\nIf the bins are to fine, then the data will be be very noisy and give no overview because they show too many individual points. On the other hand if the bins are too wide, they will not show you the overall variability in the data very well and you fail to get a good idea about the distribution.\nLet us illustrate this point using our heights data.\nIn the first case we have chosen 100 bins, which is too fine. There is almost one bar for every single data point. In this way we have a lot of spurious peaks and throughs and can not see the variation pattern in the data very clearly\n\n\nCode\nhist(heights, breaks = seq(min(heights), max(heights), length.out = 100))\n\n\n\n\n\nNow here we have the other extreme, lets assume we have only 3 bins. This would give us a pattern like this.\n\n\nCode\nhist(heights, breaks = seq(min(heights), max(heights), length.out = 3))\n\n\n\n\n\nHere the histogram is too coarse and we do not see the variation pattern either.\nThe computer usually has a built in rule of thumb for the histogram which will work well in most of the cases. Still for individual datasets it is sometimes better to choose a different bin size that more adequately mirrors the variation in the data.\n\n\n3.1.3.2 Choose boundaries that can be clearly interpreted\nTick marks and labels should fall on the bin boundaries. As in the examples discussed so far, they need not be there for every tick but it is enough if they are there between every few bars. Bin labels should also have not many significant digits, so they are easy to read. So bin sized which divide 10 and 20 evenly are easier to read than bin sizes that do not. So always take caution not to arbitrarily split bin sizes. Otherwise you can end up with off bin boundaries.\nFor example, if we just took the maximum and the minimum of the heights data and arbitrarily divided them into 7 bins, we would get the difficult to read bin boundaries\n\n\nCode\nseq(min(heights), max(heights), length.out = 7)\n\n\n[1] 161.3102 165.3188 169.3274 173.3360 177.3446 181.3532 185.3618\n\n\ninstead of the more easily readable boundaries\n\n\nCode\nseq(160,190,5)\n\n\n[1] 160 165 170 175 180 185 190\n\n\n\n\n3.1.3.3 What’s the difference between a histogram and a bar chart?\nA histogram depicts the frequency distribution of a continuous, quantitative variable, such as height, weight, time, energy consumption etc. These are variables that can take on any value and these values can be ordered from smallest to highest.\nWhen we have a categorical variable, like we encountered them in section 2, we need to use a bar chart. The bars of the bar chart typically will have a small gap between the bars, emphasizing the discrete nature of the variable. The categories in a bar chart usually have no natural ordering. As we discussed in section 2, we have even to be conscious how we display the categories to avoid suggesting an order that is in fact not there in the data."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#the-average-and-the-standard-deviation",
    "href": "summarizing_and_communicating_lots_of_data.html#the-average-and-the-standard-deviation",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.2 The average and the standard deviation",
    "text": "3.2 The average and the standard deviation\nWith a histogram we can summarize a large amount of data and get some insights about the variation in the data. Often we can summarize data much more drastically by just one number describing the center of the histogram and the spread around this center. When I write center an spread here, these are just ordinary words with no special technical meaning.\nWhen we do statistics we need precise definitions and we will study and learn these definitions in this section. The average is often used to find the center. The standard deviation measures spread around the average. 77 When the distribution is not symmetric around the mean it is better to take other maesures of center and spread. These alternative measures are called the median. The interquartile range is another alternative measure of spreads. We will learn the details about these measures later in the course.\nBefore we go into these definitions, let me show for a start two histograms, both have the same center, but the second one is more spread out, there are more observations farther away from the center.\n\n\n\n\n\n\n\n(a) Histogram 1\n\n\n\n\n\n\n\n(b) Histogram 2\n\n\n\n\nFigure 3.1: Histogram 1 and Histogram 2 have the same center but Histogram 2 is more spread out\n\n\nThese distributions can be summarized by the center and the spread. But what about a situation like this?\n\n\n\nFigure 3.2: A bimodal distribution\n\n\nIn some cases such distribution can occur naturally. Think, for example, about the distribution of the elevation data of surface area of the earth. Most of the surface area of the earth is taken by the sea floor at about three miles below the sea level or the continental planes around sea level. If we would report only the center an the spread of this histogram, plotted in the picture as the red and blue vertical lines, we would miss these peaks.\n\n3.2.1 The average\nLet us come back to the height data, we have analyzed in a histogram before as the context for which we study the concept of an average.\n\n\n\n\n\n\nAverage\n\n\n\nThe average of a list of numbers is defined as their sum, divided by how many numbers their are in the list.\n\n\nFor example the average of the list \\(L\\)\n\nL &lt;- c(9,1,2,2,0)\n\nwould be computed as:\n\\[\\begin{equation}\n\\frac{(9+1+2+2+0)}{5} = \\frac{14}{5} = 2.8\n\\end{equation}\\]\nBy now it will be no surprise for you that R provides a function for computing averages. This function is known by the alternative name for the average, called the mean.\nHere is how you would compute the mean using R.\n\nmean(L)\n\n[1] 2.8\n\n\nwhich is indeed what we should get as a result.\nLet’s go back to the issue of human height data and take our data set of 100 observations of the height of adult humans. Let us recall our histogram and give it some nice title and a precise label for the x-axis:\n\nhist(heights, xlab = \"Height in cm\", main = \"Height of 18 year old humans\")\n\n\n\n\nThe distribution looks symmetric. If we summarize these data by taking an average in one number we will capture the center of the distribution fairly well. Let us compute the average, using the mean function.\n\naverage_height &lt;- mean(heights)\naverage_height\n\n[1] 172.8044\n\n\nThe average height in this dataset is about 172.8044426 cm. The average is a very powerful way of communicating data by compressing many observations into one single number, the mean.\nThis compression is, however, only achieved by loosing some information on individual differences. For example in our dataset the average height is 172.8044426 cm.\nBut there are about 6 % who are larger than 180 cm and there are also about 7 % who are smaller than 165 cm. With 100 individuals these are individuals who are beyond these thresholds. This diversity is hidden in the aggregation.\n\n\n\n\n\n\nHow to compute percentages of observations that fulfill a condition.\n\n\n\nThis is a good opportunity to show you a cool feature or trick in R how we could in one line compute such percentages. Assume we want to compute the percentage of individuals in our data who are larger than the mean of 172.8044426. How can we do this?\nLogicals are a powerful data type to make such a computation. A logical is a special data type, similar to the data type numerical, used for representing numbers, or the data type character, representing strings. A logical can either be TRUE or FALSE and will be the output of an R operation testing a condition.\nSay we have 10 numbers, given by\n\nx &lt;- c(4,5,1,2,4,2,0,10,11,6)\n\nNow we could ask R which entries of x are larger than 2 by typing\n\nx &gt; 2\n\n [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nThe output is a vector of logicals where R compares each entry in xwith 2 and if it is larger it returns TRUE and otherwise FALSE (this includes all entries which are exactly 2).\nNote that, when you apply the function mean() to a vector of data of type logical R performs a computation by coercing TRUEto the number 1 and FALSE to the number 0. We will learn about the details of these R the coercion rules later in more detail. If you apply mean() to x you will get the proportion of values fulfilling a certain condition. So if you take the mean of\n\n\nCode\nx &gt; 2\n\n\n [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nR would firts transfrom the vector\nTRUE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE\ninto a vector\n1,1,0,0,1,0,0,1,1,1.\nWhen you take the mean \\[\\begin{equation}\n\\frac{1+1+0+0+1+0+0+1+1+1}{10}\n\\end{equation}\\] you just get the proportion of 1, i.e. cases where the condition \\(x &gt; 2\\) is TRUE, because the 0-es do not contribute to the value of the sum, only the 1-s.\nThis insight can be used to compute the percentage of values that fulfill a certain condition in R. Here the proportions of heights larger and smaller than the mean.\n\nmean(heights &gt; mean(heights))\n\n[1] 0.52\n\nmean(heights &lt; mean(heights))\n\n[1] 0.48\n\n\nHere you see the symmetry of the distribution numerically. About 50 % are larger and also smaller than the mean.\n\n\n\n\n3.2.2 The standard deviation\nWhen summarizing and communicating lots of data, it is useful not only to report at which value the center of the distribution is. It is often helpful to also think about the way how the values spread around the average. The quantity which measures this spread is called the standard deviation. It can be interpreted as an average deviation.\nLe us go back to our data on human height. There were 100 observations in our sample. The average height was 172.8 cm.\nThis average tells us that most of the humans measured in the sample had a height of around 1 m and 73 cm. But there were deviations from this average. Some humans were taller and others were smaller. We can ask how big these deviations are? In answering this question we need the concept of the standard deviation.\n\n\n\n\n\n\nStandard deviation\n\n\n\nThe standard deviation says how far away numbers on a list are from their average. Most entries on the list will usually be somewhat around one standard deviation from the average. Very few will be more than two or three standard deviations away.\n\n\nIn R you can compute the standard deviation by using the function sd(). So let us compute the standard deviation of our height data using the sd()function.\n\nsd(heights)\n\n[1] 4.726364\n\n\nThe standard deviation is at about 4.73 cm. That means that many humans differed from the average height by about 1, 2, 3, 4 or about 5 cm. Let us use R to compute the percent of observation that are within one standard deviation from the average.\nLet us compute the percentage of observations, which are within 1 standard deviation from the mean height. Do you remember the trick with using logicals?\nLet us compute the percentage of observations which are larger than the average height minus und standard deviation and smaller than the average height plus one standard deviation. The R symbol for and is &. Let us use it to formulate the condition:\n\nmean(heights &gt;= mean(heights) - sd(heights) & \n     heights &lt;= mean(heights) + sd(heights))\n\n[1] 0.71\n\n\nIn our case these are 71 % of all observations.\nSometimes it is convenient to subtract the mean from every observation and and express the units in terms of standard deviations. By construction this is a variable with mean 0 and standard deviation 1. This is called standardization or normalization. Let me show you how this works.\nWe transform our heights data to \\[\\begin{equation*}\nz_i = \\frac{x_i - \\mu}{\\sigma}\n\\end{equation*}\\] and compute the mean of \\(z_i\\) and its standard deviation. We can use R to do that:\n\nz &lt;- (heights - mean(heights))/sd(heights)\n\nmean(z)\n\n[1] 2.699057e-15\n\nsd(z)\n\n[1] 1\n\n\nNow you see that the mean is 0 and the standard deviation is 1. The mean in the R computation is not exactly 0. R says it is 2.699057e-15. What does this mean? This is a way of writing 0.000000000000002699057 in a special notation. Do not worry for now how this notation works exactly. What you can see is that this is an very very small number, practically the same as 0. In R it is not exactly zero because of rounding errors of the computer. For practical purposed the value is as good as zero. This must be the case as a consequence of how we changed the units. The mathematically inclined among you might try to derive this fact more generally using the definition of the mean and the standard deviation.\nNow if we ask the same question as before. Which proportions of \\(z\\) are within -1 and + 1 standard deviation from the mean, we get\n\n\nCode\nmean(z &gt;= mean(z) - sd(z) & \n     z &lt;= mean(z) + sd(z))\n\n\n[1] 0.71\n\n\nThis is exactly the same value as we got before. Our transformation was just about changing the units in which our data are measured. This change of unit does not change the distribution. So no matter whether we work with \\(x\\) the original heights in cm or with the \\(z\\) the normalized data, the distributional properties do not change. In statistics if data are normalized in this way the normalized values are also often referred to as the z-score.\n\n\n3.2.3 Computing the standard deviation\nUsually we will compute the standard deviation using the computer and only in rare cases will we ever compute a standard deviation by hand. Still it is important that you understand how the computation works and what is actually been computed. Here is how.\nExample 1: Find the standard deviation of the list 20, 10, 15, 15\nStep 1: We first need to find the average, which is \\[\\begin{equation}\n\\frac{20 + 10 + 15 + 15}{4} = 15\n\\end{equation}\\]\nStep 2: We next need to find the deviation from the average. In order to do so, we just subtract the average from each entry \\[\\begin{eqnarray}\n(20-15)&=5\\\\\n(10-15)&=&-5\\\\\n(15-15)&=&0\\\\\n(15-15)&=&0\n\\end{eqnarray}\\]\nStep 3: Now we have to square each one of these differences and take the square root, which is often called the root mean square\n\\[\\begin{eqnarray}\nsd&=&\\sqrt{\\frac{5^2 + (-5)^2 + 0^2 + 0^2}{4}} \\\\\n&=& \\sqrt{\\frac{25 + 25 + 0 + 0}{4}} \\\\\n&=& \\sqrt{\\frac{50}{4}}\\\\\n&=& \\sqrt{12.4}\\\\\n&\\approx& 3.5\n\\end{eqnarray}\\]\nThe standard deviation has the same units as the data. For example, when we measure height in cm then at the squaring step the units change to \\(cm^2\\) but the squre root returns \\(cm\\) again.\n\n\n\n\n\n\nNow you try\n\n\n\n\nGuess which of the following two lists has the larger standard deviation. Check your guess by computing the standard deviation for both lists.\n\n\n9,9,10,10,12\n7,8,10,11,11,13\n\n\nCan the standard deviation ever be negative?\nFor a list of positive numbers can the standard deviation ever be larger than the average?"
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#sec-moreR",
    "href": "summarizing_and_communicating_lots_of_data.html#sec-moreR",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.3 Bigger datasets and more R",
    "text": "3.3 Bigger datasets and more R\nWhen we use the computer to summarize and communicate lots of data we need to get to know more of R. In particular we need to understand how to read data and select and manipulate variables. Let me lead you through some of the most basic concepts you need to know in order to do so. Finally we will apply our new skills to the hand on analysis of a real world anthropometric dataset from the DHS survey that help us to get a global picture about the nutritional situation for children and how this information could support us in setting policy priorities.\n\n3.3.1 Vectors and indices\nSay that for some reason we would be interested in the 50th observation in our data on human height. How would we tell R to select this observation from our data?\nIndividual elements can be accessed by using an index or a set of indices. The index is specified by using brackets []. For example\n\nheights[5]\n\n[1] 172.0488\n\n\ngives the 5th element of our heights data. You can go to the beginning of this chapter to verify that we have indeed captured this element. 5 is the index in this example. To capture the 50the element, we would type\n\nheights[50]\n\n[1] 175.3654\n\n\nA very important function in R is the c()function. c is short for concatenate and this is what the function does. It builds a vector putting several numbers together in one R object. In this way we can write down several indices at once and tell R for instance to read the 4th, the 40ies and the 67th element from our heights data like this\n\nheights[c(4,40,67)]\n\n[1] 173.7980 176.6037 177.2146\n\n\n\n\n3.3.2 Selecting more than one observation\nSometimes we need to retrieve consecutive numbers. For this case we need another function, the colon operator or :. It helps us to build sequences of conscutive numbers.\nFor example\n\nheights[c(2,3,4)]\n\n[1] 174.0172 175.5666 173.7980\n\n\nis the same as\n\nheights[2:4]\n\n[1] 174.0172 175.5666 173.7980\n\n\nSay we wanted to know the average height and the standard deviation of the first 50 observations in our data we could tell R to compute this by\n\nmean(heights[1:50])\n\n[1] 173.0988\n\nsd(heights[1:50])\n\n[1] 4.99029\n\n\nAnother very powerful and useful function we will need often in data analysis is the lenght()function of R. It gives us the number of elements in a vector. Let’s check this with heights()\n\nlength(heights)\n\n[1] 100\n\n\nwhich gives us the answer to be expected.\n\n\n3.3.3 Logical subsetting, coercion rules and recycling\nAs we already learned in the context of computing proportions of values fulfilling a certain condition by using logicals and the coercion rules of R before, we can do similar things by composing different functions.\nAssume we want to know the number of observations with a height larger than 180 cm. This could be computed by typing\n\nsum(heights &gt; 180)\n\n[1] 6\n\n\nYou already know how this works. In the parentheses is a comparison. In this comparison R checks for each entry in the vector heights whether it is larger than 180 or not. For each observation where this is the case the output is TRUE and otherwise FALSE.\nBut how can such a comparison work? After all heights is a vector with 100 entries and 180 is just one number. A vector can not be compared to a number. Here some feature of R comes into place which is called recycling. R makes heightsand 180 similar by extending 180 to a vector with 100 of the same element 180 and then compare component by component resulting in a vector with 100 logicals (i.e. TRUE or FALSE).\nWhen the vector of logicals is given to the sum()function another internal rule of R is applied, which is called coercion. In this case R transforms in the background automatically the TRUE values into 1 and the FALSEvalues into 0. As a result you will get the sum of all cases where the condition is fulfilled.\nA related and also very useful function is the which()function. With this function you can find out the indices of the observations, fulfilling a condition\n\nwhich(heights &gt; 180)\n\n[1]  8 26 46 54 60 73\n\n\ntells you that the elements 8 26 46 54 60 73 were larger than 180. And what were the values of these 6 observations? Now you know already. It is\n\nheights[which(heights &gt; 180)]\n\n[1] 185.3618 180.5622 184.1186 180.5760 182.7100 182.0625\n\n\n\n\n3.3.4 Data Frames\nData Frames are the next R workhorse when we deal with data. A data frame is a rectangular table of data where every row corresponds to one data point.\nInstead of using just some data frame let us this time use a real world example of anthropometric data from the DHS. Anthropometric data can help a country to get a data driven picture on the nutritional status of its population. These data can help identify areas where policy action is needed.\n\n3.3.4.1 DHS data on the nutritional status of children under 5 years\nIn the analysis we are going to produce a statistical table on the nutritional status of children under five years. The anthropometric data reveal information about a persons nutritional status. The nutritional status of children is observed to monitor and measure malnutrition. The nutritional data of children are included in the United Nations development goals indicators. They encompass children’s sex, age, height-length, and weight. The nutritional indicators are sex and age specific because boys and girls grow at different rates. Their growth rates are also age dependent.\nFour indicators on nutritional status are collected in DHS surveys:\n\nOverweight: This indicates high weight for height and is a measure of excess weight. This results from an imbalance of energy consumed (too much) and energy expended (too little). Children with this condition have an increased risk of non-communicable diseases, such as high blood pressure and diabetes and is associated with increased risk of being obese and overweight in adulthood.\nStunting: This is low height-for-age. It is a measure of growth faltering and may result from a deficient growth environment, recurrent infections, chronic diseases and other causes. It is associated with impaired brain development and reduced academic achievement in childhood and lower economic potential as an adult.\nWasting: Is low weight-for-height and is a measure of acute weight loss. This may result from inadequate food intake or from illness or infection. With this condition children are more susceptible for disease and have a higher risk of death.\nUnderweight: Is low weight-for-age and is a measure of weight relative to a child’s age. It reflects children who are stunted or wasted or both.\n\nThe World Health Organisation (WHO) child growth standards provide a single international standard that describes the physiological growth for all children from birth to age 5. The measurements of individual children are compared to the WHO Child Growth Standards to assess each child’s growth. The WHO Growth Standards for children use a distribution that is expressed in units of standard deviations from the mean, also called z-scores. It is a distribution with a mean 0 and standard deviation 1. Within 1 standard deviation are about 67 % of all values, within 2 standard deviations it is 95 % and within 3 standard deviations 99.7%. Height for age, weight for height, and weight for age of children are expressed in these units.\nStatistical cutoffs are used to measure malnutrition. If a child’s height for age is below minus two standard deviations from the mean (expressed in z-scores), the child is considered stunted. If it is below minus three standard deviations from the mean it is considered severely stunted. If the weight-for-height z-score is below minus 2 standard deviations from the mean the child is considered wasted, if it is below three standard deviations from the mean it is considered severely wasted. If the weight for height z-score is above two standard deviations from the mean the child is considered overweight, if three standard deviations above it is considered obese. If the weight for height z-score of a child is below two standard deviations from the mean it is considered underweight, if three, severely underweight.\nThe WHO uses the following prevalence thresholds to assess the problems of stunting and wasting in a population of children:\n\nWHO Prevalence thresholds for height and weight\n\n\nClassification\nstunting\nwasting and overweight\n\n\n\n\nvery high\n30% or more\n15 % or more\n\n\nhigh\n20 - 29 %\n10 - 14 %\n\n\nmedium\n10 - 19 %\n5 - 9 %\n\n\nlow\n2.5 - 9 %\n2.5 - 4 %\n\n\nvery low\nLess than 2.5 %\nLess than 2.5 %\n\n\n\nPrevalence is the proportion of a population who have a specific characteristic in a given time period. In a survey prevalence is measured by the number of people in a sample who have the characteristic divided by all people in the sample.\nLet us go now into the actual data and learn how to use the computer and R as a tool.\n\n\n3.3.4.2 Reading the DHS data using the JWL-package\nThe data we use in these notes and in our course are usually real world data collected from a public source on the internet. Since we begin from scratch with explaining the concepts of statistics as well as of R, we develop your R knowledge step by step and explain only at the end of the course how to read data into R from various outside sources, from websites and in different file formats. While this is not difficult, it is tedious and perhaps unnecessarily confusing for a beginner. We have therefore packed all datasets used in this course into a so called R package. R-packages are libraries, which add new functionalities to R, usually new functions, which you yourself or somebody else has written. Packages must be installed once and then loaded to be usable for your current R section. The package with the name JWL contains all datasets used in the course and it has been preinstalled by your learning facilitators. You only have to load the package, when you want to use data from it in your current R session.\nThe R function, loading a package is called library() and it takes the package name as an argument. We load the DHS data now from the JWL package. We first load the package to make the data available to R. For this you type at the console of at the input cell of your Jupyter Notebook\n\nlibrary(JWL)\n\nNow the data are available to you. But how do I know the names of the datasets? R contains a function data() where you can look up datasets that are bundeled in a package. For instance if you type\n\ndata(package = \"JWL\")$results[ , \"Item\"]\n\n [1] \"child_mortality_average_schooling_of_women\"\n [2] \"children_nutrition_data\"                   \n [3] \"col_sal\"                                   \n [4] \"emissions\"                                 \n [5] \"energy_consumption_per_capita\"             \n [6] \"enrol_attend_dat\"                          \n [7] \"expenditure_outcome_dat\"                   \n [8] \"height_men\"                                \n [9] \"height_weight\"                             \n[10] \"height_women\"                              \n[11] \"infant_mortality_causes\"                   \n[12] \"infant_mortality_data\"                     \n[13] \"life_expectancy\"                           \n[14] \"ncd_height\"                                \n[15] \"ncd_weight\"                                \n[16] \"pearson\"                                   \n[17] \"povdat_by_country\"                         \n[18] \"poverty_vs_gdp\"                            \n[19] \"socr_height_weight\"                        \n\n\nR shows you all the datasets that are in the JWL package. Don’t worry for the moment about the somewhat mysterious syntax of this R statement. Given your current knowledge of R you cannot fully understand it at this moment. So just accept it as a command whith which you can look up datasets in a package.\nOur dataset from the DHS is called cildren_nutrition_data. Let’s store them in an object with a slightly simpler name, say dhs_data. How to store data in an R object, we have already learned in the previous lecture. You invent a name and assign the values to this name using the assignment operator &lt;-.\nLet’s call the object in which we save our data dhs_data\n\ndhs_data &lt;- children_nutrition_data\n\n\n\n3.3.4.3 Inspecting the data\nA useful function to inspect a data object like the one we have just created is the dim() function. It will tell us how many rows and columns our dataframe actually has\n\ndim(dhs_data)\n\n[1] 2559   17\n\n\nThis means that our data have 2559 rows and 17 columns. Quite a bit larger than the heights dataset. Here we really need a computer to analyze the infortion contained in these data.\nAnother useful function to inspect data is the heads()function. It displays by default the first 6 rows of the dataframe\n\nhead(dhs_data)\n\n          wt age_in_months    sex type_of_residence   region wealth_index\n16  1.049502       24 - 35 female             rural region 2       middle\n27  1.049502       24 - 35 female             rural region 2       poorer\n36  1.049502       36 - 48 female             rural region 2      poorest\n94  1.049502        9 - 11   male             rural region 2       poorer\n106 1.049502       36 - 48 female             rural region 2       middle\n113 1.049502       24 - 35   male             rural region 2       poorer\n    nt_ch_sev_stunt nt_ch_stunt nt_ch_haz nt_ch_sev_wast nt_ch_wast\n16            FALSE       FALSE     -0.73          FALSE      FALSE\n27            FALSE        TRUE     -2.23          FALSE      FALSE\n36            FALSE        TRUE     -2.10          FALSE      FALSE\n94            FALSE       FALSE     -0.64          FALSE      FALSE\n106           FALSE       FALSE     -0.38          FALSE      FALSE\n113           FALSE        TRUE     -2.08          FALSE      FALSE\n    nt_ch_ovwt_ht nt_ch_whz nt_ch_sev_underwt nt_ch_underwt nt_ch_ovwt_age\n16          FALSE      0.29              TRUE         FALSE          FALSE\n27          FALSE     -0.42              TRUE         FALSE          FALSE\n36          FALSE      0.37              TRUE         FALSE          FALSE\n94          FALSE     -0.77              TRUE         FALSE          FALSE\n106         FALSE     -0.54              TRUE         FALSE          FALSE\n113         FALSE     -1.02              TRUE         FALSE          FALSE\n    nt_ch_waz\n16      -0.13\n27      -1.59\n36      -1.01\n94      -0.92\n106     -0.56\n113     -1.84\n\n\nSince there are 17 variables not all of them can be displayed on a single page-width. R stacks the information in blocks. We see that some columns are numerical, some are characters and some are logicals. The variable names give already some clues about what they mean. For the exact meaning we would need some form of documentation.\n\n\n3.3.4.4 Analyzing particular variables and subsets of data\nDollar signs $ are used to adress individual columns. For example, assume we would like to count the number of boys (i.e. sex = male) in our sample we could use this together with R’s coercion rules by coding\n\nsum(dhs_data$sex == \"male\")\n\n[1] 1256\n\nsum(dhs_data$sex == \"female\")\n\n[1] 1303\n\n\nObserve a few things here: Operationally we have applied the tricks we have learned about using logicals to count up cases for which a condition is true. The variable sex in our data is coded by strings of chracters. We therefore need to put these strings into \" \" quotation marks in the comparison. Note also that the identidy sign in R is ==. The symbol = in R is an assignment operator, which we can only use to assign values to arguments in a function.\nIndices in a dataframe are pairs specifying row and column numbers. For example\n\ndhs_data[4,3]\n\n[1] male\nLevels: male female\n\n\nis the data point in the dataframe which is at row 4 and column 3. In this case it is a string of characters.\n\n\n\n\n\n\nRemember\n\n\n\nFor any subset of a data frame d, we can extract whatever rows and columns we want using the format\nd[the rows we want, the columns we want]\n\n\nSometimes dataframes do not have column names. Still we can apply functions to the vector of a column using indices. For example we could compute the mean of the variable nt_ch_wazas\n\nmean(dhs_data[ ,17])\n\n[1] -0.7392497\n\n\nNote the expression [,17]. Since there is a 17 in the second position, we are talking about column 17. And since the first position, before the comma, is empty, no rows are specified — so all rows are included. That boils down to: all of column 18.\nA powerful feature of R is that we can extract data from a dataframe using the same logic as we used when extracting data from a vector.\nFor example the follwoing code extracts variables 1, 3 and 5 and prints the first four values of them\n\ndhs_data[1:4, c(1,3,5)]\n\n         wt    sex   region\n16 1.049502 female region 2\n27 1.049502 female region 2\n36 1.049502 female region 2\n94 1.049502   male region 2\n\n\nNote that in a dataframe all columns must have the same length. But as in our dataset, every variable can contain a different data type, like numbers, characters or logicals.\nNow let us analyze the prevalence of children in our dataset that are wasted. Before we do that let us explain that the data contain a variable, which is called wt. This is short for weight. It means that each row is not counted as one observation but is weighted. The sum of the weights\n\nsum(dhs_data$wt)\n\n[1] 2494.102\n\n\nwould then give us the total number of children in the sample. Let us not go into the details of where these weights are coming from and why they are needed at this stage. The variable that encodes stunting is nt_ch_stunt and the variable that encodes wasting is nt_ch_wast.\nNow let us do the computation using the rules we have learned so far:\n\nsum(dhs_data$nt_ch_stunt*dhs_data$wt)/sum(dhs_data$wt)\n\n[1] 0.3589812\n\nsum(dhs_data$nt_ch_wast*dhs_data$wt)/sum(dhs_data$wt)\n\n[1] 0.1002921\n\n\nThe proportion of stunted children in our sample is 39.5 % and the proportion of wasted children is 10 %. Thus according to the WHO classification the prevalence in stunting is very high and the prevalence in wasting is high.\nWe could use our new knowldge to answer other questions. Assume we would want to find out which of our regions has the highest proportion in stunting?\nFirst, let us find which rows refer to region 1, region 2 etc.\n\nr1 &lt;- which(dhs_data$region == \"region 1\")\nr2 &lt;- which(dhs_data$region == \"region 2\")\nr3 &lt;- which(dhs_data$region == \"region 3\")\nr4 &lt;- which(dhs_data$region == \"region 4\")\n\nNow with these row indices we can compute the proportions on the relevant subset of the data.\n\ndata_r1 &lt;- dhs_data[r1, ]\ndata_r2 &lt;- dhs_data[r2, ]\ndata_r3 &lt;- dhs_data[r3, ]\ndata_r4 &lt;- dhs_data[r4, ]\n\nsum(data_r1$nt_ch_stunt*data_r1$wt)/sum(data_r1$wt)\n\n[1] 0.3367742\n\nsum(data_r2$nt_ch_stunt*data_r2$wt)/sum(data_r2$wt)\n\n[1] 0.4078512\n\nsum(data_r3$nt_ch_stunt*data_r3$wt)/sum(data_r3$wt)\n\n[1] 0.2998199\n\nsum(data_r4$nt_ch_stunt*data_r4$wt)/sum(data_r4$wt)\n\n[1] 0.393195\n\n\nSo we see that region 2 has the highest prevalence in stunting.\nOften we want to extract data that fulfill more than just one condition. For instance when we want to filter the values of a variable that fall in a particular interval we need the values that are larger or equal to the lower bound of this interval and lower or equal to the upper bound of the interval.\n\n\n3.3.4.5 Combining several conditions\nR allows us to combine conditions by logical operators. Assume we want to know the proportion of children in our data that are wasted and stunted. In this case both conditions need to be fulfilled. This would be coded in the following way:\n\nsum((dhs_data$nt_ch_stunt & dhs_data$nt_ch_wast)*dhs_data$wt)/sum(dhs_data$wt)\n\n[1] 0.03264431\n\n\nLet me explain. The condition that a child is both stunted and wasted means that the stunting variable evaluates to TRUE and the wasting variable evaluates to TRUE. If this is the case (dhs_data$nt_ch_stunt & dhs_data$nt_ch_wast) is TRUE. If one of these variables evaluates to FALSE then (dhs_data$nt_ch_stunt & dhs_data$nt_ch_wast) is FALSE.\nWe can do this with other logical operators. We can ask for example what is the proportion of children who are wasted or stunted. The operator here is | and (dhs_data$nt_ch_stunt | dhs_data$nt_ch_wast) evaluates to TRUE if the first condition is TRUE or if the second condition is TRUE or if both are TRUE. Let’s do the computation:\n\nsum((dhs_data$nt_ch_stunt | dhs_data$nt_ch_wast)*dhs_data$wt)/sum(dhs_data$wt)\n\n[1] 0.4266291\n\n\na proportion that is very high."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#summary",
    "href": "summarizing_and_communicating_lots_of_data.html#summary",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.4 Summary",
    "text": "3.4 Summary\nIn this chapter you have learned how to summarize and communicate lost of data. As an application context we have chosen measures of height and weight, or anthropometric data.\n\nYou have learned how to summarize data graphically by a histogram, how to construct a histogram and how to display the histogram with different scales: With absolute and relative frequencies.\nYou have learned to plot histograms in R and learned about things to care about when doing histograms, such as the choice of the bin size and choosing the boundaries of bins.\nYou have learned to summarize data by the mean and the standard deviation. You have learned what these numbers mean and how to compute them by hand and, when you have lots of data, by R.\nYou have learned new concepts in R that helped us to analyze a large dataset from the demographic and health survey by computing proportions of children suffering from certain physical conditions. In particular the new R concepts you learned were\n\nReading data from a file\nUsing indices to extract particular values from a vector.\nSelecting more than one observation.\nLogical subsetting, coercion and recycling rules of R\nData frames and how to work with them."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#exercises",
    "href": "summarizing_and_communicating_lots_of_data.html#exercises",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\n3.5.1 Exercises:\n\n\n\n\n\n\nExercise 1: Histogram of monthly wages\n\n\n\nA histogram of monthly wages for part-time employees is shown below (relative frequencies are marked in parenthesis). Nobody earned more than $1000 a month. The block over the class interval from 200 to 300 is missing. How tall must it be?\n\n\n\nWages\n\n\n\n\n\n\n\n\n\n\nExercise 2: Which histogram is right?\n\n\n\nThree people plot histograms for the weights of subjects in a study, using the relative frequency scale. Only one is right. Which one and why?\n\n\nCode\n#| echo = false\n\nlibrary(JWL)\n\ndat &lt;- height_weight\n\ndata &lt;- dat[dat$state == 1 & dat$sex == 1 & dat$age &gt; 18, ]\n\nhist_inf &lt;- hist(data$height, plot = FALSE)         # Store output of hist function\nhist_inf$density &lt;- hist_inf$counts /    # Compute density values\n  sum(hist_inf$counts) * 100\n\npng(file=\"pictures/hight_version1.png\")\nplot(hist_inf, freq = FALSE, xlab = \" \", ylab = \" \", main = \" \")\n\n\npng(file=\"pictures/hight_version2.png\")\nplot(hist_inf, freq = FALSE, xlab = \"hight (cm) \", ylab = \"Percent per 5 cm \", main = \" \")\n\n\npng(file=\"pictures/hight_version3.png\")\nplot(hist_inf, freq = FALSE, xlab = \"hight (cm) \", ylab = \"5 cm per percent\", main = \" \")\n\n\n\n\n\n\n\n\n\n(a) Version 1\n\n\n\n\n\n\n\n(b) Version 2\n\n\n\n\n\n\n\n(c) Version 3\n\n\n\n\nFigure 3.3: Three versions of a hight histogram of males over age 18\n\n\n\n\n\n\n\n\n\n\nExercise 3: Different scales in a histogram\n\n\n\nAn investigator draws a histogram for some height data, using the metric system. She is working in centimeters (cm). She wants to draw the histogram in a so called density scale, i.e. in a scale that the area of all the bars sum to 1. The vertical axes shows relative frequency and the top of the vertical axes is 10 percent per cm. Now she wants to convert to millimeter (mm). There are 10 millimeter to the centimeter. On the horizontal axis, she has to change 175 cm to ? mm, 200 cam to ? mm. On the vertical axis she has to change 10 percent per cm to ? percent per mm, and 5 percent per cm to ? percent per mm.\n\n\n\n\n\n\n\n\nExercise 4: Find the average\n\n\n\n\nThe number 3 and 5 are marked by crosses on the horizontal line below. Find the average of these two numbers and mark it by an arrow.\n\nRepeat (a) for the list 3,5,5\n\nTwo numbers are shown below by crosses on a horizontal axis. Draw an arrow pointing to their average.\n\n\n\n\n\n\n\n\n\n\nExercise 5: Questions about averages\n\n\n\nA list has 10 entries. Each entry is either 1, 2 or 3. What must the list be of the average is 1? if the average is 3? Can the average be 4?\n\n\n\n\n\n\n\n\nExercise 6: Comparing averages without computation\n\n\n\nWhich of the following lists has a bigger average? Or are they the same? try to answer without doing arithmetic.\n\n10, 7, 8, 3, 5, 9 (ii) 10, 7, 8, 3, 5, 9, 11\n\n\n\n\n\n\n\n\n\nExercise 7: Adding data and the change of averages\n\n\n\n\nTen people in a room have an average height of 1.69 m. An 11th person is 1.96 enters the room. Find the average height of all 11 people.\nTwenty-one people in a room have an average height of 1.68. A 22nd person who is 1.96 enters the room. Find the average height of all 22 people.\nTwenty-one people in a room have a height of 1.68. A 22nd person enters the room. How tall would he have to be to raise the average height by 1 inch.\n\n\n\n\n\n\n\n\n\nExercise 8: True or False ?\n\n\n\nDiastolic blood pressure is considered a better indicator of heart trouble than systolic pressure. The figure below shows age-specific average diastolic blood pressure for men age 20 and over in a health survey from the US (HANES5 (2003-04)). True or false: The data show that as men age, their diastolic blood pressure increases until age 45 or so. and then decreases. If false, how do you explain the pattern in the graph? (Blood pressure is measured in “mm” that is millimeter of mercury)\n\n\n\n\n\n\n\n\n\nExercise 9: Can you explain this?\n\n\n\nAverage hourly earnings in the US are computed each month by the Bureau fo Labor statistics using payroll data from commercial establishments. The Bureau figures the total wages paid out to non-supervisory personnel, and divides by the total hours worked. During recessions, average hourly earnings typically go up. When the recession ends, average hourly earnings often start going down. How can this be?\n\n\n\n\n\n\n\n\nExercise 10: Working with standard deviations\n\n\n\nThe socr_height_weight about the height and weight of 18 year old humans, we had used in the lecture before the average height in inches was about 173 cm and the standard deviation was about 5 cm.\n\nOne individual was 188 cm. He was above average by how many standard deviations?\nAnother individual was 174.66 cm. She was above average by how many standard deviations?\nA third individual was 1.5 standard deviations below the average height. He was how many cm?\nIf an individual was within 2.25 standard deviations of average height, the shortest height for this individual would be how many cm? The highest height would be how many cm?\n\n\n\n\n\n\n\n\n\nExercise 11: Match the height\n\n\n\n\nHere are the heights of 4 individuals. 150 cm, 130 cm, 180 cm, 172 cm. Match the heights with the description. A description may be used twice.\n\nunusually short, about average, unusually tall\n\nAbout what percentage of individuals in the data had heights between 170.2 and 173.2 ? Between 165.5 and 179.2?\n\n\n\n\n\n\n\n\n\nExercise 12: Recognizing the spread of lists\n\n\n\nEach of the following lists has an average of 50. For which one is the spread of the numbers around the average biggest? Smallest?\n\n0, 20, 40, 50, 60, 80, 100\n0, 48, 49, 50, 51, 52, 100\n0,1,2,50,98, 99, 100\n\n\n\n\n\n\n\n\n\nExercise 13: Guess the standard deviation\n\n\n\nEach of the following lists has an average of 50. For each one, guess whether the standard deviation is around 1, 2, or 10. (This does not require any arithmetic.)\n\n49, 51, 49, 51, 49, 51, 49, 51, 49, 51\n48, 52, 48, 52, 48, 52, 48, 52, 48, 52\n48, 51, 49, 52, 47, 52, 46, 51, 53, 51\n54, 49, 46, 49, 51, 53, 50, 50, 49, 49\n60, 36, 31, 50, 48, 50, 54, 56, 62, 53\n\n\n\n\n\n\n\n\n\nExercise 14: Stylized histograms, average and standard deviation\n\n\n\nBelow are three sketches of three stylized histograms (we call them “sketches” because they display a schematic shape of a hypothetical histogram. This is why these sketches do not look like real histograms.) Match the sketch with the description. Some descriptions will be left over. Give your reasoning in each case. The symbol \\(\\approx\\) is the mathematical notation for approximately.\n\nave \\(\\approx\\) 3.5, sd \\(\\approx\\) 1 (iv) ave \\(\\approx\\) 2.5, sd \\(\\approx\\) 1\nave \\(\\approx\\) 3.5, sd \\(\\approx\\) 0.5 (v) ave \\(\\approx\\) 2.5, sd \\(\\approx\\) 0.5\nave \\(\\approx\\) 3.4, sd \\(\\approx\\) 2 (vi) ave \\(\\approx\\) 4.5, sd \\(\\approx\\) 0.5\n\n\n\n\n\n\n\n\n(a) Histogram 1\n\n\n\n\n\n\n\n(b) Histogram 2\n\n\n\n\n\n\n\n(c) Histogram 3\n\n\n\n\nFigure 3.4: Three stylized histograms\n\n\n\n\n\n\n\n\n\n\nExercise 15: Estimating the size of average and standard deviation\n\n\n\nOne investigator takes a sample of 100 men age 18 - 24 in a certain town. Another one takes a sample of 1000 such men.\n\nWhich investigator will get a bigger average for the heights of the men in his sample? Or should the average be about the same?\nWhich investigator will get a bigger standard deviation for the heights of the men in his sample? or should the standard deviation be about the same for both investigators?\nWhich investigator is likely to get the tallest of the sample men? Or are the chances about the same for both investigators?\nWhich investigator is likely to get the shortest of the sample men? Or are the chances about the same for both investigators?\n\n\n\n\n\n3.5.2 Exercises R\n\n\n\n\n\n\nExercise 15: A histogram of the Nile river flow data\n\n\n\nBase R comes bundled with certain datasets which are available to R when you start it. One of these data are flow measurements of the river Nile at Aswan in Egypt. This dataset is called Nile.\n\nUse the R help function to learn more about these data. Describe in words the meaning of the datapoints.\nPlot a histogram of the Nile river flow data.\nChange the Main title of the histogram to “Histogram of Nile river flow data at Aswan Egypt” and change the x axes label to “Annual flow is recorded are 100 millions of cubic meters”. Hint: If you don’t know how to go about this either use ?hist - the R help function - to find out. Alternatively you might try a search engine like google of bing and ask something like “changing title and labels in base R histogram”.\n\n\n\n\n\n\n\n\n\nExercise 16: Analyze the full height dataset\n\n\n\nLoad the JWL package of datasets and store the variable socr_height_weight in a variable called dat. Use R’s help function to learn what the data are.\n\nThe data show heights in inches and weights in pounds. Transform both variables to metric data before you continue. Note that 1 inch is 2.54 cm and 1 pound is 0.4535924 kg.\nPlot a histogram of the heights in cm. Plot a histogram of the weight in cm.\nCompute the mean and the standard deviation of both height and weight.\nCompute the percentage of observations between plus 1 and minus 1 standard deviations from the mean.\nCompute the percentage of observations between plus and minus 2 standard deviations from the mean. Do the same with three standard deviations.\n\n\n\n\n\n\n\n\n\nExercise 17: Energy consumption per capita accross countries in the world\n\n\n\n\nLoad the dataset energy_consumption_per_capita from the JWL package and use the R help function to learn what this dataset describes.\nSelect the energy consumption in the year 2018 and plot a histogram of these data. What do you see?\nHow many countries consume more than 50.000 kilowatt-hours-per person per year?\nWhich countries fulfill this condition?\n\n\n\n\n\n3.5.3 Exercise Project:\nTo be added."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#computer-exercises",
    "href": "summarizing_and_communicating_lots_of_data.html#computer-exercises",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.6 Computer exercises",
    "text": "3.6 Computer exercises\nOpen the notebook exercises_summarizing_and_communicating_lots_of_data.ipynb and work on the exercises. Write your solutions into the notebook. You will need the computer to do this work."
  },
  {
    "objectID": "from_limited_data_to_populations.html#learning-from-data-and-the-process-of-inductive-inference.",
    "href": "from_limited_data_to_populations.html#learning-from-data-and-the-process-of-inductive-inference.",
    "title": "4  From limited data to populations",
    "section": "4.1 Learning from data and the process of inductive inference.",
    "text": "4.1 Learning from data and the process of inductive inference.\nIn the preceding chapters we have looked at examples where we had given data. Analysis of this data was sufficient if we just appropriately summarized them or displayed the data in an insightful way.\nSometimes this is really all there is to do. For instance when we studied the time trend in infant mortality we just looked at the time series of mortality data and a plot showed us the complete answer. The data we previously plotted for Ghana and Kenya - for example - show a steady decrease since 1965 starting from a share of roughly 12 % down below 2 % in 2020. These are data from public registers and not samples. The data are based on a complete set of observations in a country.\n\n\nCode\nlibrary(ggplot2)\nlibrary(JWL)\n\npl_dat &lt;- with(infant_mortality_data, infant_mortality_data[Country %in% c(\"Ghana\",\"Kenya\") & \n                                                            Year &gt;= 1965, ])\n\np &lt;- ggplot(pl_dat, aes(x = Year, y = Mortality, color = Country)) +\n     geom_point() +\n     geom_line() +\n     xlab(\"\") +\n     scale_y_continuous(labels = scales::percent)\n\np\n\n\n\n\n\nBut sometimes we want to say more about the data. We would like - for instance - to make predictions on how, for example, the trend in this share is going to look like in the future where we do not yet have data.\nOr maybe we would like to say something more basic. For example why did the share show a long term downward trend in those countries.\nSuch generalisations, where we try to learn something about the world outside of our observations, based on these observations are called in statistics inductive inference. Inductive inference is a challenging idea and it had been the topic of many philosophical and methodological controversies among scholars in the past.\n\n\n\n\n\n\nDeductive and inductive reasoning\n\n\n\nDeductive reasoning derives particular conclusions from general premises using the rules of logic. In this way if the assumptions hold and the reasoning is done correctly, i.e. the rules of logic are properly applied the conclusions is certain and irrefutable. A toy example of deductive reasoning would for instance be:\n\nMajor Premise: All plants perform photosynthesis.\nMinor Premise: A cactus is a plant.\nConclusion: A cactus performs photosynthesis.\n\nModern mathematical reasoning is all built on deduction, allowing to come up with actual proofs of certain statements given a set of axioms or assumptions. A proof is possible because deduction is logically certain.\nInductive reasoning works differently. It starts from particular instances and tries to work out generalizations from there. It goes from data to hypothesis. A simple toy example for inductive reasoning would be:\n\nData: I see fireflies in my garden every summer.\nHypothesis: This summer I will probably see fireflies in my garden.\n\nInduction does not allow to proof a hypothesis because it is generally uncertain.\n\n\nLet us go back to the entire process of going from the raw data to the statements about the entire country or region in the DHS survey.\nWhen we go from the sample to the true data about the units in the sample, the step from stage 1 to stage 2, we have to think careful about issues or problems of measurement. We want to know whether our data are reliable. They should have a low variability from occasion to occasion and should measure the same thing when we repeat a measurement. They should also measure what we intend or want to measure. This is called validity. We do not want our data to have a systematic bias.\nWhen we take again the DHS as an example, where people are asked many questions and quite a few measurements are taken on individuals, questions should be such that people give the same or a similar answer each time they are asked this question. To some extend this can be tested but these tests are not perfect. We also need to assume that people answer honestly to questions. With measurements things are similar. We want that we get the same or a very similar result if we repeat a measurement and we need to assume that the survey field workers who take the measurements do their job diligently.\nIf questions would be biased towards a particular answer a survey would also not be valid. This needs special care in developing questions. In chapter 2 we have learned about framing effects. Framing effects are often used in marketing. For example in meat packaging it is well known that how you report certain facts about the packaged meat influences sales. If the same piece of meat is one time packaged with the text “75 % lean meat” and the other time with the text “25 % fat”, which is logically the same information, several studies showed that on average the first package is preferred. This and other details have to be considered when elicting information from the sample.\nNow going from the second stage - from the sample - to the third stage - the study population requires particular care. We have to be confident that the sample observed accurately reflects characteristics of the larger group we are ultimately interested in and from which the sample has been taken. Technically this is also often called internal validity.\nHere we come to a crucial idea how bias can be avoided, the idea of random sampling. Let us explain exactly what a random sample is. Using R will support us in developing this understanding. As an example data set of our population we use as an illustration in the following discussion, let us take the dataset on the height and weight of adult humans, we used in chapter 3."
  },
  {
    "objectID": "from_limited_data_to_populations.html#why-does-random-sampling-help-avoiding-bias",
    "href": "from_limited_data_to_populations.html#why-does-random-sampling-help-avoiding-bias",
    "title": "4  From limited data to populations",
    "section": "4.2 Why does random sampling help avoiding bias?",
    "text": "4.2 Why does random sampling help avoiding bias?\n\n\nCode\nlibrary(JWL)\ndat &lt;- socr_height_weight\ndat$Height &lt;- dat$Height*2.54\ndat$Weight &lt;- dat$Weight*0.4535924\n\n\nRemember the human height data from exercise 16 in the last section on summarizing and communicating lots of data. This was a data set with 25.000 observations and three variables, an index for each individual a number of height in cm and weight in kg.2. Let’s look at the first 10 rows in this dataset.2 Note that the original data set socr_height_weight measures height in inches and weight in lbs. To get cm from inches, we have to multiply by 2.54 and to get kg from lbs we have to multiply the numbers by 0.4535924. In the data we use here we use units of cm and kg\n\n\nCode\nshow &lt;- head(dat, n= 10)\nrownames(show) &lt;- NULL\nknitr::kable(head(show, n= 10))\n\n\n\n\n\nIndex\nHeight\nWeight\n\n\n\n\n1\n167.0896\n51.25254\n\n\n2\n181.6486\n61.90960\n\n\n3\n176.2728\n69.41184\n\n\n4\n173.2702\n64.56226\n\n\n5\n172.1810\n65.45207\n\n\n6\n174.4925\n55.92903\n\n\n7\n177.2972\n64.18092\n\n\n8\n177.8374\n61.89826\n\n\n9\n172.4727\n50.97122\n\n\n10\n169.6272\n54.73372\n\n\n\n\n\nEach row of this table represents an individual. Each individual here is an adult person age 18 for which a measure of height and weight has been taken. So if we sample from the rows of this table, we get values for the index, the height and the weight of individuals.\nWe could now take samples of rows from the table by selecting every 50th row. Here is a way of how we could implement this in R. If you go back to our last section and remember what we have learned about subsetting, you might get an idea how to approach such a problem.\nLet us first create a sequence of numbers starting from 1 and going until the last row in steps of length 5. This can be done without problems because 25000 rows are divisible by 50 and this procedure will select exactly 500 rows. Here is the R code:\n\nidx &lt;- seq(from = 1, to = nrow(dat), by = 5)\nsample_1 &lt;- dat[idx, ]\nrownames(sample_1) &lt;- NULL\nknitr::kable(head(sample_1, n = 5))\n\n\n\n\nIndex\nHeight\nWeight\n\n\n\n\n1\n167.0896\n51.25254\n\n\n6\n174.4925\n55.92903\n\n\n11\n168.8787\n57.81108\n\n\n16\n180.5727\n63.50180\n\n\n21\n172.2978\n64.08385\n\n\n\n\n\nLet me explain. seq() is an R function which creates a regular sequence of numbers. The arguments specify where the sequence begins and where it ends as well as the step length it takes. We write this sequence into a variable idx. Since we do not want row names we delete these. Deleting is achieved by assigning NULL to the row names. NULL represents the null object in R. It is a reserved word like TRUEor FALSEand can not be used as name for an R object. If you assign the null obejct to an R object this means that this object is deleted. Then we apply the subsetting rules we have learned in the last section to select all rows with index idx with all its variables.\nWhile this is a sample from our population, this is not a random sample because there is no chance involved in selecting the rows. Random sampling is a sampling method that uses a random mechanism. This means that the probability of each unit in the population to become part of the sample is known.\n\n4.2.1 A virtual game of chance: Rolling a die\nWe have not yet discussed the concept of probability. We will learn about probability later in the course. But we all have an intuitive notion of probability from simple games of chance, such as from rolling a die. If we throw a die the probability of each of the points shown on the die’s faces - 1, 2, 3, 4, 5 or 6 - is \\(\\frac{1}{6}\\). This is the chance or probability we would attribute before throwing the die that it shows a particular number. When we can give a probability for each outcome of throwing a die, we have a probability distribution, which is in this particular example very simple, because the probability we have attributed to the different outcomes is always \\(\\frac{1}{6}\\). The probability of each number showing up is the same, \\(\\frac{1}{6}\\). We can show this in the form of a bar-plot, where each bar stands for an outcome and its height showing the probability of this outcome.\n\n\nCode\ndie_out &lt;- c(rep(1,10), rep(2,10), rep(3,10),rep(4,10), rep(5,10), rep(6,10))\n\n# create a histogram with 6 columns\nhist(die_out, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nIn contrast to this theoretical probability distribution an empirical distribution is the distribution of observed data. We encountered many such distributions in the previous sections and and we have learned how to summarize them graphically by histograms.\nThe computer allows us to create empirical distributions of results from throwing a virtual die. It is a simulation of a real situation where you would actually roll a six sided die physically.\nThis is an example which allows us to introduce some new functions and concepts in R, which we will need in the rest of the course.\nFirst we create a virtual die by defining an appropriate R object. This die should represent a physical die, which you know from games of chance.\n\n\n\nA die\n\n\nThe essential feature of the die is that it has six faces each showing different points starting from 1 to 6. In R we implement this by creating a vector of integers 1 to 6, like this\n\ndie &lt;- 1:6\n\nHere we have created the vector of integers 1,2,3,4,5,6 and saved these numbers in an object called die.\nNow R has a built in function, called sample() which can pick values at random from an object. We can tell R by using this function for instance that it should randomly pick a number from the six possible numbers of die.\nThis is how it works:\n\nsample(x=die, size = 1)\n\n[1] 5\n\n\nThe R-function sample() takes an object as argument. The second argument, called size, specifies the number of random picks or draws from the object. If we give the value 1 to size it is as if we threw the die once. We have now implemented in the computer an equivalent to physically throwing a die.\nOne feature that makes R so very powerful is that you can not only use built in functions, like mean, hist sample etc. but you can also write your own functions. We could for instance write a function which rolls a die if we call it.\n\n\n4.2.2 A brief digression: Writing R functions yourself\nEach function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object.\nThe syntax is given like this:\nmy_function &lt;- function() {}\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named roll_die\n\nroll_die &lt;- function(){die &lt;- 1:6 \n                         sample(die, size = 1)}\n\nNow we have written the function and saved it as an R-object we can call it like this. Let’s call it three times for example:\n\nroll_die()\n\n[1] 2\n\nroll_die()\n\n[1] 2\n\nroll_die()\n\n[1] 1\n\n\nNote that in our function roll_die() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like roll_die(). If we only call the name roll_die, R will display the code in the function body.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\nLet me finally say a few things about arguments in a function.\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”.\n\nroll_die2 &lt;- function(){sample(ball, size = 1)}\n\nIf we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call roll_die2 if we make ball an argument of the function. Lets do this:\n\nroll_die2 &lt;- function(ball){sample(ball, size = 1)}\n\nNow the function will work as long as we supply ball when we call the function.\n\nroll_die2(ball = 1:6)\n\n[1] 6\n\n\nNote that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument\n\nroll_die2 &lt;- function(ball= 1:6){sample(ball, size = 1)}\n\nNow if we type:\n\nroll_die2()\n\n[1] 5\n\n\neverything works, just as intended.\n\n\n4.2.3 Going back to simulating die rolls\nNow we are interested in an empirical distribution of points if we roll the die many times. A built in R function that would help us to do this. This function is called replicate(). It needs two arguments, how many times it should do something and of course what it should do precisely.\nSo if we roll our virtual die 10 times, we would tell R to do this:\n\nr10 &lt;- replicate(10, roll_die())\n\nwhich gives us the sequence of results from these 10 rolls. Now let’s plot the empirical distribution.\n\n\nCode\nhist(r10, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nThis does look quite different from the theoretical distribution, where each bar had the same length.\nWhen we increase the sample size the empirical distribution starts to look more similar to the theoretical distribution. Lets roll our die 100 times\n\nr100 &lt;- replicate(100, roll_die())\n\n\n\nCode\nhist(r100, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nLooks better. What about 1000 rolls?\n\nr1000 &lt;- replicate(1000, roll_die())\n\n\n\nCode\nhist(r1000, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nAs we increase the number of rolls in the simulation, the area of each bar gets closer to 16.67% (\\(\\frac{1}{6}\\)), which is the area of each bar in the probability histogram.\nLet’s do a last example with 100.000 rolls to show this\n\nr100000 &lt;- replicate(100000, roll_die())\n\n\n\nCode\nhist(r100000, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nNow we are almost there.\n\n\n4.2.4 The law of averages\nWhat we have just observed is a demonstration of a famous result of probability theory. We are going to learn about probabilities later in the course. This result, sometimes referred to as the law of averages.33 Technically the result is called the weak law of large numbers and was discovered by the Mathematician Jacob Bernoulli in 1713\nThe law says that if a chance experiment - such as throwing a fair die - is repeated independently and under identical conditions (this means that every repetition is performed in the same way regardless of the other repetitions.), then as we repeat the experiment long enough, the relative frequency of each event\n- in our case an event would be that for example the outcome of the die throw is 1 - gets closer to the theoretical probability of the event.\nIn the example of the die, we just studied by using the simulation capacities of R, we saw that the proportion that the die will land on a face showing 6 will happen in about 1/6 of all rolls.\nThis law also holds when a random sample is drawn from units of a large population. Let us use the table of heights in cm and weights in kg of 25.000 adults we studied in the last unit. Here we show the fist 10 rows.\n\n\nCode\nlibrary(JWL)\ndata &lt;- socr_height_weight\n# transfrom to metric units from inches to cm and from lbs to kg\ndata$Height &lt;- data$Height*2.54\ndata$Weight &lt;- data$Weight*0.4535924\nhead(data, n=10)\n\n\n   Index   Height   Weight\n2      1 167.0896 51.25254\n3      2 181.6486 61.90960\n4      3 176.2728 69.41184\n5      4 173.2702 64.56226\n6      5 172.1810 65.45207\n7      6 174.4925 55.92903\n8      7 177.2972 64.18092\n9      8 177.8374 61.89826\n10     9 172.4727 50.97122\n11    10 169.6272 54.73372\n\n\nLet us remind ourselves, how the histogram of the height-data looked like.\n\n\nCode\nh &lt;- hist(data$Height, \n     breaks = 30, \n     plot = F)\n\nh$counts &lt;- h$counts/sum(h$counts)*100\n\nplot(h, xlab = \"Height\", ylab = \"Percent\", main = \"Height distribution of adults\", axes = F)\naxis(1, at = c(160, 170, 180, 190), labels = c(\"160 cm\", \"170 cm\", \"180 cm\", \"190 cm\"))\naxis(2, at =c(0,2,4,6,8), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nLet us now think about the 25.000 adults in our data as a population. We draw a random sample from it. When we draw from the population we draw with replacement. This means when a particular individual is selected at one draw, it could in principle be selected also at the next one.\nLet us write an R function for this:\n\nemp_distr_height &lt;- function(n){\n  \n  data[sample(x = 1:nrow(data), size = n, replace = T), ]\n\n  }\n\nThis function draws n random row from our dataframe. Note that n is an argument for the function. The random draws are a dataframe with N row indices, say 10, and then this gives the row index to the dataframe according to the subsetting rules we have learned in the previous section data[rowindex, ]. Since there is no index in the second slot the entire rows will be selected.\nNow in analogy to the dice example we can see, that when we increase the number of draws into our sample we come closer to the distribution of the population. By this mechanism a random sample generates a subpopulation with similar distributional properties when the sample is large enough. How much is large enough? For this we need probability theory which we will encounter later in the course.\nAs with the die, let us start with 10 draws:\n\nhs10 &lt;- emp_distr_height(10)\n\n\n\nCode\nh &lt;- hist(hs10$Height, \n     breaks = 30, \n     plot = F)\n\nh$counts &lt;- h$counts/sum(h$counts)*100\n\nplot(h, xlab = \"Height\", ylab = \"Percent\", main = \"Height distribution of adults\", axes = F)\naxis(1, at = c(160, 170, 180, 190), labels = c(\"160 cm\", \"170 cm\", \"180 cm\", \"190 cm\"))\naxis(2, at =c(0,2,4,6,8), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nThis does not look very similar to the population distribution.\nWhat about a sample with size 100?\n\nhs100 &lt;- emp_distr_height(100)\n\n\n\nCode\nh &lt;- hist(hs100$Height, \n     breaks = 30, \n     plot = F)\n\nh$counts &lt;- h$counts/sum(h$counts)*100\n\nplot(h, xlab = \"Height\", ylab = \"Percent\", main = \"Height distribution of adults\", axes = F)\naxis(1, at = c(160, 170, 180, 190), labels = c(\"160 cm\", \"170 cm\", \"180 cm\", \"190 cm\"))\naxis(2, at =c(0,2,4,6,8), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nNow lets increase to 2500, 10 % of the population.\n\nhs2500 &lt;- emp_distr_height(2500)\n\n\n\nCode\nh &lt;- hist(hs2500$Height, \n     breaks = 30, \n     plot = F)\n\nh$counts &lt;- h$counts/sum(h$counts)*100\n\nplot(h, xlab = \"Height\", ylab = \"Percent\", main = \"Height distribution of adults\", axes = F)\naxis(1, at = c(160, 170, 180, 190), labels = c(\"160 cm\", \"170 cm\", \"180 cm\", \"190 cm\"))\naxis(2, at =c(0,2,4,6,8), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nThis looks already pretty close. You should by now see what random sampling does for the process of inductive inference. For a large enough random sample, the empirical distribution of the sample resembles the histogram of the population with high probability. Thus if you report summaries of the sample, like its mean and its standard deviation, you will be close to the true values in the population. You will not be exactly there. But since the random mechanism is based on a probability model, where the chance of each unit ending up in the sample is known, we can quantify the uncertainty and the error we make.\nHow probability arguments work more precisely, we will learn later in the course. For now this brief discussion should have convinced you of the importance of random sampling for making inferences from sample to population and given you an intuition how this works. As a side effect of this discussion you have also learned how to write R functions.\nNow that we have discussed random sampling, why it is important and how it works, let us finally discuss the last step in the inductive inference chain, going from the study population to the target population. Even if we have a perfect random sample, it might be the case that we were not able to ask the people who we were ultimately interested in. When we can make conclusions on the target population from the study population, we say that we have not only internal but also external validity.\nSpiegelhalter (2019), whom we have already cited before gives an example where this problem is particularly extreme. Often scientist who are ultimately interested in humans could onl study animals, as it is often the case in pharmaceutical research.\nA bit less extreme but related is a second example Spiegelhalter (2019) gives is the case of drugs tested say only on adults but it is then used also by children. These are problems which cannot entiely be solved by statistical methods but need caution and particular care and caution."
  },
  {
    "objectID": "from_limited_data_to_populations.html#when-all-data-are-available",
    "href": "from_limited_data_to_populations.html#when-all-data-are-available",
    "title": "4  From limited data to populations",
    "section": "4.3 When all data are available",
    "text": "4.3 When all data are available\nWhile surveys are a very important instrument to learn from data about the reality we live in, not all data that are available to us today are surveys. Mainly by the internet and technological development, the omnipresence of sensors and digital devices, we have today many data that are not random samples or which are not based on any sampling at all.\nMany data, especially from online commerce are available to platform firms and online shops, for example. The new field of data science got a big push from the attempt to get hold of data created through interactions on the internet and to gain information from these data.\nThe public sector also collects and stores lots of administrative data in a systematic manner. These are for example data needed to run a social security or pension system, police records, health records, demographic data, land registers and business registers, also data on crime, voters and education. These data are used for policy making, research and statistical analysis. In this case you have all the data.\nHaving all the data means that the stage of the induction process where we go from a sample to a study population collapses because the sample and the study population are the same.\nStill problems can occur. Spiegelhalter (2019) mentions the problem that the police systematically misses cases which the police does not record as a crime or which have not been reported by a victim. These problems make the step of drawing conclusions from the study population to the target population difficult. It is not only police records on crimes. You have analogous problems in all other register data as well. Take population register, as one example. Often certain groups such as homeless people or undocumented immigrants who may be hesitant to register or provide personal information are under-counted. Or you can have inaccuracies in property ownership data in land registers due to incomplete or outdated records, fraudulent transactions, or illegal land occupations. Or in health registers you could have missing or incomplete data on certain health conditions, especially those that are stigmatized or poorly understood, or patients who do not seek medical attention due to lack of access or fear of discrimination.\nOf course if we have all the data it is a straightforward process to produce statistics that describe what we have measured. But for broader conclusions we need to be very careful and we need to think about systematic biases and how we can potentially control for them. This always needs our special attention because systematic bias can jeopardize or even invalidate the reliability of our claims."
  },
  {
    "objectID": "from_limited_data_to_populations.html#the-normal-approximation-for-data",
    "href": "from_limited_data_to_populations.html#the-normal-approximation-for-data",
    "title": "4  From limited data to populations",
    "section": "4.4 The normal approximation for data",
    "text": "4.4 The normal approximation for data\nWe will now discuss an old and powerful concept that helps us in our ultimate aim to describe the target distribution. This is called the normal distribution or often also called the “bell curve”, because of its characteristic shape. We already encountered this distribution when we worked with the DHS data.\n\n4.4.1 The normal curve\nThe normal curve was discovered first around 1720 by Abraham de Moivre4 In 1809, the German mathematician Carl Friedrich Gauss5, made significant contributions to the concept of the normal curve, which is until today often also referred as a Gaussian curve. He used the curve to model errors in astronomical observations. Around 1870 the Belgian mathematician Adolphe Quetelet6 had the idea of using the curve as an ideal histogram to which historams of empirical distributions could be compared.4 Abraham de Moivre (1667-1754) was a French mathematician known for his contributions to the development of probability theory and the normal distribution as well as for his work on complex numbers. He is perhaps best known for his discovery of the formula that bears his name, which relates complex numbers to trigonometric functions. De Moivre was also a prominent member of the scientific community in 18th-century Europe and corresponded with many of the leading mathematicians and scientists of his time. 5 Carl Friedrich Gauss (1777-1855) was a German mathematician and physicist who made significant contributions to a wide range of fields, including algebra, number theory, astronomy, and physics. Gauss is considered one of the greatest mathematicians of all time and is known for his contributions to the development of calculus, the discovery of the normal distribution, and his work on the theory of functions. Gauss also made important contributions to the field of astronomy, including the discovery of the asteroid Ceres and the development of a method for calculating the orbits of celestial bodies. 6 Adolphe Quetelet (1796-1874) was a Belgian mathematician, astronomer, and statistician who is known for his work in developing the concept of the “average man” and the use of statistical methods in the social sciences. He is also credited with the development of the body mass index (BMI), which is still widely used today as a measure of obesity. Quetelet was a prominent figure in the scientific community of his time and was a founding member of the International Statistical Institute. \nThe normal curve has a slightly intimidating equation, which looks as follows \\[\\begin{equation}\ny = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left( \\frac{x - \\mu}{\\sigma}\\right)^2}\n\\end{equation}\\]\nBut don’t be afraid. You do not have to be able to manipulate this equation. This is something that R can do for you. We will learn how to ask R to do this.7 Here is a plot of the normal curve done by R:7 The mathematical formula for the normal curve is very interesting, since it contains the most famous numbers in mathematics. We have already encountered \\(\\sqrt{2}\\), the diagonal of the unit square, which is the first so called irrational number detected by the Pythagoreans in antiquity. For them it was quite a shock to discover that there are numbers which can not be represented as a fraction of whole numbers. There is the famous number \\(\\pi\\) also known since antiquity. This number represents the ratio of the circumference of any circle to its diameter and is also an irrational number. Finally there is the number \\(e\\) or sometimes also known as Euler’s number. It is the limit of the sequence \\((1+\\frac{1}{n})^n\\) and plays a fundamental role all over mathematics.\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 1000)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\", lwd = 2, xlab = \"Standard units\", ylab = \"Percent per standard unit\", main = \"The Normal Curve\", col = \"red\")\n\n\n\n\n\nThere are several features of this graph that are important to note:\n\nThe graph is symmetric about 0. The part of the curve to the right is a mirror image of the part of the curve to the left.\nThe total area under the curve equals 1 or 100 %.\nThe curve is always above the horizontal axis. It appears to get equal to zero between 3 and 4 but this is only because the distance from the horizontal axis becomes too tiny to visualize. Only about 0.00006 of the area under the curve is outside of this interval.\n\nIt will be important and helpful to find areas under the normal curve between specified values. For instance:\n\nthe area under the normal curve between +1 and - 1 standard deviations from the mean is about 68 %.\nthe area under the normal curve between -2 and +2 standard deviations is about 95 %.\nthe area under the normal curve between -3 and +3 standard deviations is about 99.7%\n\nDoes this look familiar? If you followed the course until now, you will remember that these are the numbers we encountered with the data on human height. This is because for these data the empirical distribution is such that the respective histogram is very closely approximated by a normal curve.\n\n\n4.4.2 Units and standard units\nOften it is convenient to express the units for the normal curve not in the direct unit in which observations are measured - as for example cm for human height - but in units of standard deviations from the mean.\n\n\n\n\n\n\nConversion to standard units\n\n\n\nA value is converted to standard units by checking how many standard deviations it is above or below the average. Values above the average are given a plus sign, values below the average are given a minus sign.\n\n\nTake an example from our human height data and draw 4 random values from these data, using the emp_distr_height() function we wrote earlier in this chapter say\n\nexample &lt;- emp_distr_height(4)\nexample$Height\n\n[1] 168.4362 183.3148 175.1474 173.5544\n\n\nLet us convert these values to standard units. Let us compute again the mean and the standard deviation of the height data.\n\nm &lt;- mean(data$Height)\nstd &lt;- sd(data$Height)\n\nm\n\n[1] 172.7025\n\nstd\n\n[1] 4.830264\n\n\n168.4362 is -0.8832 from the average. In standard units therefore 168.4362 is -0.8832.\nStandard units are used in the DHS data on nutrition of children and adults which we worked with in the last exercise of chapter 3. Standard units are also often referred to as the z-score. Let us go back to these data and look at them more closely again.\nLet us go back to the histogram of human height we already discussed in the chapter 3.\n\n\nCode\nh &lt;- hist(data$Height, plot = F, breaks = 30)\n\n hist_breaks &lt;- h$breaks\n color_list &lt;- rep('#C6E2FF', length(hist_breaks))\n color_list[hist_breaks &lt;= m + std] &lt;- '#6C7B8B'\n color_list[hist_breaks &lt;= m - std] &lt;- '#C6E2FF'\n\npar(mar = c(5, 4, 4, 4) + 0.3)  \nplot(h, col = color_list, freq = FALSE, xlab = \" \", ylab = \"Percent per cm \", main = \"Height distribution of adults\", axes = F)\n\npar(new = TRUE) \n## Define x and y coordinates for normal curve\n x &lt;- seq(min(data$Height), max(data$Height), length.out = 100)\n y &lt;- dnorm(x, m, std)\n \n# Add normal curve to histogram plot\n lines(x, y, col = \"red\", lwd = 2,  xlab = \" \", ylab = \" \")\n axis(side = 4, at =c(0,0.02,0.04,0.06,0.08), labels = c(\"0%\", \"9.7 %\", \"19.3 %\", \"29.0 %\", \"38.6 %\"), ylab = \"Percent per standard unit\")\n \n\naxis(1, at = c(153.38, 158.21, 163.04, 167.87, 172.70, 177.53, 182.36, 187.19, 192.02), \n     labels = c(\"153.38 cm\", \"158.21 cm\", \"163.04 cm\", \"167.87 cm\", \"172.70 cm\", \"177.53 cm\", \"182.36 cm\", \"187.19 cm\", \"192.02 cm\"))\n\naxis(1, at = c(153.38, 158.21, 163.04, 167.87, 172.70, 177.53, 182.36, 187.19, 192.02), \n     labels = c(\"-4\", \"-3\", \"-2\", \"-1\", \"0\", \"1\", \"2\", \"3\", \"4\"), line = 2.5)\n\n\n\naxis(2, at =c(0,0.02,0.04,0.06,0.08), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nThere are two x-axis in this figure. On the upper x-axis you see the height in cm. On the lower x-axis you see the height in standard units. Standard unit 1 is at 172.70 + 4.83 or at 177.53 and standard unit -1 is at 172.70 - 4.83 or 167.87.\nThere are two vertical axis in this figure, one on the left and one on the right. The histogram is drawn relative to the left y-axis, which is in percent per cm. The normal curve is drawn relative to the right one. To see how the scale on the right y-axis matches up, take the top value on each of the two axes: 38.6 % per standard unit matches 8% per cm because there are 4.8 cm to the standard unit. Spreading 38.6 % over one standard deviation is the same as spreading 38.6 % over 4.8cm and that comes to 8% per cm.\nWe explained before that the area under the normal curve between - 1 and 1 standard deviation from the mean is 68 %. In histogram the area between these values is colored in gray. The historgam follows the normal curve very closely. Sometimes it is a bit above, sometimes a bit below the curve but these deviations balance out. Indeed, if you go back to the previous chapter you see that in the empirical distribution we have indeed 68% of observations exactly between these bounds. So the shaded area in the histogram is to a very good approximation equal to the area under the normal curve and this is where the 68 % come from."
  },
  {
    "objectID": "from_limited_data_to_populations.html#finding-areas-under-the-normal-curve-with-r",
    "href": "from_limited_data_to_populations.html#finding-areas-under-the-normal-curve-with-r",
    "title": "4  From limited data to populations",
    "section": "4.5 Finding areas under the normal curve with R",
    "text": "4.5 Finding areas under the normal curve with R\nBefore the computer became a widely available tool, statisticians used standard tables for finding areas under the curve. Today this is done with a computer. R has built in standard function you can use for this purpose. In this section I show you how.\nExample 1:\nSay we want to find the area under the normal curve between 0 an 1. Now one way to find this area is to use you knowledge that 68 % of the area under the nomal curve are between - 1 and 1. By the symmetry of the normal curve thus the area between 0 and 1 must be half of this area or 34 %.\nWhen you use R to do this calculation the built in function is called pnorm(). This function gives you the area under the normal curve smaller or equal to a given value \\(x\\). The default value for the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\) is \\(\\mu = 0\\) and \\(\\sigma = 1\\).\nSo what we would do is to compute:\n\npnorm(1) - pnorm(0)\n\n[1] 0.3413447\n\n\nwhich gives 0.34 or 34 %.\n\n\n\nExample 1: Area under the normal curve between 0 and 1\n\n\nExample 2:\nFind the area between -2 and 1 under the normal curve with \\(\\mu = 0\\) and \\(\\sigma = 1\\).\nThis problem can be broken down by first computing the area under the normal curve for all values lower than 1 and then the area under the curve for all values lower than - 2 and subtract the latter from the former. In R:\n\npnorm(1) - pnorm(-2)\n\n[1] 0.8185946\n\n\nor 82 %.\n\n\n\nExample 1: Area under the normal curve between -2 and 1\n\n\nExample 3:\nFind the area to the right of 1 under the normal curve. This can be done by finding the area under the normal curve left from 1 first. Since the total area is 1, it must be the case that 1 minus the value of the area left from 1 is the same as the area right of 1.\n\n\nCode\n1 - pnorm(1)\n\n\n[1] 0.1586553\n\n\nor 16 %.\n\n\n\nExample 1: Area under the normal curve right of 1"
  },
  {
    "objectID": "from_limited_data_to_populations.html#the-normal-approximation-for-data-1",
    "href": "from_limited_data_to_populations.html#the-normal-approximation-for-data-1",
    "title": "4  From limited data to populations",
    "section": "4.6 The normal approximation for data",
    "text": "4.6 The normal approximation for data\nIn certain cases the normal curve may be used to approximate data. We go back to our example with the data on the height of adult humans.\nSay we have the information that the height of adult humans were on average 172.7 cm with a standard deviation of 4.83 cm. Assuming the distribution of the data can be approximated by a normal curve. What is the estimated percentage of humans with a height between 168 and 178 cm?\nWe can proceed in two steps. First we convert to standard units:\n\nl &lt;- (168 - 172.7)/4.83\nu &lt;- (178 - 172.7)/4.83\nl\n\n[1] -0.9730849\n\nu\n\n[1] 1.097308\n\n\nThen use the pnorm() function to estimate the area:\n\npnorm(u) - pnorm(l)\n\n[1] 0.6984912\n\n\nwhich is about 70 %.\nLet us check this approximation with the real data. We use logical subsetting to count the share of height values between \\(l\\) and \\(u\\).\n\nidx &lt;- (data$Height &gt;= 168 & data$Height &lt;= 178)\nmean(idx)\n\n[1] 0.69868\n\n\nYou see that in this case the approximation works really well. So using the normal approximation we can get a fairly precise generalization from study population to the target population, even though the target population is larger than our study population. What we have done is that we have replaced the histogram of the empirical data by a normal curve before computing areas.\nNot all histograms follow a normal curve. But it is a remarkable fact, which we will explore in more detail later in the course, that many histograms do. In this case the mean and the standard deviation are good summary statistics."
  },
  {
    "objectID": "from_limited_data_to_populations.html#percentiles",
    "href": "from_limited_data_to_populations.html#percentiles",
    "title": "4  From limited data to populations",
    "section": "4.7 Percentiles",
    "text": "4.7 Percentiles\nWhile the mean and the standard deviation are good statistics to summarize data which follow a normal distribution, they are not so suited to summarize other distributions.\nLet us go back to one of these distributions we have encountered before: Primary energy consumption per capita in the world in the year 2019.\n\n\nCode\nlibrary(JWL)\ndat &lt;- with(energy_consumption_per_capita, energy_consumption_per_capita[Year == 2019, ])\n\nhist_info &lt;- hist(dat$Cons, plot = FALSE)         # Store output of hist function\nhist_info$counts &lt;- hist_info$counts /    # Compute relative frequency values\n  sum(hist_info$counts) * 100\nplot(hist_info, freq = TRUE, xlab = \"Primary energy consumption in kilowatt-hours per person per year.\", ylab = \"Percent\", main = \"Primary energy Consuption per Capita 2019\")              # Plot histogram with percentages\n\n\n\n\n\nThe average primary energy consumption was 26883.02 kwh per person per year and a standard deviation of 33562.48 kwh per person per year.\nIf we would just blindly apply a normal approximation using these statistics we would conclude that 21.15 % of countries in the world had a negative primary energy consumption. This does not make sense.\nThe reason for this nonsensical result is that the distribution depicted by the histogram does not follow a normal distribution. To summarize such distributions, statisticians often use so called percentiles.\n\nround(quantile(dat$Cons, c(0.01,0.10,0.25,0.50,0.75,0.90,0.99)),2)\n\n       1%       10%       25%       50%       75%       90%       99% \n   249.66   1147.96   3992.01  16516.14  33573.17  64784.08 165031.69 \n\n\nLet us first explain what you see in the code snippet. R has a built in functions to compute percentiles. This function is called quantile()and takes the data as the first argumente and then a vector of percentiles in decimal form. For example quantile(dat$Cons, 0.01) would compute the 1 % percentile.\nWhat does this mean? The first percentile in our primary energy consumption distribution is 249.66. This means that in about 1 % of the countries of the world the primary energy consumption per person per year is 250 kwh or less. 99 % of the countries had a primary energy consumption above that level. The 50 % percentile (0.5) has a special name. It is called the median. The median is often used as a measure of the center of the distribution for cases where the data are distributed in a non-symmetric way. Since the median is so important in practical work, R has a special function for the median which is called median(). If you take\n\nmedian(dat$Cons)\n\n[1] 16516.14\n\n\nyou get the 50% percentile, as you should.\nThe interquartile range is the difference between the 75 % percentile (0.75) and the 25 % percentile (0.25). This is sometimes used as a measure of spread for non symmetric distributions with long tails, such as in this example. Also in this case R has a special function, calles IQR(). You may check that it actually does what it shold do by computing:\n\nIQR(dat$Cons)\n\n[1] 29581.16\n\n\nwhich is indeed 33573.17 - 3992.01.\nSometimes data are distributed like a normal curve and sometimes they are not. We will later discuss a theoretical argument which helps to explain when precisely histograms should be approximated well by a normal curve."
  },
  {
    "objectID": "from_limited_data_to_populations.html#summary",
    "href": "from_limited_data_to_populations.html#summary",
    "title": "4  From limited data to populations",
    "section": "4.8 Summary",
    "text": "4.8 Summary"
  },
  {
    "objectID": "from_limited_data_to_populations.html#exercises",
    "href": "from_limited_data_to_populations.html#exercises",
    "title": "4  From limited data to populations",
    "section": "4.8 Exercises",
    "text": "4.8 Exercises\n\n4.8.1 Exercises\n\n\n\n\n\n\nExercise 1: Understanding the notions of sample, target population and study popuation\n\n\n\n\nAssume a farmer wants to know the average yield of some crop - say maize - in a particular region. He cannot measure the yield on each and every field. So he takes a sample. Taking the notions we have explained in the previous discussion, in this example describe in words what precisely is the sample, what is the target population, what is the study population?\nSuppose you are a an official statistician in the ministry of education in your country and the minister wants to learn about the avergage performance of students on a standardized test in your countries. Again, assume it is not possible to test all students but only a sample. Taking the notions we have explained in the previous discussion, in this example describe in words what precisly is the sample, what is the target population, what is the study population?\nFinally, assume you are the head of a firm and you would like to know the average level of customer satisfaction among your customers. You cannot ask all your customers but only a sample. Explain the meaning of the notions in this context, what is the sample, what is the study population and what is the target population?\n\n\n\n\n\n\n\n\n\nExercise 2: Now you try\n\n\n\nThink of another example for\n\ndeductive reasoning\ninductive reasoning.\n\n\n\n\n\n\n\n\n\nExercise 3: Conversion to standard units with pencil and paper\n\n\n\n\nOn a certain exam, the average of the scores was 50 and the standard deviation was 10.\n\nConvert each of the following scores to standard units: 60, 45, 75.\nFind the scores which in standard units are: 0, 1.5, -2.8\n\nConvert each entry on the following list to standard units (using the average and standard deviation of the list): 13, 9, 11, 7, 10. Find the average and the standard deviation of the converted list.\n\n\n\n\n\n4.8.2 Exercises R\n\n\n\n\n\n\nEcercise 1: Select every 100th row from the height data\n\n\n\nYou can try the code we used in the text to select every 50iest row in the heights data yourself and apply it to another situation where you would for example select every 100th observation in the dataframe.\n\n\n\n\n\n\n\n\nExercise 2: Sampling numbers from a list of numbers at random\n\n\n\nIn the stylized picture of a random sample, we had 24 households in the population from which we randomly picked a sample of 8. Use the sample function to pick randomly 8 numbers out of the numbers 1 to 24.\n\n\n\n\n\n\n\n\nExercise 3: Sample mean and sample standard deviation\n\n\n\nUse the emp_distr_height()function we have just written to draw a random sample of n=5000. Compute the mean and the standard deviation of the sample distribution. Compute - or look up in chapter 3 - the mean and the standard deviation of the population data and compare.\n\n\n\n\n\n\n\n\nExercise 4: Rolling two dice\n\n\n\nWrite a function which allows you to virtually throw a pair of dice and sum up the points shown after the throw. Simulate and plot the result of your simulation. Are these dice fair of biased? Why?\n\n\n\n\n\n\n\n\nExercise 5: Conversion into standard units\n\n\n\nConsider the values 175.9599 165.3240 169.0384 from a normal distribution with mean 172.7025 and standard deviation 4.830264. Convert these values to standard units.\n\n\n\n\n\n\n\n\nExercise 6: Find the area under the normal curve\n\n\n\n\nFind the area under the normal curve:\n\n\nto the right of 1.25\nto the left of 0.80\nbetween -0.3 and 0.9\nto the left of -0.4\nbetween 0.4 and 1.3\noutside -1.5 to 1.5\n\n\nFill in the blanks:\n\n\nThe area between +/- under the normal curve equals 68%\nThe area between +/- under the normal curve equals 75 %.\n\n\n\n\n\n\n\n\n\nExercise 7: Percentiles, median, interquartile range.\n\n\n\nTake the human height data from the JWL package socr_height_weight and compute the 1%, 10%, 25%, 50%, 75%, 90%, 99% quantiles. Compare the median and the mean and compare the standard deviation and the interquartile range. What do you observe? Explain.\n\n\n\n\n4.8.3 Exercise Project:\nTo be added.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "from_limited_data_to_populations.html#exercises-with-computer",
    "href": "from_limited_data_to_populations.html#exercises-with-computer",
    "title": "4  From limited data to populations",
    "section": "4.9 Exercises with Computer",
    "text": "4.9 Exercises with Computer\nOpen the notebook from_limited_data_to_populations.ipynb and work on the exercises. Write your solutions into the notebook. You will need the computer to do.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "what_causes_what.html#correlation-does-not-imply-causation",
    "href": "what_causes_what.html#correlation-does-not-imply-causation",
    "title": "5  What causes what?",
    "section": "5.1 Correlation does not imply causation",
    "text": "5.1 Correlation does not imply causation\nMany data sources we have, just rely on education policy outcomes somebody has observed and recorded, so called observational studies. For instance, there is a large literature, mainly from before the mid 1990ies in education research, which tried to answer the following question: Do increased financial resources for public schools improve educational outcomes as measured by standard education achievement tests?\nA look across countries would - for instance - give the following picture.\n\n\nCode\nplot(expenditure_outcome_dat$Expenditure, \n     expenditure_outcome_dat$Outcome, \n     main = (\"Average learning outcomes by total education expenditure per capita\"), \n     xlab = (\"Public and private per capita expenditure on education (PPP, constant 2011-intl $)\"), \n     ylab = (\"Average harmonized learning outcome in 2005 - 2015\"), pch = 16)\n\naxis(1, at = seq(500, 3500, by = 500))\n\n\n\n\n\nIndeed a study by Larry Hedges (1994) confirmed patterns like these. There is strong evidence that education spending and educational outcomes are positively correlated. This is the statistics jargon for saying that the variables covary positively. We will learn in detain about correlation in the next chapter.\nBut does this evidence support the conclusion that there is a causal relation between increased educational spending and student outcomes? There is a straightforward reason why this can not be the case.\nWe compare very different students or schools across countries in the scatter plot we produced before. In the study by Larry Hedges (1994) the variables that were related were students from different households across different schools. In the plot we show here it is students from different households across different countries. For the general point we want to make here, this is immaterial. What matters is that simply comparing outcomes of students across countries or students across schools with different levels of spending does not tell us whether the different spending levels are causal for the variation in outcomes we observe. There may be many other differences between countries and students that are shaping this relationship.33 There are whole websites documenting examples where two variables are highly correlated but it is obvious that there can not be a causal relation. These examples are quite funny. One site you can find for example here: https://tylervigen.com/spurious-correlations Try it out!\nStatisticians have summarized this observation in the mantra that “correlation does not imply causation”. If we want to establish a causal relation from data we need to work harder."
  },
  {
    "objectID": "what_causes_what.html#what-does-it-mean-that-a-variable-is-causal",
    "href": "what_causes_what.html#what-does-it-mean-that-a-variable-is-causal",
    "title": "5  What causes what?",
    "section": "5.2 What does it mean that a variable is “causal”?",
    "text": "5.2 What does it mean that a variable is “causal”?\nWhen you ask a doctor today about the risks of smoking, he will usually give you a warning and advise against it. One of the arguments will be that the evidence shows that smoking causes lung cancer.\nBut assume you have been a smoker and now come into the unfortunate situation that you are diagnosed with lung cancer. How do you know that your lung cancer occurred, because you were smoking and you would not have got lung cancer anyway, perhaps for an entirely different reason?\nIt took the medical profession decades to accept the causality of smoking of lung cancer. The many studies that were undertaken on this subject since the 1930es usually compared people who smoke with non-smokers and tried to take conclusions about the effect from this comparison. In this comparisons the smokers came out badly, showing among many other things higher incidence of lung cancer. So many studies together established a strong correlation between smoking and lung cancer. If smoking cause lung cancer, this would be an explanation of the association between smoking and lung cancer. But this evidence is only circumstantial. There could be some hidden factor - so called confounding factor - which makes people smoke and also makes them get lung cancer. In this case there would be no point in quitting smoking.\nThe famous statistician Sir R.A. Fisher doubted the causal role of cigarettes for lung cancer and suggested possible confounding factors. Epidemiologists conducted careful studies that accumulated evidence that these confounding factors were not plausible. Over time the evidence taken together convinced the medical community that smoking is indeed causal for lung cancer. But causal here has a meaning that takes into account the unavoidable variability that underlies real life.\nThe statistical notion of causation or causality is not deterministic. If statisticians say there is evidence that smoking is causal for lung cancer it does not mean that every smoker will end up with lung cancer. It does also not mean that lung cancer can occur only if you are a smoker. It rather means that among the group of smokers lung cancer will occur more often. Thus we can not use statistical methods to establish causation in a specific case. The only thing we can possibly establish by data is that smoking increases the proportion of times lung cancer will occur in a population.\nThis has two important consequences for what needs to be done if we want to find out what causes what. We need to intervene, that is consciously change a variable, and perform experiments. But since we are in a situation of uncertainty we need to intervene more than once, to establish strong enough evidence.\nFor this reason much of research in medicine and epidemiology today relies on evidence gained from so called controlled experiments. Let us explain next what the ideas behind such an approach is. These ideas have also been extended to other fields, like the research on what works in policies attempting to improve education and to other fields."
  },
  {
    "objectID": "what_causes_what.html#controlled-experiments",
    "href": "what_causes_what.html#controlled-experiments",
    "title": "5  What causes what?",
    "section": "5.3 Controlled experiments",
    "text": "5.3 Controlled experiments\n\n5.3.1 Field trials for early polio vaccines in the US in the 1950ies\nLet us take an example from medicine first. Suppose a new drug is found and we would like to introduce it as a medication that can be prescribed by doctors to their patients.44 This discussion follows Freedman, Pisani, and Purves (2009)\nThe basic idea of testing the effectiveness of the medicine is comparison. An experiment is designed in which the medicine is given to a treatment group. But there are other subjects that are not treated and form another group, the control group. Then the responses of the two groups are compared. The assignment of subjects to both groups should be random and the experiment should be run double blind: Neither the subjects nor the researchers who measure the response should know who was in the control group.\nThe key of randomization in this protocol is that it keeps the two groups equal except for the treatment. Then, if you measure different outcomes you can conclude with some confidence that the observed difference must be the result of the treatment. This would then establish causality.\nOne of the first epidemiological studies implementing these ideas was a randomized controlled experiment to show the effectiveness of a vaccine against polio in the US in 1954 . It was conducted by the Public Health Service and the National Foundation for Infantile Paralysis (NFPI).\nThe field trial was conducted in select school districts throughout the country where the risk of polio was high. Two million children were involved and half a million were vaccinated. A million were deliberately left unvaccinated as controls, half a million refused vaccination.\nChildren could, however, only be vaccinated with their parents consent, so only children whose parents agreed to the vaccination could go into the treatment group. Parents with higher income were more likely to consent to treatment. This biases the design against the vaccine, since polio is a disease of hygiene. Children in more hygienic surroundings have less exposure to mild cases of polio in early childhood. This prevents the generation of antibodies which protect them against severe infections at a later age.\nMany experts recognized the flaw in the design. The assignment to treatment and control was not random and thus the treatment and control group were systematically different. The idea, however, is that both groups should be as similar as possible except for treatment. If the two groups differ to some factor other than treatment this factor my confound or get mixed up with the effect of treatment.\nTo exclude the bias in assignment introduced by parental consent in the NFPI design, it was suggested that the control group has to chosen from the same population as the treatment group using an impartial chance procedure. Such an approach is called randomized controlled.\nThe idea of double blindness was also already used in a new improved design. The children in the control group were given an injection of salt dissolved in water rather than the vaccine, a so called placebo. During the experiment therefore the subjects did not know whether they were in the control group or in the treatment group. Also the doctors were not told which group the child belonged to and neither knew those who evaluated the responses.\nA subsequent program evaluation published in a scientific journal (Francis (1955)), showed the bias in the NFPI study in comparison with the randomized controlled double blind experiment:\n\n\nCode\nRCT &lt;- data.frame(Group = c(\"Treatment\", \"Control\", \"No consent\"), Size = c(200000, 200000, 350000), Rate = c(28, 71, 46))\n\nNFIP &lt;- data.frame(Group = c(\"Grade 2 (vaccine)\", \"Grades 1 and 2 (control)\", \"Grade 2 (no consent)\"), Size = c(225000, 725000, 125000), Rate = c(25, 54, 44))\n\nlibrary(knitr)\n\n# table on the left\nkable(RCT)\n# table on the right\nkable(NFIP)\n\n\n\n\n\n(a) Randomized Trial\n\n\nGroup\nSize\nRate\n\n\n\n\nTreatment\n200000\n28\n\n\nControl\n200000\n71\n\n\nNo consent\n350000\n46\n\n\n\n\n\n\n(b) NFPI study\n\n\nGroup\nSize\nRate\n\n\n\n\nGrade 2 (vaccine)\n225000\n25\n\n\nGrades 1 and 2 (control)\n725000\n54\n\n\nGrade 2 (no consent)\n125000\n44\n\n\n\n\n\n\nTable 5.1: Results of the 1954 polio vaccine trials\n\n\n\nThe two tables show the results of the randomized trial design with the NFPI design. The size of groups and the rates of polio cases per 100.000 in each group are shown in the columns Size and Rate.\nIn the randomized controlled trial the vaccination cut the polio rate from 71 to 28 per hundred thousand. The NFPI study - by contrast - shows a reduction from 54 to 25 per hundred thousand. The source of the bias which accounts for the difference was confounding. The NFPI treatment group contained only only children whose parents consented to the vaccination. However the control group contained also children whose parents would not have consented. Therefore control and treatment group were not comparable.\n\n\n5.3.2 School subsidies for the poor: The Mexican progresa poverty program\nWhile randomized controlled trials have their origin in medical research and epidemiology, this is an approach that can be applied in other contexts to establish a causal effect. In such fortunate cases, it is possible to produce actual evidence that a certain policy works or not.\nAs an example consider an education policy program for the poor designed in Mexico in the 1990 called Progresa. The problem Progresa tried to address was how a reasonable support program could be designed to increase school enrollment among the poor.\nIt was known that school enrollment among the poor was low and that one reason for this low enrollment was that children needed to work to help supporting their families rather than going to school. This situation could be characterized as a kind of poverty trap because the income of the poor families is so low that they cannot afford to enroll the children in school which prevents them to get enough training and eventually higher incomes which can raise them out of poverty.\nThe Finance minister at that time, Santiago Levy, initiated a program trying to address this problem by designing education grants to poor mothers to support the increase of school enrollment of their children. The Progresa program tried to avoid some known problems of past poverty alleviation programs. If a transfer conditions on current income, this can for instance create an incentive against taking up a job. This can happen because if a recipient of the transfer joins the paid labor force the transfer is lost as earnings increase. This can lead to disincentives to attempts of joining the paid labor force.\nThe Progresa Program therefore tried to minimize disincentives to work by not conditioning the transfer on current income but only on an initial assessment of household poverty information. This approach was tried as an alternative to build more schools and augment other educational resources such as teachers and teaching quality. While there was evidence that this approach would increase enrollment it was known that such supply policies were not effective for increasing enrollment amongt the poor. So Santiago Levy took a household demand approach providing subsidies which can administratively be targeted to the poor and thereby perhaps help increase the enrollment rates in these groups. But would this work?\nTo find out the ministry of Finance conducted a randomized controlled trial on a set of villages, selecting some of them at random for receiving Progresa benefits. To provide additional incentives for child enrollment the transfers were conditioned on whether children attended school regularly and the family used preventive health care. More money was given to children in secondary school than to children in primary school to compensate for lost wages (from going to school rather than taking up paid work) and also more money was given for enroling girls than for enroling boys.\nThrough the selection of all potentially eligible households based on geographical and household poverty information all of these households share similar characteristics. The random allocation of Progresa recipients splits this population into a treatment group (the Progresa recipients) and a control group ( the households which did not receive Progresa transfers). If the groups are then compared with respect to enrollment rates we can say that they are similar in all dimensions except treatment. Differences in enrollment rates between the groups can then be understood as causally linked to the treatment. There is no confounding. This is the basic idea of a randomized controlled trial.\nIt could indeed be established that Progresa worked to increase school enrollment among the poor in Mexico. Here are the numbers: For boys the enrollment increased from 73 % in the control group to 77 % in the Progresa group. For girls the increase was even bigger, from 67 % in the control group to 75 % in the Progresa group.\nThe evidence for the success of the Progresa Program also helped that the program survived when political power was transferred to a new government in the next election. Although Santiago Levy was not prime minister any more, the new government continued the program under the new name Oppordunitades."
  },
  {
    "objectID": "what_causes_what.html#exercises",
    "href": "what_causes_what.html#exercises",
    "title": "5  What causes what?",
    "section": "5.7 Exercises",
    "text": "5.7 Exercises\n\n5.7.1 Exercises\n\n\n\n\n\n\nExercise 1: The polio vaccine study\n\n\n\nData from the polio vaccine studies discussed in the text suggest that in 1954, the school districts in the NFIP trial and in the randomized controlled experiment had similar exposure to the polio virus. (a) The data also show that children in the two vaccine groups (for the randomized controlled experiment and the NFPI design) came from families with similar incomes and educational backgrounds. Which two numbers in table Table 5.1 confirm this finding? (b) The data show that children in the two no-consent groups had similar family backgrounds. WHich pair of numbers in the table confirm this finding? (c) The data show that children in the two control groups had different family backgrounds. Which pair of numbers in the table confirm this finding? (d) In the NGPI study, neither the control group nor the no-consent group got the vaccine. Yet the no-consent group had a lower rate of polio. Why? (e) To show that the vaccine works, someone wants to compare the 44/100.000 in the NFPI study with 25/100.000 in the vaccine group. What’s wrong with these data?\n\n\n\n\n\n\n\n\nExercise 2: Did the vaccine field trial cause children to get polio?\n\n\n\nThe polio vaccine studies were conducted only in certain experimental areas (school districts) selected by the Public Health Service in consultation with local officials. In these areas there were about 3 million children in grades 1,2, or 3; and there were about 11 million children in those grades in the United States. In the experimental areas, the incidence of polio was about 25% higher than in the rest of the country. Did the vaccine field trial cause children to get polio instead of preventing it? Answer yes or no and explain briefly.\n\n\n\n\n\n\n\n\nExercise 3: The effects of smoking on health\n\n\n\nThe Public Health Service studies the effects of smoking on health, in a large sample of representative households. For men and for women in each age group, those who had never smoked were on average somewhat healthier than the current smokers, but the current smokers were on average much healthier that those who had recently stopped smoking.\n\nWhy did they study men and women in the different age groups separately?\nThe lesson seems to be that you shouldn’t start smoking, but once you’ve started don’t stop. Comment briefly.\n\n\n\n\n\n\n\n\n\nExercise 4: Evaluating a prisoners rehabilitation program\n\n\n\nA program to rehabilitate prisoners before their release is evaluated. The object of the program is to reduce the recidivism rate - the percentage who will be back in prison within two years of release. The program involves several months of “boot camp” - military-style basic training with very strict discipline. Admission to the program is voluntary. According to a prison spokesman, those who complete the boot camp are less likely to return to prision than other inmates.\n\nWhat is the treatment group in the prison spokesman’s comparison? What is the control group?\nIs the prison spokesman’s comparison based on an observational study or a randomized controlled experiment?\nTrue of false: The data show that the bootcamp worked.\n\nExplain your answers.\n\n\n\n\n5.7.2 Exercises R\n\n\n\n\n\n\nExercise 5: Practicing the tapply function with the DHS dataset.\n\n\n\nLet’s revisit the children nutrition data children_nutrition_data in the JWLpackage from the DHS discussed in section 2. Load the JWLpackage. Earlier in our lecture we analyzed the data using the subsetting rules of the R language. Let us redo some of the analysis now using the tapply()function.\n\nCount the number of rows with boys and girls in this dataset by suming the weight variable (wt) for the boys and the girls using the tapply()function.\nCompute the prevalence of stunting by region using the tapply() function.\n\n\n\n\n\n\n\nBanerjee, Abhijit, and Esther Duflo. 2012. Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty. Public Affairs.\n\n\nFrancis, Thomas. 1955. “An Evaluation of the 1954 Poliomyelitis Vaccine Trials - Summary Report.” American Journal of Public Health 45: 1–63.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2009. Statistics. Fourth. Viva-Norton.\n\n\nLarry Hedges, Rob Greenwald, Richard Laine. 1994. “Does Money Matter? A Meta-Analysis of Studies of the Effects of Differential School Inputs on Student Outcomes.” Educational Researcher 3 (23): 5–14.\n\n\nRoser, Max. 2021. “Access to Basic Education: Almost 60 Million Children of Primary School Age Are Not in School.” https://ourworldindata.org/children-not-in-school.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books.\n\n\nTouloumis A. 2015. “R Package multgee: A Generalized Estimating Equations Solver for Multinomial Responses.” Journal of Statistical Software 64 (8): 1–14. https://doi.org/10.18637/jss.v064.i08."
  },
  {
    "objectID": "what_causes_what.html#observational-studies",
    "href": "what_causes_what.html#observational-studies",
    "title": "5  What causes what?",
    "section": "5.4 Observational studies",
    "text": "5.4 Observational studies\nControlled experiments are different from observational studies. In a controlled experiment the investigator decides who will be in the treatment group and who will be in the control group. In an observational study, by contrast, the investigator can just observe what happens and he has no control over assigning subjects to different groups. In an observational study the subjects assign themselves to treatment and control.\nThis terminology can be confusing because it uses the word control in two meanings: On the one hand, as we discussed in the previous sections a control is a subject who did not get the treatment. A controlled experiment is a study where the investigator decides who will be in the treatment group and who will be in the control group.\nGoing back to studies on the effects of smoking, which we discussed earlier in this lecture, we can see that such studies must necessarily be observational. We can not make some people smoke for a decade or more just to fulfill a statistically satisfactory research design.\nNote, however, that the basic idea of treatment and control groups is still used. The statistician in an observational study on the effects of smoking would compare the smokers - the treatment group with exposure to smoking - with the non-smokers, the control group. The idea is that we can learn something about the effects from smoking by such a comparison.\nIn this comparison, in the many observational studies that have been done on smoking since the early 20th century, smokers come out pretty badly. They suffer more frequently than non-smokers from heart attacks, lung cancer and other diseases. If smoking would cause these higher rates, this would explain the correlation observed in the data of the observational studies.\nHowever, observing this association, is incomplete as a proof for causation. There might be a hidden factor, not included in the observational study, which makes people smoke and at the same time makes people ill. Such hidden factors are called confounding factors or confounders in the technical jargon of statisticians.\nTo think about potential problems of confounding, it is helpful to ask yourselve how the controls were selected. The question is whether the control group is really similar to the treatment group apart from the exposure of interest. If there is confounding something has to be done to deal with it. Statisticians often talk about controlling for confounding factors in an observational study. This is yet another, a third sense in which the word control is used in statistics.\nThe meaning of the term control in this case is that in the case where subjects differ among themselves in an observational study in crucial ways beside treatment, the statistician adjusts the sample by comparing smaller and more homogemous groups.\nA famous example illustrating this point and featured in many textbooks5 is an observational study on sex bias in admission to graduate studies at the University of California in Berkeley in the 1975. During the study period, 8442 men and 4321 women applied for admission to graduate school. The acceptance rate among men was 44 % and that of women 35 %.5 I follow here the discussion in the textbook by Freedman, Pisani, and Purves (2009) pp. 18-19\nAssuming that men and women overall were equally well qualified this difference in the admission rate looks like evidence of a different treatment of men and women.\nSince each subject did its own admission to graduate studies, the university should have been able to identify the subjects which discriminated against women. But at that point a puzzle appeared. If subjects were looked at seperately subject by subject there didn’t seem to be any bias against women.If there was a bias it was a bias against men. How is this possible?\nOver a hundred subjects were involved. However, the six largest together accounted for over one-third of the total number of applicants. The pattern for these subjects was typical of the whole campus.\nThese are the data for men:\n\nAdmissions data for men graduate programs in the six largest subjects at University of California, Berkeley\n\n\nSubject\nNumber of Applicants\nPercent admitted\n\n\n\n\nA\n825\n62\n\n\nB\n560\n63\n\n\nC\n325\n37\n\n\nD\n417\n33\n\n\nE\n191\n28\n\n\nF\n373\n6\n\n\n\nAnd these are the data for women:\n\nAdmissions data for women graduate programs in the six largest subjects at University of California, Berkeley\n\n\nSubject\nNumber of Applicants\nPercent admitted\n\n\n\n\nA\n108\n82\n\n\nB\n25\n68\n\n\nC\n593\n34\n\n\nD\n375\n35\n\n\nE\n393\n24\n\n\nF\n341\n7\n\n\n\nIn each subject, the percentage of female applicants who were admitted is roughly equal to the percentage of male applicants. Subject A is an exception. It appears to discriminate against men because it admitted 82 % of the women and only 62 % of the men. The subject that looks most biased against women is E. But the difference here is only 4 percentage points. When we take all six subjects together they admitted 44% of the male applicants and only 30 % of the females. The difference amounts to 14 percentage points.\nThere is, however an explanation for this seemingly paradox situation. The first two subjects A and B were easy to get into. Over 50% of men applied to these two subjects. The other four were much harder to get into. Over 90% of the women applied to these subjects.\nThis means that men applied to the easy subjects whereas women applied for the hard ones. there was a confounding effect. The effect due to the choice of subject was confounded with the effect due to sex. When the choice of the subject is controlled for there is little difference between admission rates of men and women.\nThis is known as Simpson’s paradox which occurs when the apparent direction of an association is reversed by adjusting for a confounding factor."
  },
  {
    "objectID": "what_causes_what.html#exercises-1",
    "href": "what_causes_what.html#exercises-1",
    "title": "5  What causes what?",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\nTo be added."
  },
  {
    "objectID": "what_causes_what.html#can-we-ever-conclude-causation-from-observational-data",
    "href": "what_causes_what.html#can-we-ever-conclude-causation-from-observational-data",
    "title": "5  What causes what?",
    "section": "5.5 Can we ever conclude causation from observational data ?",
    "text": "5.5 Can we ever conclude causation from observational data ?\nSpiegelhalter (2019) discusses a list of criteria first suggested by the British applied statistician Austin Bradford Hill in 1965. These criteria should be considered before concluding that an observed link between an outcome and an exposure was causal. These criteria were much debated and we show here a version of the list presented in Spiegelhalter (2019) and which is due to Jermey Howick - a clinical epidemiologist form the University of Oxford in the UK - and his colleagues.\nThese criteria are separated in three categories: Direct, mechanistic and parallel evidence. I reproduce the list from the book of Spiegelhalter (2019) here:\n\nDirect evidence\n\nThe size of the effect is so large that it can not be explained by plausible confounding.\nThere is an appropriate temporal and/or spatial proximity, in that cause precedes effect and effect occurs after a plausible interval and/or cause occurs at the same site as the effect.\nThe effect increases as the exposure increases and the effect reduces upon reduction in exposure (dose responsiveness and reversibility).\n\nMechanistic evidence\n\nThere is a plausible mechanism of action which could be biological, chemical or mechanical with external evidence for a causal chain.\n\nParallel evidence\n\nThe effect fits with what is known already\nThe effect is also found when the study is replicated\nThe effect is found in similar, but not identical studies.\n\n\nThe appropriate handling of causation is still a contested field of statistics. There are various techniques that are based on the idea to use certain techniques to bring an observational study as close as possible to the situation of a randomized controlled experiment. All these techniques adjust in more or less sophisticated ways for potential confounders. Most of these techniques are based on regression, a topic we study later in this course. Whatever sophisticated technique is used judgement is always required if causation is claimed."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#statistical-models",
    "href": "modelling_relationships_using_regression_part_1.html#statistical-models",
    "title": "6  Modelling relationships: Points, Lines and Scatterplots",
    "section": "6.1 Statistical models",
    "text": "6.1 Statistical models\nIn our course so far we have encountered various methods which allow us to visualize and summarize data consisting of a single set of numbers. These simple techniques have already taken us quite far and allowed us to look at various interesting real world questions. We have also learned in some instances describing data where more than one variable is involved in studying some scatterplots.\nIn general modern data are much more complex. In such more complex situations, involving whole lists of related variables, we need more powerful tools. In this chapter we are going to take a first step in introducing the idea of a statistical model.\nA statistical model is a mathematical representation of the relationship between variables which we can use for the prediction of the value of one variable using the value of another variable or the values of a list of other variables. It also allows us a more precise description of the relationship between variables.\nStatistical models inevitably involve some mathematical ideas, which you need to understand to confidently work with these models and use them. In this chapter we will introduce you to these ideas step by step, going into actual statistical models and its most important form, regression analysis in the next chapter."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#plotting-points-and-lines",
    "href": "modelling_relationships_using_regression_part_1.html#plotting-points-and-lines",
    "title": "6  Modelling relationships: Points, Lines and Scatterplots",
    "section": "6.2 Plotting points and lines",
    "text": "6.2 Plotting points and lines\nIn this section we review some ideas about plotting points and lines which we will need for understanding statistical models. For some of you this might be a refresher of concepts you have already encountered in school, for others this material might be new.\n\n6.2.1 Reading points off a graph\nFigure 6.1 shows two coordinate systems, coordinate-system 1 and coordinate-system 2. The graphs of the coordinate systems have a horizontal axis, the \\(x\\)-axis and a vertical axis, the \\(y\\)-axis. Each point in the plane can be described by its coordinates, given by an \\(x\\) value and a \\(y\\) value.\nFor example, the point shown in coordinate-system 1 is the point \\((x,y) = (2,3)\\). This is the point you get in the plane, if you move from the origin - \\((0,0)\\) - 2 units to the right on the \\(x\\)-axis and then 3 units up on the \\(y\\)-axis.\nIn the second coordinate system, coordinate-system 2, you see the plot of a point \\((x,y) = (-1.5, -0.8)\\) with the coordinates \\((-1.5,-0.8)\\).\nThe idea to represent points in a plane by pairs of numbers (their coordinates) is very powerful because it combines two apparently different field, algebra - the manipulation of numbers by adding, subtracting etc. - and geometry, lines, points and other geometric objects.\nThis idea goes back to the french philosopher and mathematician Rene Descartes (1556 - 1630).1 This is why the \\(x\\) and \\(y\\) coordinates are often referred as “Cartesian coordinates”.1 Rene Descartes was born in 1556 and died in 1630. He was a french philosopher and mathematician and scientist. His connection of the two previously separated fields of geometry and algebra into the field of analytic geometry is one of his lasting contribution to mathematics. Here you see a portrait painted by the Dutch painter Frans Hals. \n\n\n\n\n\n\n\nCoordinate-System 1\n\n\n\n\n\n\n\nCoordinate-System 2\n\n\n\n\nFigure 6.1: Coordinate Systems\n\n\n\n\n6.2.2 Plotting Points\nAssume we want to plot a point \\((x,y) = (2,3)\\).\n\n\n\n\n\n\n\n(a) Plotting point (2,3): The coordinate system\n\n\n\n\n\n\n\n(b) Plotting point (2,3): The x-coordinate\n\n\n\n\n\n\n\n(c) Plotting point (2,3): The y-coordinate combined with the x-coordinate\n\n\n\n\nFigure 6.2: Plotting a point.\n\n\nHere is a step by step procedure outlined in Figure 6.2. We start with a coordinate system as depicted in Figure 6.2 (a). The \\(x\\) coordinate is at 2. We could imagine a dotted line starting at \\(x=2\\) and extending vertically through the coordinate system as shown in Figure 6.2 (b). Finally the \\(y\\) coordinate has value 3. We could imagine a dashed line starting at the \\(y = 3\\) coordinate parallel to the \\(x\\) axes as shown in Figure 6.2 (c) The point \\((2,3)\\) is where both of the dashed lines intersect.\nWith the help of the computer, you can of course plot points automatically. Let me show you how.\nSay we want to plot the points \\((1,1)\\), \\((2,2)\\), \\((3,3)\\) and \\((4,4)\\) with R. The way we plot these data would be that we represent the coordinates in R as a dataframe (see 3.2.4) like this.\n\npoints &lt;- data.frame(x = c(1,2,3,4), y= c(1,2,3,4))\n\nThe first column in the dataframe which we named points contains all the values of the x-coordinates, the second column contains all the values of the y-coordinates.\nNow if we give the object pointsas an argument to the plot function of R we get a plot of the points in the appropriate coordinate system. Let’s try this:\n\nplot(points, pch = 19)\n\n\n\n\nAs an aside the argument pch = 19 given to the plot function tells R to show the dots filled instead as unfilled circles. This is a bit opaque and difficult to remember. The default values also vary depending on which object is plotted. You do not have to remember such details. They are usually recovered as needed from the help function.22 Here is the full list of possible values for the pch argument and what they do. The default value in plot is pch = 1. \n\n\n6.2.3 Slope and Intercept\nThe following figure shows a line.\n\n\n\nFigure 6.3: Line\n\n\nTake any point on this line, say point A. Now move up the line to any other point, say B. When you make such a move, your \\(x\\)-coordinate has increased by some amount. We call this increase \\(\\Delta x\\).3 In our picture \\(\\Delta x = 2\\) because we moved two units to the right. At the same time you \\(y\\)-coordinate has increased by some other amount which we call \\(\\Delta y\\). In our picture \\(\\Delta y = 1\\). The ratio \\(\\Delta y/ \\Delta x\\) is called the slope of the line. Whatever points you take on this line the slope will always be 1/2.3 \\(\\Delta\\) is a greek letter and in mathematical notation it is often used to symbolize change\n\n\n\n\n\n\nImportant\n\n\n\nslope = \\(\\frac{\\Delta y}{\\Delta x}\\)\n\n\nThe slope is the rate at which \\(y\\) increases with \\(x\\) along the line.\nThe intercept of a line is the value of the \\(y\\)-coordinate at \\(x = 0\\). You could also say that the intercept is the point where the line crosses the \\(y\\)-axis.4.4 We have to be careful with this rule of thumb because this intersection property is true only if the \\(x\\)-axis and the \\(y\\)-axis cross at the origin \\((0,0)\\)\nFor example in Figure 6.3 the intercept is 1, in Figure 6.9 (a) is 2 and in Figure 6.9 (b) the intercept is 3.\n\n\n6.2.4 How to plot a line\nAssume you have to plot a line which passes through the point \\((2,1)\\) and has a slope of \\(\\frac{1}{2}\\). How would we plot this line?\nThe first step is that we draw the point \\((2,1)\\) into the coordinate system, like this:\n\n\n\nFigure 6.4: Coordinate system with point (2,1)\n\n\nNext you move any convenient distance \\(\\Delta x\\) to the right like this, say we move 3 units. Make a point at this location like this\n\n\n\nFigure 6.5: Coordinate system with point (2,1) and construction point (5,1)\n\n\nSince the line slopes up - the slope is positive an 1/2 - the line passes above this point. The question is, how far?. To find the answer to the question ho much will the line rise in a \\(\\Delta x\\) of 3? The answer can be found from the formula of the slope. The line rises at the rate of half a vertical unit per horizontal unit. Thus we have \\[\\begin{equation}\n\\Delta y = \\Delta x \\times \\text{slope}\n\\end{equation}\\] In our case this means \\(3 \\times \\frac{1}{2} = 1.5\\) Thus from the contruction point \\((5,1)\\) we have to go up 1.5 units and mark a new point there. Now if you join these two points using a ruler you get the line like this\n\n\n\nFigure 6.6: The third point\n\n\n\n\n6.2.5 The algebraic equation for a line\nHere is a rule for computing the coordinate \\(y\\) of a point from its \\(x\\) coordinate \\[\\begin{equation}\ny = \\frac{1}{2}x + 1\n\\end{equation}\\]\nLet us make a table from this equation\n\n\n\nx\ny\n\n\n\n\n1\n1.5\n\n\n2\n2.0\n\n\n3\n2.5\n\n\n4\n3.0\n\n\n\nIf you plot these points, they all fall on a line. Any point whose \\(y\\)-coordinate is related to its \\(x\\)-coordinate by the same equation \\(y = \\frac{1}{2}x + 1\\) will be on this line. This line is called the graph of the equation. The slope is 1/2, the coefficient of \\(x\\) in the equation. The intercept is 1, the constant term in the equation. We can draw a picture illustrating the situation\n\n\n\nFigure 6.7: The line through (2,2) with slope 1/2\n\n\nUsing a general formulation we can say\n\n\n\n\n\n\nImportant\n\n\n\nThe graph of equation \\(y = kx + d\\) is a straight line with slope \\(k\\) and intercept \\(d\\)\n\n\n\n\n6.2.6 Plotting lines, using R\nThe examples we discussed so far helped you understand how to draw equations like \\[\\begin{equation}\ny = k x + d\n\\end{equation}\\]\nby hand and to switch between the algebraic and geometric description of such equations. Of course you can use R to do such plots.\nWe briefly discuss how. Let us generate some example data for the case of the first equation in exercise 1:\n\\[\\begin{equation}\ny = 2x + 1\n\\end{equation}\\]\nWe can draw such lines in R using the R function curve(). This function requires as arguments the equation of the line and the range from where to where the \\(x\\) variable should go. Let us start at \\(x=0\\) and let x go to \\(x=5\\):\n\ncurve(1 + 2*x, from = 0, to = 5)\n\n\n\n\nHere is another example, which is slightly more interesting because of its context. Consider the linear equation\n\\[\\begin{equation}\ny = 1007 - 0.393 x\n\\end{equation}\\]\nAccording to Andrew Gelman (2021) this line approximates the trajectory of the world record time in seconds for the mile run based on data starting from the year 1900.\nTo draw a curve like this by R, and assuming we plot the curve for the arguments \\(x = 1\\), \\(x = 2\\), \\(x = 3\\) , \\(x = 4\\) and \\(x = 5\\), you would best choose the R function curve() and a code like this:\n\ncurve(1007 - 0.393*x, from = 0, to = 5)\n\n\n\n\n\n\n\n\n\n\nNow you try\n\n\n\nTest your understanding of interpreting this line in the given context.\n\nWhat is the intercept of the line?\nWhat is the slope?\nWhat is the interpretation of the intercept in the given context?\nWhat is the interpretation of the slope in this context?\nWhat is the mile run record in the beginning of our data in 1900?\nWhat is the mile run record in 1990 assuming the line describes the relation accurately?\nIs it reasonable to use the line to predict the mile run record in 2000? What do you think?\nIs it reasonable to assume that this trend will go on forever?\n\n\n\nFinally, let us assume we want to present a graph of the line, conveying the context and which is easily readable. Then we need to use a different location and scale of the graph and add a title as well as informative descriptions of the \\(x\\) and the \\(y\\) axis. This would be done in R as shown in the following code.\n\ncurve(1007 - 0.393*x, \n      from = 1900, to = 2000, \n      xlab = \"Year\", \n      ylab = \"Time (seconds)\",\n      main = \"Approximate trend of world record times\\n for the mile run\")\n\n\n\n\nLocation and scale are controlled by starting the curve not at \\(x=0\\) but at \\(x=1900\\). R then automatically adapts the scale by choosing appropriate units for \\(x\\) and displaying not each single year."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_2.html#contents",
    "href": "modelling_relationships_using_regression_part_2.html#contents",
    "title": "7  Modelling Relationships using Regression Part 2",
    "section": "7.1 Contents",
    "text": "7.1 Contents"
  },
  {
    "objectID": "modelling_relationships_using_regression_part_2.html#outcome",
    "href": "modelling_relationships_using_regression_part_2.html#outcome",
    "title": "7  Modelling Relationships using Regression Part 2",
    "section": "7.2 Outcome",
    "text": "7.2 Outcome"
  },
  {
    "objectID": "how_sure_can_we_be.html#contents",
    "href": "how_sure_can_we_be.html#contents",
    "title": "8  How sure can we be about what is going on",
    "section": "8.1 Contents",
    "text": "8.1 Contents\nProbability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable."
  },
  {
    "objectID": "how_sure_can_we_be.html#outcome",
    "href": "how_sure_can_we_be.html#outcome",
    "title": "8  How sure can we be about what is going on",
    "section": "8.2 Outcome",
    "text": "8.2 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "9  Probability: Quantifying uncertainty and variablility",
    "section": "",
    "text": "10 Contents\nProbability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable.\n\n\n11 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability_and_statistics.html#contents",
    "href": "probability_and_statistics.html#contents",
    "title": "10  Putting probability and statistics together",
    "section": "10.1 Contents",
    "text": "10.1 Contents\nThis will be conceptually the most difficult part of the course. The main ideas that should be conveyed in this unit are\n\nUsing probability theory we can derive the sampling distribution of summary statistics from which formulae for confidence intervals can be derived.\nExplain what a 95 % confidence interval means\nThe central limit theorem and the normal distribution\nThe role of systematic error due to non random causes and the role of judgment\nExplain the idea that confidence intervals can be calculated even when we observe all the data which then represent uncertainty about the parameters of an underlying metaphorical population."
  },
  {
    "objectID": "probability_and_statistics.html#outcome",
    "href": "probability_and_statistics.html#outcome",
    "title": "10  Putting probability and statistics together",
    "section": "10.2 Outcome",
    "text": "10.2 Outcome\nThe students should gain a firm understanding of confidence intervals and how they help us in quantifying uncertainty of predictions we make based on our available data. They should see and understand how and why it is sometimes more convenient and parsimonious to have formulae for confidence intervals rather than quantifying the uncertainty from simulation. The intuitive understanding of the limit theorems and when they can be legitimately applied will be important here."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html#contents",
    "href": "answering_questions_and_claiming_discoveries.html#contents",
    "title": "11  Answering questions and claiming discoveries",
    "section": "11.1 Contents",
    "text": "11.1 Contents\nFormal statistical testing as a major empirical tool for answering questions and claiming discoveries.\n\nTests of null hypothesis as a major part of statistical practice\np-value as the measure of incompatibility between the observed data and the null hypothesis\nThe traditional p value thresholds.\nThe need to adjust thresholds with multiple tests\nCorrespondence between p-values and confidence intervals\nNeyman-Pearson theory (alternative hypothesis and type 1 and type 2 error).\nSequential testing\nThe misinterpretation of p-values."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html#outcomes",
    "href": "answering_questions_and_claiming_discoveries.html#outcomes",
    "title": "11  Answering questions and claiming discoveries",
    "section": "11.2 Outcomes",
    "text": "11.2 Outcomes\nStudents should learn the basic ideas of hypothesis testing and the terminology around it."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Antony, Volk, and Atkinson Jeremy. 2013. “Infant and Child Death\nin the Human Environment of Evolutionary Adaptation.”\nEvolution and Human Behavior 34: 182–92.\n\n\nBanerjee, Abhijit, and Esther Duflo. 2012. Poor Economics: A Radical\nRethinking of the Way to Fight Global Poverty. Public Affairs.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for\nCommunication. New Riders.\n\n\nFrancis, Thomas. 1955. “An Evaluation of the 1954 Poliomyelitis\nVaccine Trials - Summary Report.” American Journal of Public\nHealth 45: 1–63.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2009.\nStatistics. Fourth. Viva-Norton.\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nLarry Hedges, Rob Greenwald, Richard Laine. 1994. “Does Money\nMatter? A Meta-Analysis of Studies of the Effects of Differential School\nInputs on Student Outcomes.” Educational Researcher 3\n(23): 5–14.\n\n\nRoser, Max. 2019. “Child Mortality Is an Everyday Tragedy of\nEnormous Scale That Rarely Makes the Headlines.” https://ourworldindata.org/child-mortality-everyday-tragedy-no-headlines.\n\n\n———. 2021. “Access to Basic Education: Almost 60 Million Children\nof Primary School Age Are Not in School.” https://ourworldindata.org/children-not-in-school.\n\n\nRosling, Hans, Ola Rosling, and Anna Rosling-Rönnlund. 2018.\nFactfullness, 10 Reasons Why We Are Wrong about the World - and Why\nThings Are Better Than You Think. Sceptre.\n\n\nSmil, Vaclav. 2020. Numbers Don’t Lie: 71 Things You Need to Know\nabout the World. Penguin Books.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from\nData. Pelican Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#exercises",
    "href": "modelling_relationships_using_regression_part_1.html#exercises",
    "title": "6  Modelling relationships: Points, Lines and Scatterplots",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\n6.4.1 Exercises\n\n\n\n\n\n\nExercise 1: Points in a coordinate system\n\n\n\n\n\n\nFigure 6.8: Five points in a coordinate system\n\n\nFigure 6.8 shows 5 points plotted in a coordinate system.\n\nWrite down the \\(x\\) and \\(y\\) coordinate for each point.\nAs you move from point A to point B in Figure 6.8, explain by how mauch your \\(x\\)-coordinate and your \\(y\\)-coordinate goes up.\nOne point in Figure 6.8 has a \\(y\\)-coordinate 1 bigger than the \\(y\\)-coordinate of point E. Which point is this?\n\n\n\n\n\n\n\n\n\nExercise 2: What is the slope?\n\n\n\nWhat is the slope of these two lines?\n\n\n\n\n\n\n\n(a) Slope 1\n\n\n\n\n\n\n\n(b) Slope 2\n\n\n\n\nFigure 6.9: Different slopes\n\n\n\n\n\n\n\n\n\n\nExercise 3: Where on the line are you?\n\n\n\n\nDraw lines through the point (2,1) with the following slopes: 1, -1, 0\nStart at point (2,1). If you move 2 to the right and 1 up will you be on the line, above the line or below the line?\nDraw the line with intercept 2 and slope -1.\nDraw the line with intercept 2 and slope 1.\n\n\n\n\n\n\n\n\n\nExercise 4: Plotting graphs (by hand)\n\n\n\n\nPlot the graphs of the following equations \\[\\begin{eqnarray}\ny &=& 2 x + 1 \\\\\ny &=& \\frac{1}{2}x + 2\n\\end{eqnarray}\\] In each case, determine the slope and the intercept and give the height of the line at \\(x = 2\\).\nPlot four different points whose \\(y\\)-coordinate are double their \\(x\\)-coordinates. Do these points lie on a line? What is the equation of this line?\nPlot the points \\((1,1)\\), \\((2,2)\\), \\((3,3)\\) and \\((4,4)\\) on the same graph. These points all lie on a line. What is the equation of this line?\nTrue of false:\n\nIf \\(y\\) is bigger than \\(x\\), then the point \\((x,y)\\) is above the line of exercise 3.\nIf \\(y = x\\) then the point \\((x,y)\\) is on the line of exercise 3.\nIf \\(y\\) is smaller than \\(x\\), then the point \\((x,y)\\) is below the line of exercise 3.\n\n\n\n\n\n\n\n\n\n\nExercise 5: Where is the point\n\n\n\n\nThree out of the following four points lie on a line. Which one does not? It is above or below the line? \\((0,0), (0.5,0.5), (1,2), (2.5, 2.5)\\) You can try to answer this with pencil and paper or using R. Try both ways.\nThe table below shows four points. In each case the y-coordinate is computed from the x-coordinate by the rule \\(y = 2x + 1\\). Fill in the blanks and then plot the four points. What can you say about them?\n\n\n\n\nx\ny\n\n\n\n\n1\n3\n\n\n2\n5\n\n\n3\n\n\n\n4\n\n\n\n\n\nLook at the following shaded regions. Which of the following two points is in the region (1,2) or (2,1)? Answer this question for all three figures.\n\n\n\n\n\n\n\n\n(a) Figure 1\n\n\n\n\n\n\n\n(b) Figure 2\n\n\n\n\n\n\n\n(c) Figure 3\n\n\n\n\nFigure 6.10: Where is the point.\n\n\n\n\n\n\n\n\n6.5 Exercise 6: The mathematics of lines\nLook at the following line\n\n\n\n\n\n(a) Figure 3\n\n\n\n\n\nWhat is the equation of this line?\nWhat is the height of the this line at \\(x = 1\\)?\nPlot the line whose equation is \\(y = - \\frac{1}{2}x + 4\\)\n\n\n\nFigure 6.11: ?(caption)"
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#exercise-2",
    "href": "modelling_relationships_using_regression_part_1.html#exercise-2",
    "title": "6  Modelling relationships",
    "section": "6.4 Exercise 2:",
    "text": "6.4 Exercise 2:\n\nThree out of the following four points lie on a line. Which one does not? It is above or below the line? \\((0,0), (0.5,0.5), (1,2), (2.5, 2.5)\\) You can try to answer this with pencil and paper or using R. Try both ways.\nThe table below shows four points. In each case the y-coordinate is computed from the x-coordinate by the rule \\(y = 2x + 1\\). Fill in the blanks and then plot the four points. What can you say about them?\n\n\n\n\nx\ny\n\n\n\n\n1\n3\n\n\n2\n5\n\n\n3\n\n\n\n4\n\n\n\n\n\nLook at the following shaded regions. Which of the following two points is in the region (1,2) or (2,1)? Answer this question for all three figures.\n\n\n\n\n\n\n\n\n(a) Figure 1\n\n\n\n\n\n\n\n(b) Figure 2\n\n\n\n\n\n\n\n(c) Figure 3\n\n\n\n\nFigure 6.10: Where is the point.\n\n\n\n\n\n\nAndrew Gelman, Aki Vehtari, Jenifer Hill. 2021. Regression and Other Stories. Cambridge University Press."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#computer-exercises",
    "href": "modelling_relationships_using_regression_part_1.html#computer-exercises",
    "title": "6  Modelling relationships: Points, Lines and Scatterplots",
    "section": "6.5 Computer exercises",
    "text": "6.5 Computer exercises\nOpen the Jupyter notebook “points_and_lines.ipynb” and work on the exercises there. When you have done the exercises save your notebook and submit it.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAndrew Gelman, Aki Vehtari, Jenifer Hill. 2021. Regression and Other Stories. Cambridge University Press."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#modelling-relationships-and-making-predictions",
    "href": "modelling_relationships_using_regression_part_1.html#modelling-relationships-and-making-predictions",
    "title": "6  Modelling relationships: Points, Lines and Scatterplots",
    "section": "6.3 Modelling relationships and making predictions",
    "text": "6.3 Modelling relationships and making predictions\nWith the understanding of the mathematics of points and lines you are now well prepared for the next big topic in this course: Prediction. An important aspect of data analysis is what given data can tell us about the future. In the next chapter we will learn about one of the most used methods for making predictions: Regression analysis. Regression analysis helps us to model how different variables depend on each other and how we can make predictions using our understanding of this relation.\nRelations between two variables are described in statistics using scatter plots. A famous example from the history of statistics is an anthropometric dataset assembled by one of the pioneers of Statistics, Charles Pearson5.5 Karl Pearson 1857 – 1936 was an English mathematician and biostatistician. He has been credited with establishing the discipline of mathematical statistics.He founded the world’s first university statistics department at University College London in 1911, and contributed significantly to the field of biometrics and meteorology. \nThe data are in the JWL package and are called pearson. Lets store these data in a variable we call dat here:\n\nlibrary(JWL)\ndat &lt;- pearson\n\nWhen we plot the heights of fathers and sons in one diagram, each pair - i.e. each row in dat - is a point and the set of all points plotted in this way is called a scatter-plot. When we give datto R as an argument R will automatically produce a scatter-plot. We show the code here:\n\nplot(dat, \n     pch = 19, \n     cex = 0.3, \n     xlab = \"Father's height (inches)\", \n     ylab = \"Son's height (inches)\",\n     main = \"Pearson's height data from 1903\")\nabline(0,1, lty = 2)\nabline(v = 71.5, lty = 2, col = \"blue\")\nabline(v = 72.5, lty = 2, col = \"blue\")\n\n\n\n\nLet me briefly explain the code. plot(dat) would produce a default scatter plot with R. Here I adjust a few graphical parameters to make the plot more readable: I do not have to explain again how xlab, ylab and main works. pch = 19 makes the dots filled contrary to the hollow points of the default configuration and cex = 0.9 makes them smaller than the default, so you can see the differences better.\nThe figure illustrates the mechanics of scatter plots. It is a cloud of points, which in this case slopes upward to the right, the y-coordinates tending to increase as the x-coordinate increases. One says that there is a positive association between the heights of the fathers and the heights of the sons. The dashed diagonal line is the line of all points where the son’s father’s height and the son’s height are equal. If the father’s height is close to the son’s height the points are close to the line.\nThere is quiet some spread, showing the the relationship is weak. Suppose for example that you have to guess the height of a son from a father’s height. In the stripe bordered by the blue dashed lines you see all the points where the father is 72 inches to the nearest inch. There is a lot of variability in the heights of the sons. You can see it in the scatter within the stripe. So even if you know the father’s height there is still a lot of room for error.\n\n\n\n\n\n\nImportant\n\n\n\nIf there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_2.html",
    "href": "modelling_relationships_using_regression_part_2.html",
    "title": "7  Modelling relationship: Prediction using regression",
    "section": "",
    "text": "We ended the last chapter with Pearson’s historical data on the heights of fathers and sons to introduce the concept of a scatter-plot.\n\n\nCode\nlibrary(JWL)\ndat &lt;- pearson\n\nplot(dat, \n     pch = 19, \n     cex = 0.3, \n     xlab = \"Father's height (inches)\", \n     ylab = \"Son's height (inches)\",\n     main = \"Pearson's height data from 1903\")\nabline(0,1, lty = 2)\n\n\n\n\n\nWe discussed how we could use the positive association in this scatter-plot to predict the height of a son from the height of the father. One intuition might be to use the straight dashed line that run along the diagonal. If we would use this line as a model for prediction, each adult son would be predicted to show the same height as his father. But we can do better than that and improve on this choice.\nFor any straight line we might choose, each data-point will give rise to a residual which is the size of the error were we to use the line as prediction.\n\n\nCode\nplot(dat, \n     pch = 19, \n     cex = 0.3, \n     xlab = \"Father's height (inches)\", \n     ylab = \"Son's height (inches)\",\n     main = \"Pearson's height data from 1903\")\nabline(0,1, lty = 2)\nabline(reg = lm(dat$Son ~ dat$Father), col = \"red\", lwd = 2)\npoints(63.1, 74.3, pch = 19, col = \"red\")\nlines(c(63.1, 63.1), c(66.3264, 74.3), col = \"red\", lty = 2)\n\n\n\n\n\nSuppose for instance that we took the father with a height of 63.1 inches and predicted the height of the son according to the model described by the red line, we would have predicted the son to have height 66.3 inches. In the data the son of the father with height 63.1 inches is in fact 74.3 inches. This is marked out as the red point in the plot. The length of the dashed line from the red point to the red line is the residual.\nThe idea of prediction using models of straight lines is to make these residuals small. We will learn a method how to do this. This approach to prediction is called regression and we will learn how it works in this chapter."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_2.html#correlation",
    "href": "modelling_relationships_using_regression_part_2.html#correlation",
    "title": "7  Modelling relationship: Prediction using regression",
    "section": "7.1 Correlation",
    "text": "7.1 Correlation\nIn the beginning of this lecture we studied the example of infant mortality. Let us first study a scatter plot which plots the number of children per woman against the infant mortality rate across countries in the world in the year 2000.\n\n\nCode\nlibrary(JWL)\ndat &lt;- infant_mortality_data\n\ndat_2000 &lt;- dat[dat$Year == 2000, ]\n\nplot(dat_2000$Children, dat_2000$Mortality,\n     yaxt = \"n\",\n     xlab = \"Children per woman\",\n     ylab = \"Infant mortality\",\n     pch = 19,\n     cex = 0.5)\naxis(2, at=pretty(dat_2000$Mortality), lab=paste0(pretty(dat_2000$Mortality) * 100, \" %\"), las=TRUE)\n\n\n\n\n\nNotice the positive association in the data. The scatter of points is sloping upwards, indicating that a higher number of children per woman tends to go along with a higher infant mortality rate.\nLet’s look at another example, where we plot the average years of schooling for women on the x-axis and child mortality1 on the y-axis. Here we see a negative correlation. Child mortality across countries in a given year, here the year 2020, tends to be negatively associated with the average years of schooling of women. In such cases we speak of a negative correlation.1 Remember from the section on counts and categorical data that child mortality is the rate of children who die before their fifth birthday. Infant mortality in contrast is the rate of children dying before their first birthday.\n\n\nCode\ndata &lt;- child_mortality_average_schooling_of_women\n\ndata_2020 &lt;- data[data$Year == 2020, ]\n\nplot(data_2020$YEW15, data_2020$MR,\n     xlab = \"Average years of schooling for women\",\n     ylab = \"Child mortality\",\n     pch = 19,\n     cex = 0.5)\n\n\n\n\n\nThe correlation coefficient measures how tightly a scatterplot is clustered around a straight line. The correlation coefficient is therefore called a measure of linear association. It is usually expressed shorthand as correlation and often abbreviated by the letter \\(r\\).\nHere are some mathematical properties of correlation which we will observe by simulation in a next step.\n\n\n\n\n\n\nMathematical properties of correlation\n\n\n\n\nThe correlation coefficient \\(r\\) is a number between -1 and +1.\n\\(r\\) measures the extend to which the scatter plot clusters around a straight line.\n\\(r=1\\) if the scatter plot is a straight line sloping upwards and \\(r = -1\\) if the scatter plot is a straight line sloping downwards\n\n\n\n\n7.1.1 The correlation coefficient\nR has a built in function for computing the correlation coefficient. This function is called cor() and takes the \\(x\\) and the \\(y\\) values of a scatter plot as arguments. So, for example, take the data on children per woman and infant mortality.\nWe read the data from the JWLpackage and store them in an object which we call dat.\n\nlibrary(JWL)\ndat &lt;- infant_mortality_data\n\nLet’s check the variable names.\n\nnames(dat)\n\n[1] \"Country\"    \"Code\"       \"Year\"       \"Mortality\"  \"Continent\" \n[6] \"Population\" \"Births\"     \"Deaths\"     \"Children\"  \n\n\nWe filter the data for the year 2000 and store these data in a new object called dat_2000\n\ndat_2000 &lt;- dat[dat$Year == 2000, ]\n\nNow we compute the correlation between the average number of children per woman Children and the child mortality rate Mortality, where each data-point corresponds to a particular country around the world.\n\ncor(dat_2000$Children, dat_2000$Mortality)\n\n[1] 0.8711762\n\n\nindicating a strong positive correlation.\n\n\n\n\n\n\nWrite your own R-function to study properties of correlation\n\n\n\nNow let us use a function, which we call r_scatter to explore how the correlation coefficient shapes a scatter plot. This is an excellent opportunity to refresh our knowledge of how to write your own functions in R.\nFirst, remember that each function is an R object and has a name. We already decided to call the function r_scatter. Then the syntax is to write function() and put the function arguments within the paranthesis.\nHere we have two arguments, the correlation coefficient - let us call it according to the convention r - and the number of data points we want to consider. We call this number n and assign a default value to it, let’s say 1000.\nNext we have to write the function body between {}. This is the part containing the actual instructions.\nFirst we create two vectors of length \\(n\\) of values \\(x\\) and \\(z\\) from a normal distribution with mean 0 and standard deviation 1. This is achieved by the r function rnorm(n). Note that n is a function argument and since we have set a default value of n, unless specified otherwise R will set n to 1000.\nNext we use a mathematical fact, which we do not explain here in detail. When \\(x\\) and \\(y\\) have the same standard deviation then the variable \\(y\\) given by \\[\\begin{equation}\ny = r x + \\sqrt{1 - r^2} z\n\\end{equation}\\] will have correlation coefficient \\(r\\) with \\(x\\). Thus we can implement this formula in the function body, leaving \\(r\\) unspecified. It is the function argument of r_scatter and will be specified by you when you call the function with a specific \\(r\\).\nThen we do a scatterplot of \\(x\\) and \\(y\\) which is the output of the function. The function will always render the last expression in {} as the output.\nThis is how the code looks like in its entirety:\n\nr_scatter &lt;- function(r, n = 1000){\n  \n  x &lt;- rnorm(n) # create n observations from a standard normal distribution\n  z &lt;- rnorm(n) # create n observations from a standard normal distribution\n  \n  y &lt;- r*x + sqrt(1-r^2)*z # implement the formula discussed in text.\n  \n  # do a scatter plot of x and y\n  \n  plot(x,y,\n     xlab = \"x\",\n     ylab = \"y\",\n     main = paste(c(\"Correlation coefficient is\", r), collapse = \" \"),\n     pch = 19,\n     cex = 0.5)\n  \n}\n\nNow we call r_scatter a few times to see what the correlation coefficient does.\n\nr_scatter(0.9)\n\n\n\n\n\nr_scatter(0.25)\n\n\n\n\n\nr_scatter(0)\n\n\n\n\n\nr_scatter( -0.7)\n\n\n\n\n\nr_scatter(- 0.98)\n\n\n\n\n\n\n\n\n7.1.2 Computing r\nHow does cor() compute the correlation coefficient? We do not give a mathematical derivation here. One way to explain the computation is to say that\n\n\n\n\n\n\nFormula for \\(r\\)\n\n\n\n\\(r\\) is the average of the products of the two variables when both variables are measured in standard units or z-scores.\n\n\nLet’s do a simple example in R to explain the computation. Consider the follwoing toy data\n\n\nCode\nx &lt;- c(1,3,4,5,7)\ny &lt;- c(5,9,7,1,13)\n\nknitr::kable(data.frame(x,y))\n\n\n\n\n\nx\ny\n\n\n\n\n1\n5\n\n\n3\n9\n\n\n4\n7\n\n\n5\n1\n\n\n7\n13\n\n\n\n\n\nTo convert into standard units we have to subtract the mean from each observation and divide by the standard deviation. You should be able to verify for yourself that the mean of \\(x\\) is 4 and the standard deviation is 2 and for \\(y\\) the corresponding values are 7 and 4.\nDoing this operation on the values in our table will give us:\n\n\nCode\nknitr::kable(data.frame(x,y, x_su = (x - mean(x))/(sd(x)*sqrt(4/5)), y_su = (y- mean(y))/(sd(y)*sqrt(4/5))))\n\n\n\n\n\nx\ny\nx_su\ny_su\n\n\n\n\n1\n5\n-1.5\n-0.5\n\n\n3\n9\n-0.5\n0.5\n\n\n4\n7\n0.0\n0.0\n\n\n5\n1\n0.5\n-1.5\n\n\n7\n13\n1.5\n1.5\n\n\n\n\n\nFinally we have to add the product of the standardized values \\(x_{su}\\) and \\(y_{su}\\) to get\n\n\nCode\nknitr::kable(data.frame(x,y, x_su = (x - mean(x))/(sd(x)*sqrt(4/5)), y_su = (y- mean(y))/(sd(y)*sqrt(4/5)), xy_su = (x - mean(x))/(sd(x)*sqrt(4/5))*(y- mean(y))/(sd(y)*sqrt(4/5))))\n\n\n\n\n\nx\ny\nx_su\ny_su\nxy_su\n\n\n\n\n1\n5\n-1.5\n-0.5\n0.75\n\n\n3\n9\n-0.5\n0.5\n-0.25\n\n\n4\n7\n0.0\n0.0\n0.00\n\n\n5\n1\n0.5\n-1.5\n-0.75\n\n\n7\n13\n1.5\n1.5\n2.25\n\n\n\n\n\nTaking the average across these values gives \\(r\\). Thus we get \\[\\begin{equation}\n\\frac{0.75-0.25+0.00-0.75+2.25}{5} = 0.40\n\\end{equation}\\]\nNow check this against cor():\n\ncor(x,y)\n\n[1] 0.4\n\n\nLet’s do a scatter plot of the data:\n\n\nCode\nplot(x,y)\n\ngrid(nx = 2, ny = 2,\n     lty = 2, col = \"gray\", lwd = 2)\n\npar(new = TRUE)\nplot(x,y, \n     xlab = \"x\",\n     ylab = \"y\",\n     main = \"Scatter plot toy example\",\n     pch = 19, \n     col = 4)\ntext(x = 4.1, y = 6.6, \n     label = \"0\")\ntext(x = 3, y = 8.4, \n     label = \"-0.25\")\ntext(x = 1, y = 4.4, \n     label = \"0.75\")\ntext(x = 5, y = 1.6, \n     label = \"-0.75\")\ntext(x = 7, y = 12.4, \n     label = \"2.25\")\ntext(x = 2.5, y = 10, \n     label = paste(c(\"-\", \"-\", \"-\", \"-\")), cex = 4)\ntext(x = 5.5, y = 10, \n     label = \"+\", cex = 4)\ntext(x = 5.5, y = 4, \n     label = paste(c(\"-\", \"-\", \"-\", \"-\")), cex = 4)\ntext(x = 2, y = 4, \n     label = \"+\", cex = 4)\n\n\n\n\n\nIn this plot the \\(x\\) axis and the \\(y\\) axis show the values of the original table and in the plot we associate the values of the product of standardized units in the plot. The dashed lines are plotted through the points of averages, dividing the scatter plot into four quadrants. If a point is in the lower left quadrant both variables are below average and are negative in standard units. So their product is positive. In the upper right quadrant the product of two positives is positive. In the remaining two quadrants the product of a positive and a negative is negative.2 The average of all these products is the correlation coefficient.2 For those of you who need a refresher about the algebra of multiplying positive and negative numbers, you might resort to the following intuition: \\(3 \\times 5 = 15\\): Getting 5 Dollars 3 times gives you 15 Dollars. \\(3 \\times (-5) = -15\\): Paying a 5 Dollar penalty 3 times gives you a 15 Dollar penalty. \\((-3) \\times 5 = -15\\): Not getting 5 Dollars 3 times is not getting 15 Dollars. $(-3) (-5) = 15: Not paying a 5 Dollar penalty 3 times is getting 15 Dollars.\nIf \\(r\\) is positive, then points in the two positive quadrants will predominate as for example here\n\n\nCode\nr_scatter(r = 0.93)\n# Vertical grid\nabline(v = 0,\n       lty = 2, col = \"gray\")\n\n# Horizontal grid  \nabline(h = 0,\n       lty = 2, col = \"gray\")\n\n\n\n\n\nIf \\(r\\) is negative points in the two negative quadrants will dominate as for example here.\n\n\nCode\nr_scatter(r = -0.95)\n# Vertical grid\nabline(v = 0,\n       lty = 2, col = \"gray\")\n\n# Horizontal grid  \nabline(h = 0,\n       lty = 2, col = \"gray\")\n\n\n\n\n\nNote that the way correlation as a measure of linear association makes \\(r\\) sensitive to outliers. Consider the following example from Adhikari, DeNero, and Wagner. Let’s assume we have a scatter plot like this\n\n\nCode\nx &lt;- c(1:4)\ny &lt;- c(1:4)\nplot(x,y, col = \"red\", pch = 19)\n\n\n\n\n\nThe correlation coefficient here is 1, since the points are all exactly on a line. Now assume we add a fifth point, like this - an outlier.\n\n\nCode\nx &lt;- c(1:4, 5)\ny &lt;- c(1:4, 0)\nplot(x,y, col = \"red\", pch = 19)\n\n\n\n\n\nNow the correlation \\(r\\) is 0. So a perfect correlation is changed to no correlation by just adding one single outlying point.\n\n\n7.1.3 Properties of r\nFrom these observations we can observe three properties of \\(r\\)\n\n\n\n\n\n\nThree properties of r:\n\n\n\n\n\\(r\\) is a pure number. It has no units. This is because \\(r\\) is computed from standard units.\n\\(r\\) is unaffected by changing the units on either axis. This is again because \\(r\\) is computed based on standard units.\n\\(r\\) is unaffected when we switch the axes. Algebraically this must be the case because when we multiply \\(x\\) by \\(y\\) the value of the product is the same as when we multiply \\(y\\) by \\(x\\). Geometrically switching axes reflects the scatterplot along the line \\(x=y\\) but does not change the amount of clustering along the line.\n\n\n\n\n\n7.1.4 r is a measure of clustering along a line.\n\\(r\\) is a measure of clustering along a line. This is often referred to by calling \\(r\\) a measure of linear association. Note that data are often associated in other ways not necessarily along a line. In such cases \\(r\\) would show a weak association only.\nAs one real world examples let us take the relation between the share of extremely poor people across countries in the world in a given year versus GDP per capita in that same year. Here we take the year 2015. This is what we get.\n\n\nCode\nlibrary(JWL)\n\n# remove World, which should not be there and Luxembourg which is an extreme outlier\n\ndata_2015 &lt;- poverty_vs_gdp[(poverty_vs_gdp$Year == 2015 &\n                            !(poverty_vs_gdp$Entity %in% c(\"World\", \"Luxembourg\"))), ]\n\noptions(scipen = 999)\nplot(data_2015$GDPpc, data_2015$Share,\n     xlab = \"GDP per capita\",\n     ylab = \"Share in extreme poverty\",\n     main = \"Share of population living in extreme poverty vs. GDP per\ncapita, 2015\",\n     pch = 19,\n     cex = 0.5,\nxaxt = \"n\",\nyaxt = \"n\")\n\naxis(1, at = c(0, 20000, 40000, 60000, 80000),\n     labels = paste0(c(\"0\", \"20.000\", \"40.000\", \"60.000\", \"80.000\"), \" \", \"$\"), cex.axis = 0.9)\naxis(2, at = c(0, 10, 20, 30, 40, 50, 60),\n     labels = paste0(as.character(c(0, 10, 20, 30, 40, 50, 60)), \" \", \"%\"), cex.axis = 0.9)\n\n\n\n\n\nYou can see that the data show a strong association between GDP per capita and the share of extreme poverty, which is intuitive. Moreover this association is not along a straight line.\nIf we would compute the correlation coefficient in these data, we would get a value of -0.4532214, a weak negative association.\nIn the following picture we show the data with smooth curve that approximates the shape of the data in red. Had we fitted a straight line to these data, you can see from the picture that the fit is poor. Though there seems to be a relatively strong association between GDP per capita and the share of people living in extreme poverty, it is not a linear association, i.e. an association along a straight line. The correlation coefficient only captures the clustering of data points along a straight line.\n\n\nCode\ndf &lt;- data.frame(x = data_2015$GDPpc, y = data_2015$Share)\nplot(df$x, df$y,\n     xlab = \"GDP per capita\",\n     ylab = \"Share in extreme poverty\",\n     main = \"Share of population living in extreme poverty vs. GDP per\ncapita, 2015\",\n     pch = 19,\n     cex = 0.5,\nxaxt = \"n\",\nyaxt = \"n\")\n\naxis(1, at = c(0, 20000, 40000, 60000, 80000),\n     labels = paste0(c(\"0\", \"20.000\", \"40.000\", \"60.000\", \"80.000\"), \" \", \"$\"), cex.axis = 0.9)\naxis(2, at = c(0, 10, 20, 30, 40, 50, 60),\n     labels = paste0(as.character(c(0, 10, 20, 30, 40, 50, 60)), \" \", \"%\"), cex.axis = 0.9)\n\nmod &lt;- lm(y ~ x, data = df)\n\nlines(stats::lowess(df, f= 1/7), lw = 2, col= \"red\")\n\nabline(mod, col = 4, lw = 2)\n\n\n\n\n\n\n\n7.1.5 Association is not causation\nIn the discussion of correlations we need to come back to an issue we had discussed in a previous lecture. It is important to keep in mind that correlations measure linear association between two variables but association is not the same as causation.\nLet me come back to an example we discussed earlier in section 3, where we looked at the correlation between average learning outcomes and total education expenditure by per capita documented in the study by Larry Hedges (1994)\n\n\nCode\nlibrary(JWL)\nplot(expenditure_outcome_dat$Expenditure, \n     expenditure_outcome_dat$Outcome, \n     main = (\"Average learning outcomes by total education expenditure per capita\"), \n     xlab = (\"Public and private per capita expenditure on education (PPP, constant 2011-intl $)\"), \n     ylab = (\"Average harmonized learning outcome in 2005 - 2015\"), pch = 16)\n\naxis(1, at = seq(500, 3500, by = 500))\n\nmod &lt;- lm(Outcome ~ Expenditure, data = expenditure_outcome_dat)\n\nabline(mod, col = 4, lw = 2)\n\n\n\n\n\nThe correlation in these data is 0.5684246. But does this evidence does not support the conclusion that there is a causal relation between increased educational spending and student outcomes. Comparing outcomes of students across countries or students across schools with different levels of spending does not tell us whether the different spending levels are causal for the variation in outcomes we observe. There may be many other differences between countries and students that are shaping this relationship.\n\n\n7.1.6 Ecological correlations\nThe previous example shows a case of correlations based on averages. We have seen by now various examples of such correlations. Correlations based on averages can be frequently found in social sciences such as sociology, political science or economics. However these correlations have to be assessed with care because they can overstate the strength of an association.\nLet me illustrate this point by a hypothetical example. Many studies have looked at the association between individual education and individual income, claiming a positive association between the level of education and individual income. Suppose a survey had collected data on income and education for individuals in three countries, labelled by three colors: Country red, country orange and country blue. Each individual is marked by one color symbolizing it’s country of residence. Let’s do not worry about the exact units. Say education is measured by some form of time of education and income in some monetary unit.\n\n\nCode\nset.seed(12) # set the random number generator seed\n\n  x &lt;- 0:24\n  y &lt;- 1153*x + rnorm(25, 20000, 8000) # create the toy data of duration of edcuation and income\n  \n  df &lt;- data.frame(Education = x, Income = y) # write into dataframe\n  \n  \n  df$State &lt;-  with(df, ave(seq_along(Income), Income, FUN = function(x)\n    sample(c(\"Red\", \"Blue\", \"Orange\"), size = length(x), replace = TRUE, prob = c(1/3, 1/3, 1/3)))) |&gt;    as.factor() # asign colors as factors to the education and income combination\n\n  color_pallete_function &lt;- colorRampPalette(\n  colors = c(\"red\", \"orange\", \"blue\"),\n  space = \"Lab\" # Option used when colors do not represent a quantitative scale\n  ) # define a state by a color\n  \n  num_colors &lt;- nlevels(df$State)\n  df_state_state &lt;- color_pallete_function(num_colors)\n  \n# Do the exercise for state averages\n  \n  col1 &lt;- tapply(df$Education, df$State, mean)\n  col2 &lt;- tapply(df$Income, df$State, mean)\n\ndf_avg &lt;- data.frame(Education = col1, Income = col2, State = as.factor(names(col1)))\n\nnum_colors_avg &lt;- nlevels(df_avg$State)\ndf_state_state_avg &lt;- color_pallete_function(num_colors_avg)\n\n# plot side by side\n\npar(mfrow = c(1,2))\n\n# plot for individual values\n  \n  plot(\n  x = df$Education,\n  y = df$Income,\n  xlab = \"Individual education\",\n  ylab = \"Individual income\",\n  pch = 20, # solid dots increase the readability of this data plot\n  col = df_state_state[df$State],\n  xaxt = \"n\",\n  yaxt = \"n\")\n  \n# plot for aggregated values\n\n plot(\n  x = df_avg$Education,\n  y = df_avg$Income,\n  xlab = \"Country average education\",\n  ylab = \"Country average income\",\n  pch = 20, # solid dots increase the readability of this data plot\n  col = df_state_state_avg[df_avg$State],\n  xaxt = \"n\",\n  yaxt = \"n\"\n)\n\n\n\n\n\nIn the averaged data the correlation increases from 0.848 to 0.996 or by about 17.5 %. So bear in mind that, while correlations based on averages are frequently encountered in the press, in reports and in the research literature, they tend to overstate the strength of an association."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_2.html#the-regression-line",
    "href": "modelling_relationships_using_regression_part_2.html#the-regression-line",
    "title": "7  Modelling relationship: Prediction using regression",
    "section": "7.2 The regression line",
    "text": "7.2 The regression line\nThe correlation coefficient does not only measure the strength of linear association along a line, it also helps us identifying this line. In order to help you discover this, let us go back to the Pearson data on the height of fathers and sons, we used in the beginning of this chapter.\n\n\nCode\nlibrary(JWL)\ndat &lt;- pearson\n\nplot(dat, \n     pch = 19, \n     cex = 0.3, \n     xlab = \"Father's height (inches)\", \n     ylab = \"Son's height (inches)\",\n     main = \"Pearson's height data from 1903\")\n\nabline(0, 1.036765, lty = 2, col = 4, lwd = 2)\nabline(v = 67.69 + 2.75 - 0.5, lty = 2)\nabline(v = 67.69 + 2.75 + 0.5, lty = 2)\n\npoints(70.44, 70.12, pch = 19, col = \"red\")\npoints(73.19, 71.56, pch = 19, col = \"red\")\npoints(62.19, 65.8, pch = 19, col = \"red\")\n\nmod &lt;- lm(Son ~ Father, data = dat)\nabline(mod, col = \"red\", lwd = 2)\n\n\n\n\n\nThe summary statistics of the data are:\n\n\nCode\ndata_father_son &lt;- data.frame(Father = round(c(mean(dat$Father), sd(dat$Father)),2), \n                              Son = round(c(mean(dat$Son), sd(dat$Son)),2), row.names = c(\"Mean\", \"Standard Deviation\"))\nknitr::kable(data_father_son)\n\n\n\n\n\n\nFather\nSon\n\n\n\n\nMean\n67.69\n68.68\n\n\nStandard Deviation\n2.75\n2.82\n\n\n\n\n\nThe correlation coefficient is \\(r\\) = 0.501.\nThe dashed blue line in the picture is the line starting at 0 and increasing with the slope given by the standard-deviation of the father’s height - 2.82 - divided by the standard deviation of the son’s height - 2.72. This is a slope of approximately 1. The vertical strip with the dashed black lines show the fathers who were one standard deviation above average - 70.44 to the nearest inch, i.e. plus minus 0.5 inches. You can see that most of the points in this vertical strip are below the dashed red line. This means that most of the son’s whose father’s were 1 standard deviation above average were less than a standard deviation above average. The average height of the sons is only a fraction of a standard deviation above the average height. This is where the correlation coefficient \\(r\\) = 0.51 comes in. Associated with an increase of one standard deviation in height by the father is and increase of only 0.51 standard deviations in height of the son on average.\nTake the father whose height is one standard deviation above average: 67.69 + 2.75 = 70.44 in. The average height of their sons will be above average by 0.51 standard deviations of their height, or \\(0.51 \\times 2.82\\), which is \\(1.44\\). So the average height of a son is \\(68.68 + 1.44\\) The point \\((70.44, 70.12)\\) is marked by a red dot in the figure.\nNow let’s look at the father’s who are two standard deviations above the average height: \\(67.69 + 2 \\times 2.75\\) which is \\(73.19\\). The average height of their sons should be above \\(2 \\times 0.51\\) of their height, or \\(1.02\\) standard deviations of height. That is \\(1.02 \\times 2.82\\) or \\(2.88\\). That is their average height is around \\(68.68 + 2.88\\) or \\(71.56\\). We mark the point \\((73.19, 71.56)\\) also as a red dot in our picture.\nWhat about those who are 2 standard deviations below the average? Their height is \\(67.69 - 2 \\times 2.75\\), which is \\(62.19\\). The average height of the sons is then about \\(68.68 - 2.88\\) or \\(65.8\\). We also mark the point \\((62.19, 65.8)\\) by a red dot.\nNow all points that combine the father’s height with the estimate of the average height of the son fall on the solid red line which connects the red dots. This line is called the regression line. The line goes through the points of averages. Fathers with average heights should have sons with average height.\n\n\n\n\n\n\nRegression line\n\n\n\nThe regression line for \\(y\\) on \\(x\\) estimates the average value for \\(y\\) corresponding to each value of \\(x\\).\n\n\nThe method with which we have estimated this line is called the regression method. It associated in our example with each increase in one standard deviation in height by the father an increase of \\(0.51\\) standard deviations of the son. More generally:\n\n\n\n\n\n\nRegression method\n\n\n\nAssociated with each increase of one standard deviation in \\(x\\) there is an increase of only \\(r\\) standard deviations in \\(y\\) on average.\n\n\nFrom our picture and the discussion we can also learn how the regression line can be described mathematically:\nWhen the variables \\(x\\) and \\(y\\) are measured in standard units, the regression line for predicting \\(y\\) based on \\(x\\) has slope \\(r\\) and passes through the origin. Thus the regression equation can be written as \\[\\begin{equation*}\n\\text{estimate of} \\, y = r x \\, \\text{when both variables are measured in standard units}\n\\end{equation*}\\]\nWhen we write this equation not in standard units but in the original units of the data, this equation becomes \\[\\begin{equation*}\n\\frac{\\text{estimate of} \\, y - \\text{average of } \\,y}{\\text{sd of} \\,y} = r \\,\\, \\frac{\\text{given} \\, x - \\text{average} \\, x}{\\text{sd of} \\, x}\n\\end{equation*}\\]\nThis means, we have:\n\n\n\n\n\n\nSlope and Intercept of the regression line\n\n\n\n\nThe slope of the regression line is: \\(r \\,\\, \\frac{\\text{sd}\\, y}{\\text{sd}\\, x}\\)\nThe intercept of the regression line is: \\(\\text{average of}\\, y - \\text{slope} \\times \\text{average of}\\, x\\)\n\n\n\n\n7.2.1 The method of least squares\nFinding a regression line is a standard problem often encountered in statistics either to make predictions, when you run controlled experiments or to describe the data you see for a given population.\nIn the previous section we have explained how a regression line for a scatter plot can be found. In our discussion we followed the path how the regression line was historically detected by the pioneers of statistics.\nIn current practice the regression line is usually determined in another way. When the points in a scatter plot seem to follow a straight line, we could ask the question which line fits the points best.\nLet’s go back to the graph discussed in the very beginning:\n\n\nCode\nlibrary(JWL)\ndat &lt;- pearson\n\nplot(dat, \n     pch = 19, \n     cex = 0.3, \n     xlab = \"Father's height (inches)\", \n     ylab = \"Son's height (inches)\",\n     main = \"Pearson's height data from 1903\")\nabline(0,1, lty = 2)\nabline(reg = lm(dat$Son ~ dat$Father), col = \"red\", lwd = 2)\npoints(63.1, 74.3, pch = 19, col = \"red\")\nlines(c(63.1, 63.1), c(66.3264, 74.3), col = \"red\", lty = 2)\npoints(71.1, 66.3, pch = 19, col = \"red\")\nlines(c(71.1,71.1), c(66.3, 70.4384), col = \"red\", lty = 2)\n\n\n\n\n\nAny straight line drawn through this point cloud will be near to some points and farther from others. One such distance from the line to a particular point is marked in the graph by a red dot. When we move the red line around, we will get closer with the line to some points and farther from others.\nThis conflict can be resolved in two steps: First define an average distance of all the points to the line. Second, move the line around until this average distance is as small as possible.\nWhen \\(x\\) is used to predict \\(y\\) the error at each point is the vertical distance of the point to the line, like in our graph. Now, some distances will be positive, some will be negative, so to minimize the average distance, you can not just add the distances because negative and positive values will cancel each other. Instead statisticians take the square of these distances, because the square will always be positive, no matter, whether the distance has a negative or positive value. This square of the distances is also known as the root mean square error.\n\n\n\n\n\n\nRegression line by least squares\n\n\n\nAmong all lines, the line that minimizes the root mean square error in predicting \\(y\\) from \\(x\\) is the regression line. Moreover the regression line is the only line that minimizes the root mean square error.\n\n\n\n\n7.2.2 Finding the regression line with R\n\n7.2.2.1 The lm() function\nRegression is such an important topic and basic tool in statistics, that every software package has built in functions, to help you compute the regression line. In R this function is called lm() for linear model. As usual with functions, lm() takes arguments.\nThe first argument is of a type we did not encounter before. It is called in R a formula and has the following syntax. Suppose we want to fit a linear model \\[\\begin{equation}\ny = k x + d\n\\end{equation}\\] to our height data. \\(y\\) is the vector of the father’s heights and \\(x\\) is the vector of the son’s heights. The intercept of this line with the \\(y\\) axis is \\(d\\). To tell R that this is our formula we write name of y ~ name of x.\nLet me explain. Here in our discussion we stored the data in a dataframe which we called dat. This dataframe has two variables, which we called Father and Son. Now the formula we need to give to lm needs to be called in this case Son ~ Father.\nWhere is the constant \\(d\\) in this formula? lm() automatically assumes that there is an intercept term. This has not to be specified in particular when passing the formula to lm(). The second argument you need to give to lm() is the data. This is called in our case dat.\nSo now we have the syntax:\n\nlm(Son ~ Father, data = dat)\n\n\nCall:\nlm(formula = Son ~ Father, data = dat)\n\nCoefficients:\n(Intercept)       Father  \n     33.893        0.514  \n\n\nThe output shows the value of the intercept \\(d = 33.893\\) and the slope \\(k = 0.514\\). Let’s briefly check how this fits together with your discussion of the regression line. The correlation, the standard deviation of the son’s height and the standard deviation of the father’s height determine the slope \\(k\\)\n\nr &lt;- cor(dat$Father, dat$Son)\nSDF &lt;- sd(dat$Father)\nSDS &lt;- sd(dat$Son)\n\nr*SDS/SDF\n\n[1] 0.5140059\n\n\nwhich is indeed the slope of the regression line.\nFor the intercept we derived:\n\nmean(dat$Son) - r*mean(dat$Father)\n\n[1] 34.76212\n\n\nwhich is the (almost) the same as the value of the intercept computed by lm().\n\n\n\n7.2.3 Plotting the regression line\nAssume we would like to plot the regression line together with a scatter-plot of the data. We would first store the output of lm() in an object, let us call this object mod for model\n\nmod &lt;- lm(Son ~ Father, data = dat)\n\nThen plotting the scatter plot together with the regression line can be done as follows:\n\nplot(dat$Father, dat$Son, \n     pch = 19, \n     cex = 0.3, \n     xlab = \"Father's height (inches)\", \n     ylab = \"Son's height (inches)\",\n     main = \"Pearson's height data from 1903\") # Syntax for the scatter plot \n\nabline(mod, col = \"red\", lwd = 2) # syntax for the regression line\n\n\n\n\nWe first draw the scatter plot using the R plot()function. The arguments pch and cex control the type and size of the dots and xlab, ylab and main control the lables of the x-axis, the y-axis and the main plot.\nThen we give the output of lm() the function abline() and tell abline()that the color of the line should be red and the line width should be 2. That’s it.\n\n\n7.2.4 Using the predict() function\nFinally, let us briefly show you an R function which helps you to check the predictions made by the linear model of the regression line using the built in function predict().\nNote that our regression line was computed as \\[\\begin{equation}\n\\text{Son's height} = 0.51 \\times \\text{Father's height} + 33.89\n\\end{equation}\\] Now assume you want to make predictions for the height of the son based on the height of the father using this linear model. In this case this is easy to compute by hand but for many values (and even for few) this step can be automated using the predict()function of R. predict()needs as arguments the output of lm() plus a dataframe of new data.\nWe have already stored the output of lm()in our object mod. Now define some new data and then give the model and the new data to the predict()function:\n\nnewdata &lt;- data.frame(Father = c(60, 70, 80))\n\npredict(mod, newdata)\n\n       1        2        3 \n64.73316 69.87321 75.01327 \n\n\nSo what predict does for you in one step is to compute for example for the value 80\n\n0.514 *80 + 33.893\n\n[1] 75.013"
  },
  {
    "objectID": "categorical_data_and_proportions.html#exercises",
    "href": "categorical_data_and_proportions.html#exercises",
    "title": "2  Categorical Data and Proportions",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\nA note on exercises. Exercises will help you to actively remember the concepts you, learn, they will help you to practice applying these concepts to problems and develop your competence and skills. The exercises will be divided into two categories.\nExercises will ask questions related to the study material. They will help you remembering the concepts learned. They will also help you to test and develop your understanding by applying the concepts to various situations, some similar but slightly different than in the text. This will help you develop your skills, aqcuring routine through practice and reflect in your understanding.\nExercises R will be exercises for which you will need the computer and access to your digital tools like R and the Jupyter notebook. These exercises will be very important. They support you in developing your computer skills and gain practice in modern data analysis. Data analysis today is always done using a computer. You should consider these as an integral part of your general skills in statistics.\nExercises Project will be exercises related to a long term project which requires from you a continuous development done unit by unit and cumulatively drawing on the skills you learned during the course. It will also need the computer and your use of R and the Jupyter notebook. The project we are going to work on during this course is called: “People count: The future of humanity in pictures and numbers”. In this project we will apply what we learned in this data to the application of statistical techniques to understand data about demography. Demography is the study of the characteristics of human populations, such as size, growth, density, distribution, and vital statistics.\nThe exercises will also be graded and solving them is an essential part of documenting your learning process and determining whether you will successfully finish this course.\n\n2.6.1 Exercises\n\n\n\n\n\n\nExercise 1: Refresh your knowledge of computing with percentages\n\n\n\nCompute and report the reduction in the share of infant mortality for each of the 8 countries between 1860 and 2020 based on the numbers reported in Table 2.1 and Table 2.2. You can do this with pencil and paper. You could do it also using the computer. When you report your result use a Jupyter-Notebook\n\n\n\n\n\n\n\n\nExercise 2: Framing\n\n\n\nIn the text we gave as an example of framing the reporting of infant mortality by the percentage of infant-deaths and equivalently as infant-survivals. Can you think of another example of framing i.e. an example where you would report the same information in two different ways ?\n\n\n\n\n\n\n\n\nExercise 3: Communicating counts and proportions\n\n\n\nDavid Spiegelhalter (Spiegelhalter 2019) whose book we have encountered when we discussed the question of how many trees there are on the planet, describes an advertisement in the London Underground, saying that 99 % of young Londoners do not commit serious crimes. The add was presumably intended to reassure passengers that riding on the London Underground is very safe. Try to imagine what this statement would mean when you think about this information in terms of crowds of young Londoners, assuming that “young” means between 15 and 25. Try to exlpain how the same information presented differently may have a different impact. What kind of communication tools we have just heard about, have been applied here?\n\n\n\n\n\n\n\n\nExercise 4: Absolute and relative risk when comparing proportions\n\n\n\nGo back to the first example in the section comparing pairs of proportions. Assume the absolute risk is 4 out of 10 and the relative risk increase is 50 %. How many out of 10 are now at risk?\n\n\n\n\n2.6.2 Exercises R\n\n\n\n\n\n\nExercise 1: Transforming percentages into rates using R\n\n\n\nUse R to transform the mortality numbers reported in percent in Table 2.1 and Table 2.2 in mortality rates, i.e. the (approximate) number of infant-deaths per 1000 life births.\n\n\n\n\n\n\n\n\nExercise 2: Your first barplot\n\n\n\nUse R to redo the barplot visualization we just did for the 1860 data for the 2020 data.\n\n\n\n\n\n\n\n\nExercise 3: A bar chart on the causes of infant mortality\n\n\n\nUse R to visualize the table of causes of infant mortality Table 2.4 in a barchart. Don’t worry if some of the causes are cut off at the left of the graph. We will learn during the course how to better control the appearance of a visualization and control for details like this.\n\n\n\n\n2.6.3 Project People Count. Unit 1\nIn this course we will follow a larger project which we will call People count and their data stories. You will develop this project unit by unit adding new material as we go along. At each step you are going to apply concepts you learned in the unit. When you add the individual steps you will at the end have a data driven story of humanity in a few very interesting numbers and charts. It will help you understand some key aspect about humanity and its past and destination.\nIn this section we begin with counting people. We will use a variant of our first visualization tool we have learned in this lesson, the bar chart. In this first part we are applying what we have learned in this unit by counting people in Kenya. Like in all countries in the world, national statistical offices describe people living in a country by age, sex, race, education, and a number of other factors. Are these descriptions important?\nIn particular in this project we ask whether the description of a country by age groups does help us better understand a country? We are going to study the “shape” of Keynia based on the counts of people living in the country by age.\nAs in real life national statistics we are going to produce data visualizations that display the data by way of barplots and other summaries. These descriptions begin to unpack stories about people living in this country. Behind these numbers and graphs are real people, their sisters and brothers and parents and nieces and nephews. The stories revealed in these data help us better understand a particular country.\nThe following table shows the counts of women and men in Kenya in the year 2022. The data are made available through an international demographic database, which compiles the local information from national statistical authorities and provide the data publicly via a website hosted by the Census Bureau of the United States.1111 The website is here https://www.census.gov/programs-surveys/international-programs/about/idb.html and I have retrieved the data from this site.\n\n\n\nAge\nNumber of Women\nNumber of Men\n\n\n\n\n0-4\n3487490\n3531278\n\n\n5-9\n3404421\n3431166\n\n\n10-14\n3444606\n3473103\n\n\n15-19\n3225971\n3249738\n\n\n20-24\n2656730\n2664966\n\n\n30-34\n1946994\n1926219\n\n\n40-44\n1606763\n1610246\n\n\n45-49\n1170869\n1196519\n\n\n50-54\n858998\n890662\n\n\n55-59\n672025\n679943\n\n\n60-64\n516769\n488074\n\n\n65-69\n387773\n340546\n\n\n70-74\n272320\n229912\n\n\n75-79\n168499\n138357\n\n\n80-84\n93724\n73093\n\n\n85-89\n39226\n28473\n\n\n90-94\n10461\n6918\n\n\n95-99\n1520\n902\n\n\n\n\nProduce a barplot of the total population with R. Also produce a separate barplot for men and women.\nDemographers usually use the following terminology: A population distribution is defined by the following layers: The bottom-layer in terms of age group refers to the counts of people in the 0 to 24 years old age groups. The lower middle-layer refers to the counts of people in the 25 to 49 years old age groups. The upper middle-layer refers to the count of people in the 50 to 74 years old age groups. The top layer refers to the count of people in the 75 to 100+ years old age groups. Depending on which layer is the largest a country’s “shape” is defined by demographers as a bottom-layered a middle layered and a top layered country. Use R to do a horizontal box-plot where the number of people in each age group are shown in percent. For computing the percentages you will need the total number of people over all age groups. Fortunately R has a function for this. It is called sum() and is applied as follows: Assume you have a vector x  &lt;- c(1,4,5), then using the sum function sum(x) you will get 10 in this example.\nIdentify two age groups in which the number of people in one age group is approximately double the count in the other age group.\n“Old” and “young” are subjective descriptions that in many cases are defined by several factors other than age (for example, health status, or income status). For this unit, however, consider the definition of “young” as people less than 10 years old, and the definition of “old” as people who are 65 years old or older. What is the ratio of “old” to “young” using the above definitions of young and old?\nIf there are approximately 500 students in a typical school for students who are 5 to 14 years old, estimate the number of schools needed to educate the students who are 5 to 14 years old.\nHere is an R challenge that you can do, if you have enough time left and if you love challenges. It will not This will not be graded. It is an option to give this challenge a try. You can also come back to it later, when you have more experience with R. When you have no time, just leave it out.\n\nDemographers, have a very nice visualization of population counts by 5 year age groups and by sex, which are called “population pyramids”. These graphs are a very nice example of the power of data visualization, because they convey at one glance whether a particular country is a bottom, middle layered or a top layered country and how the relative cohort sizes of men and women look like for each age cohort.\nIt looks approximately like this:\n\n\n\nExample of a population pyramid\n\n\nCould you construct a similar graph for the Kenya data using the barplot function and R’s graphics options? You might consult the help function, or the internet when you have internet access.\n\n\n\n\nAntony, Volk, and Atkinson Jeremy. 2013. “Infant and Child Death in the Human Environment of Evolutionary Adaptation.” Evolution and Human Behavior 34: 182–92.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for Communication. New Riders.\n\n\nRoser, Max. 2019. “Child Mortality Is an Everyday Tragedy of Enormous Scale That Rarely Makes the Headlines.” https://ourworldindata.org/child-mortality-everyday-tragedy-no-headlines.\n\n\nSmil, Vaclav. 2020. Numbers Don’t Lie: 71 Things You Need to Know about the World. Penguin Books.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "categorical_data_and_proportions.html#using-r-via-the-jupyter-notebook",
    "href": "categorical_data_and_proportions.html#using-r-via-the-jupyter-notebook",
    "title": "2  Categorical Data and Proportions",
    "section": "2.3 Using R via the Jupyter Notebook",
    "text": "2.3 Using R via the Jupyter Notebook\nIn these first steps we have used R via the console. In this course we will use another powerful tool to use R: The Jupyter-Notebook. The notebook is a file format that allows you to store both text and code within one file. It allows you also to produce documents and store them. This has a few big advantages over using R just via the console:\n\nIt allows you to store your work and to come back to it later.\nIt also allows you to show your work to a learning facilitator to get feedback or grades as often needed in this course.\nIt finally allows you to verbally comment your code, so you later - when you come back to it - remember what you have done.\nIt is a document format that gives you an elegant and powerful tool to report on your data work in the form of a readable and professionally layed out document.\n\nLet us show, how the Jupyter-Notebook works. You start the Notebook by double clicking the Jupyter icon in your docker.\n\n\n\n\n\n\nComment for Seitwerk: Please uncollapse\n\n\n\n\n\nHere you should demonstrate in a vido how to launch the Jupyter-Notebook in the docker version you have installed for the course.\n\n\n\nUpon start Jupyter will show you your directory structure and a navigation pane. On my screen it looks like this:  Now you need to navigate to the folder where you would like to store your notebooks. This will be a folder of your choice with a name you have given it. If the directory has been created you can navigate there from the Jupyter menue by just double clicking on the directory names. For instance, in my case the files are stores in my R directory. So I double click R and will then see the subdirectories of R. Then I click on my folder Statistics_JWL because this is where I store my files for this course and in this directory I click notebooks because this is the folder for my Jupyter notebooks. In your case this will be different depending on how you chose to organise your directories.\nAfter navigating to my folder my screen will look like this:\n Now in the gry bar you see the directory you are in. In this directory you can create a new notebook-file by selecting the button new in the upper right corner of the screen. When I do this I get a menu of options, like this  In my case I can choose from three options. These options refer to different computing engines, which will do code execution from the notebook. The options might look a bit different in your installation but what is important is that you should have an option R in your pull down menu: Please choose this option.\nWhen you do that you have created a new notebook which will be able to use R as its computing engine and be able to send R commands from the notebook to R as well as receive R output in the notebook. This is the way we will use the Jupyter-Notebook in this course. You should now see a screen like this:\n You see now the Ricon in the upper right corner, you see a menue bar on the top. You also see that the notebook has yet no name and is called Untitled1. Let’s give this notebook a name. In order to do so, you click the File menu, select Save as and then give the file a name: I called mine testbook.\nTo work with the notebooks for now, you have just to understand very few things beyond how to open Juyter and create a new notebook. Let me explain these few concepts here and then you are ready to go.\n\n2.3.1 Cells\nJupyter-Notebooks are file formats that can combine code and text and represent these different forms of input in a single documents. Whether something you type is understood by the notebook as a text or as code depends on the type of so called cells.\nFor example in the new file we created we see a line which is boxed with a color bar on the left. Then its has In []: followed by a kind of ply symbol and then a light gray box, like this:\n This is a cell type that can take inputs of R code. You can see this, by looking at the line below the menu bar which says Code. Now let us give the cell some R input like for example the vector of mortality rates in 1860.\n Now you can execute the cell by typing SHIFT and ENTER. This executes the cell. This means that the code is sent to R and processed there as if you had typed the same command on the console. Note that something has also changed with the cell. The Input[]: has now be given a number by the notebook. This is the first input the notebook has received in this session and it is recorded. At the same time R has opened a new input cell for code.\n Say now that you want to inspect the object mr_1860 in which we have stored the data you just would type this name into the cell and press SHIFT ENTER again. The resulting screen would then look like this:\n Now say you want to write a comment as text. Then you first need to change the type of the cell. To do so go to the pull down menu that says Code and choose Markdown. Now the cell is a textcell which accepts Text and text formatting commands in a markup language called Markdown. For the moment we do not need to know any details about this. The screen will look now as follows\n Now write a text comment into this cell, for example: “These are the percentages of infant mortality in 1860 in a group of European countries” and then press SHIFT ENTER. You will now see a screen like this:\n Now you can safe the file by just clicking the save symbol or choosing the menu File and choose Save and Checkpoint. Then choose in the File menue Close and Halt to close your notebook. You can now open it any time again by double clicking the file in your directory. The Jupyter Notebooks have the file extension ipynb. The file can be uploaded to a platform, attached to an e-mail etc. as with any other file. Anybody who has the Jupyter Notebook software can open the file and read it. If the right software installation is in place the cells can be also correctly executed.\nFrom now on let us do our R stuff with Jupyter Notebooks to make the R work readable to others as well as reproduceable.\n\n\n2.3.2 A first data visualization\nWe showed the infant mortality rates of a group of European countries before in Table 2.1 and Table 2.2.\nHumans are very visual creatures. Thus using our visual system to explore data and absorb information in these data visualizations can be very powerful. To deploy their power, we must - however - follow some principles, which we will learn step by step over this course.\nAssume we would like to display the information in our tables in a so called bar chart. A bar chart would combine in a plot bars for each country in the table with the bar length proportional to the mortality rate. This will give us a visual impression how the countries differ in one view, which might be more informative as just looking at the numbers themselves in a table.\n\n2.3.2.1 Functions\nR is not just a calculator and data storage device. What makes R very powerful is that it comes equipped with many functions which we can use to do things with data, like for instance producing plots like a bar chart.\nFunctions in R have a name followed by parenthesis. In the very first step we typed for example quit() at the prompt. This is a function and by typing its name followed by the parenthesis, R knows that it has to close the program and shut down.\nFunctions can also have arguments, which we can assign certain values to. For example, R has a function which would round numbers. This function is called round(). It has also arguments. You need to tell R which numbers to round and the number of digits the rounding should consider.\n\nround( x = 2.4356789123456, digits = 2)\n\n[1] 2.44\n\n\nThe first argument in the function round is x. We can give x a value, which we assign by =. The second argument is called digits and we assign to it the value 2. The output is then, not very surprisingly, 2.44.\nNote that R is programmed such that we could also have typed:\n\nround(2.4356789123456, 2)\n\n[1] 2.44\n\n\nR would have known automatically that the first value is assigned to xand the second to digits.\nWe will encounter a lot of R functions during this course. We will also learn how to access R documentation to know for so many different functions, what is their name, which arguments they accept as input and how we can use them.\n\n\n2.3.2.2 Visualizing the infant mortality data\nR has a built in function for plotting bar charts, which is called barplot(). Let is make use of this function to show the infant mortality rates of 1860 as a bar chart. The arguments taken by R are the data. Then we can add additional arguments which determine details of the plot display and appearance.\nLet us first store the data in an object with the name mrfor mortality rates of 1860.\n\nmr_1860 &lt;- c(0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)\n\nHere we see another important function of R which we will need all of the time, the c()function. This function concatenates values in a vector of values. So the output of the operation will be a vector \\((0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)\\) with the name mr_1860. Thus when we type\n\nmr_1860\n\n[1] 0.237 0.139 0.136 0.150 0.260 0.102 0.174 0.124\n\n\nR will print the whole vector as one object. This is why the counting label is [1] and not [8]. The c() function concatenates the number so a single object, a vector containing all eight numbers.99 In case you are worried about the technical term of a vector, don’t worry. We use this term here losely and not in a rigorous mathematical sense. You can think for the moment of a vector in R as a single object that can hold several data at once, like numbers, or characters. I avoided the term list in the text to avoid confusion with the list data structure which is a special data structure in R which we learn about in the course later.\nNow lets see what happens when we give mr_1860 as an argument to barplot.\n\nbarplot(mr_1860)\n\n\n\n\nWe see on the y axis the infant mortality rates from 0 to 0.25 and on the x axis a bar for each country with a length proportional to the infant mortality rate in this country.\nBut here it is difficult to connect the bars to the countries. So let us store the country names in another vector and call them ctr\n\nctr &lt;- c(\"Austria\", \"Belgium\", \"Denmark\", \"France\", \n         \"Germany\", \"Norway\", \"Spain\", \"Sweden\")\n\nNote that the names of the countries had to be written between quotation marks \" \". This is the way to tell R that the sequence of letters are characters. Characters are a specific data type representing text. Now we have a vector of words, the country names. We can give the country names as an argument to barplot()like this:\n\nbarplot(mr_1860, names.arg = ctr)\n\n\n\n\nNot too bad.\nIt would perhaps be more convenient to flip the chart around and interchange the x and the y axes here. This can be done by another argument to barplot(). This argument is called horiz and it assumes a logical value. A logical is another R data type which allows us to express whether something is true or false. Logical true and false values are expressed as TRUE and FALSE in R.\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE)\n\n\n\n\nThis does not yet help much, because this flip of coordinates can only support a better display of the data if the country names are also printed horizontally.\nYou might guess it already: This can be controlled by another argument which is called in the case of this function las. If las gets value 1 we get what we want.\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE, las = 1)\n\n\n\n\nNow we have visualized the information we had displayed in a table before. It is not yet perfect because the names of countries with longer names are cut off a bit. This could be fixed by additional function arguments, but let us not go too much in the details of the barplot()function at this stage. We will learn a lot about powerful visualization techniques in R as we go along.\nBefore we close this first encounter with R and data visualization, let me point out an important aspect of bar charts. The visual impression is powerful and truthful, if we choose the origin of the bars carefully. It is usually the best idea to start the bars at zero. So we see clearly the relative lengths and the magnitude of differences in the context of the entire dataset.\nChanging the origin with not enough care can visually exaggerate the differences between countries. This is a manipulative visualization, which should be avoided but which is encountered often. So be mindful about the choice of origin in a bar chart.\nLet me show you what I mean by telling R, for example, to start the plot of the data for the 1880 data at \\(0.08\\) instead as of \\(0\\).\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE, las = 1, \n        xlim = c(0.08, 0.275), xpd = F)\n\n\n\n\nDo you see that now the differences appear bigger? The choice of origin can have a big influence on the appearance of differences between the length of the bars. Always be mindful of this effect and reflect what happens if for some reason you have to choose a different origin for the bar chart than zero. Alberto Cairo, who is the author of an influential book on data visualization (Cairo 2016) recommends to always choose a “…logical and meaningful baseline”.\n\n\n\n\n\n\nBe mindful in choosing a logical and meaningful origin for barcharts\n\n\n\nWhen you compare porportions visually with a bar chart always think of choosing a logical and meaningful origin. In most cases this will be 0. If 0 is not possible, think about what would be a choice that gives a truthfull and not exaggerated display of differences.\n\n\nBefore we close this digression into visualization, let me briefly discuss another aspect of data presentation which you need to consider. The ordering of the rows in the data table, or in this case, the order of the bars in the barchart has to be carefully considered.\nIf you look at the bar chart we have produced, you see that the bars have an order corresponding to the alphabetical order of the countries, starting with A for Austria and ending with S for Sweden.\nNow consider we had ordered the data according to mortality rate like this:\n\n\nCode\nbarplot(rate_1860[order(-rate_1860$Mortality) , ]$Mortality, \n        names.arg = rate_1860[order(-rate_1860$Mortality) , ]$Country, horiz = TRUE, las = 1)\n\n\n\n\n\nNow the bar chart could suggests that the infant mortality rate is an important and meaningful way of comparing this particular group of countries.\nSuch ranking comparisons are very popular in the media but they can be misleading. They can be misleading because the differences could be there just by chance.\nThere could be also systematic differences between countries affecting infant mortality rates. For example, countries that are small with populations under 10 Million and that have very homogeneous populations and low birth rates tend to show lower infant mortality rates just because of these demographic features. So an ordering like the one presented in the graph might suggest a ranking that is in fact spurious and is not really substantial.\n\n\n\n\n\n\nBe mindful about order in displaying the data\n\n\n\nWhen you display proportions in a table or a bar chart be mindful of the ordering of data and avoid spurious rankings. Choose a particular ordering only if there is a meaningful and logical reason to do so."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#exercise-15-a-histogram-of-the-nile-river-flow-data",
    "href": "summarizing_and_communicating_lots_of_data.html#exercise-15-a-histogram-of-the-nile-river-flow-data",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.6 Exercise 15: A histogram of the Nile river flow data",
    "text": "3.6 Exercise 15: A histogram of the Nile river flow data\nBase R comes bundled with certain datasets which are available to R when you start it. One of these data are flow measurements of the river Nile at Aswan in Egypt. This dataset is called Nile.\n\nUse the R help function to learn more about these data. Describe in words the meaning of the datapoints.\nPlot a histogram of the Nile river flow data.\nChange the Main title of the histogram to “Histogram of Nile river flow data at Aswan Egypt” and change the x axes label to “Annual flow is recorded are 100 millions of cubic meters”. Hint: If you don’t know how to go about this either use ?hist - the R help function - to find out. Alternatively you might try a search engine like google of bing and ask something like “changing title and labels in base R histogram”.\n\n\n\n\n\n\n\nExercise 16: Analyze the full height dataset\n\n\n\nLoad the JWLlibrary of datasets and store the variable socr_height_weight in a variable called dat. Use R’s help function to learn what the data are.\n\nThe data show heights in inches and weights in pounds. Transform both variables to metric data before you continue. Note that 1 inch is 2.54 cm and 1 pound is 0.4535924 kg.\nPlot a histogram of the heights in cm. Plot a histogram of the weight in cm.\nCompute the mean and the standard deviation of both height and weight.\nCompute the percentage of observations between plus 1 and minus 1 standard deviations from the mean.\nCompute the percentage of observations between plus and minus 2 standard deviations from the mean. Do the same with three standard deviations.\n\n\n\n\n3.6.1 Exercise Project:\nTo be added."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#exercise-16-analyze-the-full-height-dataset",
    "href": "summarizing_and_communicating_lots_of_data.html#exercise-16-analyze-the-full-height-dataset",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.7 Exercise 16: Analyze the full height dataset",
    "text": "3.7 Exercise 16: Analyze the full height dataset\nLoad the JWLlibrary of datasets and store the variable socr_height_weight in a variable called dat. Use R’s help function to learn what the data are.\n\nThe data show heights in inches and weights in pounds. Transform both variables to metric data before you continue. Note that 1 inch is 2.54 cm and 1 pound is 0.4535924 kg.\nPlot a histogram of the heights in cm. Plot a histogram of the weight in cm.\nCompute the mean and the standard deviation of both height and weight.\nCompute the percentage of observations between plus 1 and minus 1 standard deviations from the mean.\nCompute the percentage of observations between plus and minus 2 standard deviations from the mean. Do the same with three standard deviations.\n\n\n3.7.1 Exercise Project:\nTo be added."
  },
  {
    "objectID": "from_limited_data_to_populations.html#exercises-r",
    "href": "from_limited_data_to_populations.html#exercises-r",
    "title": "4  From limited data to populations",
    "section": "4.9 Exercises R",
    "text": "4.9 Exercises R\n\n\n\n\n\n\nSelect every 100th row from the height data\n\n\n\nYou can try the code we used in the text to select every 50iest row in the heights data yourself and apply it to another situation where you would for example select every 100th observation in the dataframe.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "from_limited_data_to_populations.html#exercise-4-rolling-two-dice",
    "href": "from_limited_data_to_populations.html#exercise-4-rolling-two-dice",
    "title": "4  From limited data to populations",
    "section": "4.9 Exercise 4: Rolling two dice",
    "text": "4.9 Exercise 4: Rolling two dice\nWrite a function which allows you to virtually throw a pair of dice and sum up the points shown after the throw. Simulate and plot the result of your simulation. Are these dice fair of biased? Why?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "from_limited_data_to_populations.html#exercise-3-conversion-to-standard-units-with-pencil-and-paper",
    "href": "from_limited_data_to_populations.html#exercise-3-conversion-to-standard-units-with-pencil-and-paper",
    "title": "4  From limited data to populations",
    "section": "4.9 Exercise 3: Conversion to standard units with pencil and paper",
    "text": "4.9 Exercise 3: Conversion to standard units with pencil and paper\n\nOn a certain exam, the average of the scores was 50 and the standard deviation was 10.\n\nConvert each of the following scores to standard units: 60, 45, 75.\nFind the scores which in standard units are: 0, 1.5, -2.8\n\nConvert each entry on the following list to standard units (using the average and standard deviation of the list): 13, 9, 11, 7, 10. Find the average and the standard deviation of the converted list.\n\n\n4.9.1 Exercises R\n\n\n\n\n\n\nEcercise 1: Select every 100th row from the height data\n\n\n\nYou can try the code we used in the text to select every 50iest row in the heights data yourself and apply it to another situation where you would for example select every 100th observation in the dataframe.\n\n\n\n\n\n\n\n\nExercise 2: Sampling numbers from a list of numbers at random\n\n\n\nIn the stylized picture of a random sample, we had 24 households in the population from which we randomly picked a sample of 8. Use the sample function to pick randomly 8 numbers out of the numbers 1 to 24.\n\n\n\n\n\n\n\n\nExercise 3: Sample mean and sample standard deviation\n\n\n\nUse the emp_distr_height()function we have just written to draw a random sample of n=5000. Compute the mean and the standard deviation of the sample distribution. Compute - or look up in chapter 3 - the mean and the standard deviation of the population data and compare.\n\n\n\n\n\n\n\n\nExercise 4: Rolling two dice\n\n\n\nWrite a function which allows you to virtually throw a pair of dice and sum up the points shown after the throw. Simulate and plot the result of your simulation. Are these dice fair of biased? Why?\n\n\n\n\n\n\n\n\nExercise 5: Conversion into standard units\n\n\n\nConsider the values 175.9599 165.3240 169.0384 from a normal distribution with mean 172.7025 and standard deviation 4.830264. Convert these values to standard units.\n\n\n\n\n\n\n\n\nExercise5: Find the area under the normal curve\n\n\n\n\nFind the area under the normal curve:\n\n\nto the right of 1.25\nto the left of 0.80\nbetween -0.3 and 0.9\nto the left of -0.4\nbetween 0.4 and 1.3\noutside -1.5 to 1.5\n\n\nFill in the blanks:\n\n\nThe area between +/- under the normal curve equals 68%\nThe area between +/- under the normal curve equals 75 %.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "what_causes_what.html#contents",
    "href": "what_causes_what.html#contents",
    "title": "5  What causes what?",
    "section": "5.1 Contents",
    "text": "5.1 Contents\nIn this lecture we discuss what causation means in a statistical sense and why we need be careful to distinguish between causation and correlation. This is a conceptually difficult topic. It is, however, not technically difficult but it will need lots of examples. Fortunately there are many good (and bad) real world examples that I hope will stick with you as reference examples after the course. At the end of this lecture you should understand the idea of randomized trials and observational studies as well as the general ideas of comparison in statistical analysis.\n\nCausation in a statistical sense means that when we intervene, the chances of different outcomes vary systematically.\nCausation is difficult to establish statistically, but well designed randomized trials are the best available framework.\nYou will learn about the principles that helped clinical trials to identify effects and how you can transfer these principles to other fields.\nObservational data may have background factors influencing the apparent relationships between exposure and outcome which may be either observed confounders or lurking factors.\nStatistical methods do not suspend judgment which is always required for the confidence with which causation can be claimed."
  },
  {
    "objectID": "what_causes_what.html#analyzing-and-producing-tables-in-r-the-tapply-function.",
    "href": "what_causes_what.html#analyzing-and-producing-tables-in-r-the-tapply-function.",
    "title": "5  What causes what?",
    "section": "5.6 Analyzing and producing tables in R: The tapply function.",
    "text": "5.6 Analyzing and producing tables in R: The tapply function.\nIn this section we discussed examples of randomized controlled experiments and observational studies to explain key ideas of examining data and in particular how to compare treatment and control groups. In terms of R this gives us an opportunity to learn in this context about a very powerful R function which would help us analyze datasets that result from such experiments and help us to summarize large datasets.\nuse the arthritis table."
  },
  {
    "objectID": "what_causes_what.html#analyzing-and-producing-tables-in-r",
    "href": "what_causes_what.html#analyzing-and-producing-tables-in-r",
    "title": "5  What causes what?",
    "section": "5.6 Analyzing and producing tables in R",
    "text": "5.6 Analyzing and producing tables in R\nIn this section we discussed examples of randomized controlled experiments and observational studies to explain key ideas of examining data and in particular how to compare treatment and control groups.\nIn terms of R this gives us an opportunity to learn in this context about a very powerful R data types and functions which help us to analyze and summarize datasets that result from such experiments.\nLet us use a real dataset resulting from a randomized controlled trial of testing a drug to be taken against arthritis.6 This dataset is part of the R package multgee (Touloumis A. (2015)). We have added this dataset to our course data library. Let us inspect the first few lines6 The current issue of Wikipedia describes this illness as follows: “Arthritis is a term often used to mean any disorder that affects joints. Symptoms generally include joint pain and stiffness. Other symptoms may include redness, warmth, swelling, and decreased range of motion of the affected joints. In some types of arthritis, other organs are also affected. Onset can be gradual or sudden.”\n\nlibrary(JWL)\nhead(arthritis)\n\n  id y    sex age     trt baseline    time baselinescore score\n1  1 4   male  54    drug        2 Month 1             2     4\n2  1 5   male  54    drug        2 Month 3             2     5\n3  1 5   male  54    drug        2 Month 5             2     5\n4  2 4 female  41 placebo        3 Month 1             3     4\n5  2 4 female  41 placebo        3 Month 3             3     4\n6  2 4 female  41 placebo        3 Month 5             3     4\n\n\nThis dataset contains observations of the self-assessment score of arthritis, an ordered variable with five categories, collected at baseline and three follow-up times during a randomized comparative study of alternative treatments of 302 patients with rheumatoid arthritis. The data came from a randomized clinical trial that aimed to evaluate the effectiveness of the drug Auranofin versus the placebo therapy for the treatment of rheumatoid arthritis. The five-level (1=poor, . . . , 5=very good) ordinal multinomial response variable was the self-assessment of rheumatoid arthritis recorded at one (t = 1), three (t = 2) and five (t = 3) follow-up months.\n\n5.6.1 The factor class\nWe have already learned that each R object has a class or a type. We have learned about some of these classes like double, integer, chraracter, logical before. Some of the variables in our example dataset have a class we already know: id the identity of individuals who are part of the sample, y the response variable, age, the individual’s age, baseline are all of class or type numeric. We can find out the class of variables using the R function class(). If we type for example\n\nclass(arthritis$y)\n\n[1] \"integer\"\n\n\nR answers that this is an integer.\nFor categorical variables, we discussed in our first unit, R has a special class the factor-class. In our dataset the variables sex, trt, baselinescore and score have this class. For example of you type\n\n\nCode\n#|code-fold: false\n\nclass(arthritis$sex)\n\n\n[1] \"factor\"\n\n\nR answers that this is a factor. R factors are used for categorical variables. The categorical variable in our example is sex. This is the name the data creator chose for this variable. This variable has two categories or levels, which we can find out with the R levels()function by\n\nlevels(arthritis$sex)\n\n[1] \"female\" \"male\"  \n\n\nNote that the values of an R factor must be quoted. Either single or double quote marks is fine (though the marks don’t show up when we use head).\nFactors can sometimes be a bit tricky to work with, but the above is enough for now. Let’s see how to apply the notion in the current dataset.\nWe could for instance use the factor structure to compute the mean of the response of the treated male individuals in the sample in Month 1 and the mean response of the non-treated male participants in Month 1:\n\nwhichmt &lt;- which(arthritis$sex == \"male\" & \n                 arthritis$trt == \"drug\" & \n                 arthritis$time == \"Month 1\")\n\nwhichmnt &lt;- which(arthritis$sex == \"male\" & \n                  arthritis$trt == \"placebo\" & \n                  arthritis$time == \"Month 1\")\n\nmean(arthritis[whichmt,2], na.rm = T)\n\n[1] 3.294643\n\nmean(arthritis[whichmnt,2], na.rm = T)\n\n[1] 3.095238\n\n\nLet us unpack this code. The which()function returns the index of entries for which all the conditions imposed on sex, drug and time in the arthritis dataframe are true.\nNow, look at the expression arthritis[whichmt,2]. Remember, data frames are accessed with two subscript expressions, one for rows, one for column, in the format\nd[the rows we want, the columns we want]\nThus we select the rows for which all conditions are met and the second column which contains the response variable. Then we take the average of all these observations.\n\n\n5.6.2 The tapply function\nR often offers various different ways to achieve things we want to compute or express. Some of them are stunningly short and elegant. They are also very useful if we have to work with tables like the arthristis dataset. You must therefor learn about the magic tapply()function of R.\nAssume we would have the task to produce a so called demographic table from the arthritis data. The table should show:\n\nHow many individuals got the drug and how many got the placebo?\nWhat is the mean age and the standard deviation of the individuals who got the drug and those who got the placebo?\nWhat are the numbers of males and females who got the drug or the placebo ?\nWhat is the distribution of baseline scores between individuals who got the drug and those who got the placebo?\n\nWe saw before how we could arrive at all of these values using the R subsetting rules for dataframes.\nBut tapplyoffers you an even more powerful option. Suppose we want to count the cases which were treated and those who got a placebo in Month 1. We first filter the data to only contain the observations for Month 1 and save these values in a new object dat.\n\ndat &lt;- arthritis[arthritis$time == \"Month 1\", ]\n\ntapply(dat$y, dat$trt, length)\n\nplacebo    drug \n    149     153 \n\n\nNow comes the magic of the tapplyfunction. The code expressed in words says the following: Group the variable y into the levels of the factor trt. This gives you two groups the group “drug” and the group “placebo”. Now apply the function length()to both groups. This is the same as counting the cases.\nWhat about question 2? Let us apply the same logic here as before.\n\ntapply(dat$age, dat$trt, mean)\n\n placebo     drug \n50.70470 50.05882 \n\ntapply(dat$age, dat$trt, sd)\n\n placebo     drug \n11.23490 11.02583 \n\n\nHere we again apply the same logic. Group the data in dataccroding to the levels of our fatcor trt, i.e. in the group placebo and the group drug and apply to each group the funtion mean() and the function sd.\nTo make sure it is clear how this works, say for the case of computing the mean, let’s look at a small artificial example, which I have taken from a nice book by (Mattloff2023?):\n\nx &lt;- c(8,5,12,13)\ng &lt;- c('M',\"F\",'M','M')\n\nSuppose \\(x\\) is the ages of some kids, who are a boy, a girl, then two more boys, as indicated in \\(g\\). For instance, the 5-year-old is a girl.\nLet’s call tapply:\n\ntapply(x,g,mean)\n\n F  M \n 5 11 \n\n\nThat call said, “Split x into two piles, according to the corresponding elements of g, and then find the mean in each pile.\nNote that it is no accident that x and g had the same number of elements above, 4 each. If on the contrary, \\(g\\) had 5 elements, that fifth element would be useless — the gender of a nonexistent fifth child’s age in \\(x\\). Similarly, it wouldn’t be right if \\(g\\) had had only 3 elements, apparently leaving the fourth child without a specified gender.\nNow, let us take another, perhaps slightly more complex example by looking at question 3. Let us begin with the absolute numbers. Now we have two factors according to which we form the groups. Now we do not only group by treatment but also by sex. Then we count the cases in each group. If we have multiple factors, we have to give the grouping argument to apply by the syntax list(factor1, factor2, etc.)\n\ntapply(dat$y, list(dat$sex , dat$trt), length)\n\n       placebo drug\nfemale      43   40\nmale       106  113\n\n\nFinally, we would answer question 4 by coding\n\ntapply(dat$y, list(dat$baselinescore, dat$trt), length)\n\n  placebo drug\n1      11   12\n2      35   38\n3      70   69\n4      28   28\n5       5    6"
  },
  {
    "objectID": "what_causes_what.html#tabulating-and-analyzing-data-from-controlled-randomized-trials-in-r",
    "href": "what_causes_what.html#tabulating-and-analyzing-data-from-controlled-randomized-trials-in-r",
    "title": "5  What causes what?",
    "section": "5.6 Tabulating and analyzing data from controlled randomized trials in R",
    "text": "5.6 Tabulating and analyzing data from controlled randomized trials in R\nIn this section we discussed examples of randomized controlled experiments and observational studies to explain key ideas of examining data and in particular how to compare treatment and control groups.\nIn terms of R this gives us an opportunity to learn in this context about a very powerful R data types and functions which help us to analyze and summarize datasets that result from such experiments.\nLet us use a real dataset resulting from a randomized controlled trial of testing a drug to be taken against arthritis.6 This dataset is part of the R package multgee (Touloumis A. (2015)). We have added this dataset to our course data library. Let us inspect the first few lines6 The current issue of Wikipedia describes this illness as follows: “Arthritis is a term often used to mean any disorder that affects joints. Symptoms generally include joint pain and stiffness. Other symptoms may include redness, warmth, swelling, and decreased range of motion of the affected joints. In some types of arthritis, other organs are also affected. Onset can be gradual or sudden.”\n\nlibrary(JWL)\nhead(arthritis)\n\n  id y    sex age     trt baseline    time baselinescore score\n1  1 4   male  54    drug        2 Month 1             2     4\n2  1 5   male  54    drug        2 Month 3             2     5\n3  1 5   male  54    drug        2 Month 5             2     5\n4  2 4 female  41 placebo        3 Month 1             3     4\n5  2 4 female  41 placebo        3 Month 3             3     4\n6  2 4 female  41 placebo        3 Month 5             3     4\n\n\nThis dataset contains observations of the self-assessment score of arthritis, an ordered variable with five categories, collected at baseline and three follow-up times during a randomized comparative study of alternative treatments of 302 patients with rheumatoid arthritis. The data came from a randomized clinical trial that aimed to evaluate the effectiveness of the drug Auranofin versus the placebo therapy for the treatment of rheumatoid arthritis. The five-level (1=poor, . . . , 5=very good) ordinal multinomial response variable was the self-assessment of rheumatoid arthritis recorded at one (t = 1), three (t = 2) and five (t = 3) follow-up months.\n\n5.6.1 The factor class\nWe have already learned that each R object has a class or a type. We have learned about some of these classes like double, integer, chraracter, logical before. Some of the variables in our example dataset have a class we already know: id the identity of individuals who are part of the sample, y the response variable, age, the individual’s age, baseline are all of class or type numeric. We can find out the class of variables using the R function class(). If we type for example\n\nclass(arthritis$y)\n\n[1] \"integer\"\n\n\nR answers that this is an integer.\nFor categorical variables, we discussed in our first unit, R has a special class the factor-class. In our dataset the variables sex, trt, baselinescore and score have this class. For example of you type\n\n\nCode\n#|code-fold: false\n\nclass(arthritis$sex)\n\n\n[1] \"factor\"\n\n\nR answers that this is a factor. R factors are used for categorical variables. The categorical variable in our example is sex. This is the name the data creator chose for this variable. This variable has two categories or levels, which we can find out with the R levels()function by\n\nlevels(arthritis$sex)\n\n[1] \"female\" \"male\"  \n\n\nNote that the values of an R factor must be quoted. Either single or double quote marks is fine (though the marks don’t show up when we use head).\nFactors can sometimes be a bit tricky to work with, but the above is enough for now. Let’s see how to apply the notion in the current dataset.\nWe could for instance use the factor structure to compute the mean of the response of the treated male individuals in the sample in Month 1 and the mean response of the non-treated male participants in Month 1:\n\nwhichmt &lt;- which(arthritis$sex == \"male\" & \n                 arthritis$trt == \"drug\" & \n                 arthritis$time == \"Month 1\")\n\nwhichmnt &lt;- which(arthritis$sex == \"male\" & \n                  arthritis$trt == \"placebo\" & \n                  arthritis$time == \"Month 1\")\n\nmean(arthritis[whichmt,2], na.rm = T)\n\n[1] 3.294643\n\nmean(arthritis[whichmnt,2], na.rm = T)\n\n[1] 3.095238\n\n\nLet us unpack this code. The which()function returns the index of entries for which all the conditions imposed on sex, drug and time in the arthritis dataframe are true.\nNow, look at the expression arthritis[whichmt,2]. Remember, data frames are accessed with two subscript expressions, one for rows, one for column, in the format\nd[the rows we want, the columns we want]\nThus we select the rows for which all conditions are met and the second column which contains the response variable. Then we take the average of all these observations.\n\n\n5.6.2 The tapply function\nR often offers various different ways to achieve things we want to compute or express. Some of them are stunningly short and elegant. They are also very useful if we have to work with tables like the arthristis dataset. You must therefor learn about the magic tapply()function of R.\nAssume we would have the task to produce a so called demographic table from the arthritis data. The table should show:\n\nHow many individuals got the drug and how many got the placebo?\nWhat is the mean age and the standard deviation of the individuals who got the drug and those who got the placebo?\nWhat are the numbers of males and females who got the drug or the placebo ?\nWhat is the distribution of baseline scores between individuals who got the drug and those who got the placebo?\n\nWe saw before how we could arrive at all of these values using the R subsetting rules for dataframes.\nBut tapplyoffers you an even more powerful option. Suppose we want to count the cases which were treated and those who got a placebo in Month 1. We first filter the data to only contain the observations for Month 1 and save these values in a new object dat.\n\ndat &lt;- arthritis[arthritis$time == \"Month 1\", ]\n\ntapply(dat$y, dat$trt, length)\n\nplacebo    drug \n    149     153 \n\n\nNow comes the magic of the tapplyfunction. The code expressed in words says the following: Group the variable y into the levels of the factor trt. This gives you two groups the group “drug” and the group “placebo”. Now apply the function length()to both groups. This is the same as counting the cases.\nWhat about question 2? Let us apply the same logic here as before.\n\ntapply(dat$age, dat$trt, mean)\n\n placebo     drug \n50.70470 50.05882 \n\ntapply(dat$age, dat$trt, sd)\n\n placebo     drug \n11.23490 11.02583 \n\n\nHere we again apply the same logic. Group the data in dataccroding to the levels of our fatcor trt, i.e. in the group placebo and the group drug and apply to each group the funtion mean() and the function sd.\nTo make sure it is clear how this works, say for the case of computing the mean, let’s look at a small artificial example, which I have taken from a nice book by (Mattloff2023?):\n\nx &lt;- c(8,5,12,13)\ng &lt;- c('M',\"F\",'M','M')\n\nSuppose \\(x\\) is the ages of some kids, who are a boy, a girl, then two more boys, as indicated in \\(g\\). For instance, the 5-year-old is a girl.\nLet’s call tapply:\n\ntapply(x,g,mean)\n\n F  M \n 5 11 \n\n\nThat call said, “Split x into two piles, according to the corresponding elements of g, and then find the mean in each pile.\nNote that it is no accident that x and g had the same number of elements above, 4 each. If on the contrary, \\(g\\) had 5 elements, that fifth element would be useless — the gender of a nonexistent fifth child’s age in \\(x\\). Similarly, it wouldn’t be right if \\(g\\) had had only 3 elements, apparently leaving the fourth child without a specified gender.\nNow, let us take another, perhaps slightly more complex example by looking at question 3. Let us begin with the absolute numbers. Now we have two factors according to which we form the groups. Now we do not only group by treatment but also by sex. Then we count the cases in each group. If we have multiple factors, we have to give the grouping argument to apply by the syntax list(factor1, factor2, etc.)\n\ntapply(dat$y, list(dat$sex , dat$trt), length)\n\n       placebo drug\nfemale      43   40\nmale       106  113\n\n\nFinally, we would answer question 4 by coding\n\ntapply(dat$y, list(dat$baselinescore, dat$trt), length)\n\n  placebo drug\n1      11   12\n2      35   38\n3      70   69\n4      28   28\n5       5    6"
  },
  {
    "objectID": "what_causes_what.html#tabulating-and-analyzing-data-from-a-controlled-randomized-trial-in-r",
    "href": "what_causes_what.html#tabulating-and-analyzing-data-from-a-controlled-randomized-trial-in-r",
    "title": "5  What causes what?",
    "section": "5.6 Tabulating and analyzing data from a controlled randomized trial in R",
    "text": "5.6 Tabulating and analyzing data from a controlled randomized trial in R\nIn this section we discussed examples of randomized controlled experiments and observational studies to explain key ideas of examining data and in particular how to compare treatment and control groups.\nIn terms of R this gives us an opportunity to learn in this context about a very powerful R data types and functions which help us to analyze and summarize datasets that result from such experiments.\nLet us use a real dataset resulting from a randomized controlled trial of testing a drug to be taken against arthritis.6 This dataset is part of the R package multgee (Touloumis A. (2015)). We have added this dataset to our course data library. Let us inspect the first few lines6 The current issue of Wikipedia describes this illness as follows: “Arthritis is a term often used to mean any disorder that affects joints. Symptoms generally include joint pain and stiffness. Other symptoms may include redness, warmth, swelling, and decreased range of motion of the affected joints. In some types of arthritis, other organs are also affected. Onset can be gradual or sudden.”\n\nlibrary(JWL)\nhead(arthritis)\n\n  id y    sex age     trt baseline    time baselinescore score\n1  1 4   male  54    drug        2 Month 1             2     4\n2  1 5   male  54    drug        2 Month 3             2     5\n3  1 5   male  54    drug        2 Month 5             2     5\n4  2 4 female  41 placebo        3 Month 1             3     4\n5  2 4 female  41 placebo        3 Month 3             3     4\n6  2 4 female  41 placebo        3 Month 5             3     4\n\n\nThis dataset contains observations of the self-assessment score of arthritis, an ordered variable with five categories, collected at baseline and three follow-up times during a randomized comparative study of alternative treatments of 302 patients with rheumatoid arthritis. The data came from a randomized clinical trial that aimed to evaluate the effectiveness of the drug Auranofin versus the placebo therapy for the treatment of rheumatoid arthritis. The five-level (1=poor, . . . , 5=very good) ordinal multinomial response variable was the self-assessment of rheumatoid arthritis recorded at one (t = 1), three (t = 2) and five (t = 3) follow-up months.\n\n5.6.1 The factor class\nWe have already learned that each R object has a class or a type. We have learned about some of these classes like double, integer, chraracter, logical before. Some of the variables in our example dataset have a class we already know: id the identity of individuals who are part of the sample, y the response variable, age, the individual’s age, baseline are all of class or type numeric. We can find out the class of variables using the R function class(). If we type for example\n\nclass(arthritis$y)\n\n[1] \"integer\"\n\n\nR answers that this is an integer.\nFor categorical variables, we discussed in our first unit, R has a special class the factor-class. In our dataset the variables sex, trt, baselinescore and score have this class. For example of you type\n\n\nCode\n#|code-fold: false\n\nclass(arthritis$sex)\n\n\n[1] \"factor\"\n\n\nR answers that this is a factor. R factors are used for categorical variables. The categorical variable in our example is sex. This is the name the data creator chose for this variable. This variable has two categories or levels, which we can find out with the R levels()function by\n\nlevels(arthritis$sex)\n\n[1] \"female\" \"male\"  \n\n\nNote that the values of an R factor must be quoted. Either single or double quote marks is fine (though the marks don’t show up when we use head).\nFactors can sometimes be a bit tricky to work with, but the above is enough for now. Let’s see how to apply the notion in the current dataset.\nWe could for instance use the factor structure to compute the mean of the response of the treated male individuals in the sample in Month 1 and the mean response of the non-treated male participants in Month 1:\n\nwhichmt &lt;- which(arthritis$sex == \"male\" & \n                 arthritis$trt == \"drug\" & \n                 arthritis$time == \"Month 1\")\n\nwhichmnt &lt;- which(arthritis$sex == \"male\" & \n                  arthritis$trt == \"placebo\" & \n                  arthritis$time == \"Month 1\")\n\nmean(arthritis[whichmt,2], na.rm = T)\n\n[1] 3.294643\n\nmean(arthritis[whichmnt,2], na.rm = T)\n\n[1] 3.095238\n\n\nLet us unpack this code. The which()function returns the index of entries for which all the conditions imposed on sex, drug and time in the arthritis dataframe are true.\nNow, look at the expression arthritis[whichmt,2]. Remember, data frames are accessed with two subscript expressions, one for rows, one for column, in the format\nd[the rows we want, the columns we want]\nThus we select the rows for which all conditions are met and the second column which contains the response variable. Then we take the average of all these observations.\n\n\n5.6.2 The tapply function\nR often offers various different ways to achieve things we want to compute or express. Some of them are stunningly short and elegant. They are also very useful if we have to work with tables like the arthristis dataset. You must therefor learn about the magic tapply()function of R.\nAssume we would have the task to produce a so called demographic table from the arthritis data. The table should show:\n\nHow many individuals got the drug and how many got the placebo?\nWhat is the mean age and the standard deviation of the individuals who got the drug and those who got the placebo?\nWhat are the numbers of males and females who got the drug or the placebo ?\nWhat is the distribution of baseline scores between individuals who got the drug and those who got the placebo?\n\nWe saw before how we could arrive at all of these values using the R subsetting rules for dataframes.\nBut tapplyoffers you an even more powerful option. Suppose we want to count the cases which were treated and those who got a placebo in Month 1. We first filter the data to only contain the observations for Month 1 and save these values in a new object dat.\n\ndat &lt;- arthritis[arthritis$time == \"Month 1\", ]\n\ntapply(dat$y, dat$trt, length)\n\nplacebo    drug \n    149     153 \n\n\nNow comes the magic of the tapplyfunction. The code expressed in words says the following: Group the variable y into the levels of the factor trt. This gives you two groups the group “drug” and the group “placebo”. Now apply the function length()to both groups. This is the same as counting the cases.\nWhat about question 2? Let us apply the same logic here as before.\n\ntapply(dat$age, dat$trt, mean)\n\n placebo     drug \n50.70470 50.05882 \n\ntapply(dat$age, dat$trt, sd)\n\n placebo     drug \n11.23490 11.02583 \n\n\nHere we again apply the same logic. Group the data in dataccroding to the levels of our fatcor trt, i.e. in the group placebo and the group drug and apply to each group the funtion mean() and the function sd.\nTo make sure it is clear how this works, say for the case of computing the mean, let’s look at a small artificial example, which I have taken from a nice book by (Mattloff2023?):\n\nx &lt;- c(8,5,12,13)\ng &lt;- c('M',\"F\",'M','M')\n\nSuppose \\(x\\) is the ages of some kids, who are a boy, a girl, then two more boys, as indicated in \\(g\\). For instance, the 5-year-old is a girl.\nLet’s call tapply:\n\ntapply(x,g,mean)\n\n F  M \n 5 11 \n\n\nThat call said, “Split x into two piles, according to the corresponding elements of g, and then find the mean in each pile.\nNote that it is no accident that x and g had the same number of elements above, 4 each. If on the contrary, \\(g\\) had 5 elements, that fifth element would be useless — the gender of a nonexistent fifth child’s age in \\(x\\). Similarly, it wouldn’t be right if \\(g\\) had had only 3 elements, apparently leaving the fourth child without a specified gender.\nNow, let us take another, perhaps slightly more complex example by looking at question 3. Let us begin with the absolute numbers. Now we have two factors according to which we form the groups. Now we do not only group by treatment but also by sex. Then we count the cases in each group. If we have multiple factors, we have to give the grouping argument to apply by the syntax list(factor1, factor2, etc.)\n\ntapply(dat$y, list(dat$sex , dat$trt), length)\n\n       placebo drug\nfemale      43   40\nmale       106  113\n\n\nFinally, we would answer question 4 by coding\n\ntapply(dat$y, list(dat$baselinescore, dat$trt), length)\n\n  placebo drug\n1      11   12\n2      35   38\n3      70   69\n4      28   28\n5       5    6"
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#exercise-6-the-mathematics-of-lines",
    "href": "modelling_relationships_using_regression_part_1.html#exercise-6-the-mathematics-of-lines",
    "title": "6  Modelling relationships: Points, Lines and Scatterplots",
    "section": "6.5 Exercise 6: The mathematics of lines",
    "text": "6.5 Exercise 6: The mathematics of lines\nLook at the following line"
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#exercises-r",
    "href": "modelling_relationships_using_regression_part_1.html#exercises-r",
    "title": "6  Modelling relationships: Points, Lines and Scatterplots",
    "section": "6.6 Exercises R",
    "text": "6.6 Exercises R\n\n\n\n\n\n\nExercise 1: Life expectancy\n\n\n\nThroughout history, life expectancy at birth was on average 30 year everywhere in the world. This time span is an average. It does not mean that even in historical times some people did not reach a higher age but the distribution of life expectancy in years at birth was centered at 30. This started to change slowly at around 1800. Today the life expectancy in the world is 70 years. This is a dramatic improvement, if you think about it.\nLife expectancy is associated with the level of income, measured as GDP per capita. GDP is a shorthand and means “gross domestic product” in the jargon of economists. What it measures is the value of all goods and services produced in a given country in a year expressed in a suitable monetary unit. This could be the currency of a country, it could be US-Dollars or other units, you choose them. If you divide this number by the population in a country in a given year you get GDP per capita. It is a proxy of the level of income in a country when viewed as an aggregate entity and is one of the most used number in economic statistics. In 2018 the poorest country in the world had a GDP per capita of about 623 $ expressed in international dollars at 2011 prices, a hypothetical monetary unit that makes income comparable accross countries. The richest country is in comparison at about 150.000 $ international.\nFor the year 2018 the data show that the relation of GDP follows a straight line with the equation \\(y=67.78+0.0002549 x\\)\nIn the exercise you are asked to add points to a plot. This is done in R like this. You first write the code for the line graph and then write the code for a point with the syntax points(x,y).\n\nUse R to plot this curve starting from 0 and ranging to 10000. Also add a title and axes descriptions.\nNow let’s check real data and see where selected countries in the world lie in this graph, by plotting these countries as points within the graph. In order to do so, we need first to load the data. The data are in your JWL package, which needs to be loaded with the command library(JWL) in R.The data we need here are called life_expectancy. Load these data and store them in an R object.\nCreate a new R object which contains only the data of the year 2018.\nAdd the points for some countries to this plot. Say, we choose Chad, Egypt, Kenya, Vietnam, Thailand, India, Switzerland, United States, Qatar. You have learned in the lecture to draw a line using the curve()function. We did not discuss how to add points to such a plot. Try to figure out how to do this using the R help function ?points. You can also learn how to anotate the points by looking up ?text in the R help function.\nAdd your own country to the plot.\nQuatar is an outlier here. How would you interpret this peculiar position? Try to think of an explanation, that could be investigated and fact checked in a next step.\n\n\n\n\n\n\n\n\n\nExercise 2: Reporduce the scatter plot on primary school enrollment rate\n\n\n\nIn this exercise we ask you to reproduce the scatter plot on the enrollment rate in primary school and the attendance rate discussed in section 3.\n\nLoad the JWL package. The dataset is called enrol_attend_dat. Use the R help function to study the data an the meaning of the variables as well as the original data source.\nCan you write an R code reproducing the graphics?\nGive an interpretation of the graph in your own words.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAndrew Gelman, Aki Vehtari, Jenifer Hill. 2021. Regression and Other Stories. Cambridge University Press."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_1.html#life-expectancy",
    "href": "modelling_relationships_using_regression_part_1.html#life-expectancy",
    "title": "6  Modelling relationships: Points, Lines and Scatterplots",
    "section": "6.7 Life expectancy",
    "text": "6.7 Life expectancy\nThroughout history, life expectancy at birth was on average 30 year everywhere in the world. This time span is an average. It does not mean that even in historical times some people did reach a higher age but the distribution of life expectancy in years at birth was centered at 30. This started to change slowly at around 1800. Today the life expectancy in the world is 70 years. This is a dramatic improvement, if you think about it."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_2.html#three-properties-of-r",
    "href": "modelling_relationships_using_regression_part_2.html#three-properties-of-r",
    "title": "7  Modelling relationship: Prediction using regression",
    "section": "7.2 Three properties of r:",
    "text": "7.2 Three properties of r:\n\n\\(r\\) is a pure number. It has no units. This is because \\(r\\) is computed from standard units.\n\\(r\\) is unaffected by changing the units on either axis. This is again because \\(r\\) is computed based on standard units.\n\\(r\\) is unaffected when we switch the axes. Algebraically this must be the case because when we multiply \\(x\\) by \\(y\\) the value of the product is the same as when we multiply \\(y\\) by \\(x\\). Geometrically switching axes reflects the scatterplot along the line \\(x=y\\) but does not change the amount of clustering along the line.\n\n:::\n\n7.2.1 r is a measure of clustering along a line.\n\\(r\\) is a measure of clustering along a line. This is often referred to by calling \\(r\\) a measure of linear association. Note that data are often associated in other ways not necessarily along a line. In such cases \\(r\\) would show a weak association only.\nAs one real world examples let us take the relation between the share of extremely poor people across countries in the world in a given year versus GDP per capita in that same year. Here we take the year 2015. This is what we get.\n\n\nCode\nlibrary(JWL)\n\n# remove World, which should not be there and Luxembourg which is an extreme outlier\n\ndata_2015 &lt;- poverty_vs_gdp[(poverty_vs_gdp$Year == 2015 &\n                            !(poverty_vs_gdp$Entity %in% c(\"World\", \"Luxembourg\"))), ]\n\noptions(scipen = 999)\nplot(data_2015$GDPpc, data_2015$Share,\n     xlab = \"GDP per capita\",\n     ylab = \"Share in extreme poverty\",\n     main = \"Share of population living in extreme poverty vs. GDP per\ncapita, 2015\",\n     pch = 19,\n     cex = 0.5,\nxaxt = \"n\",\nyaxt = \"n\")\n\naxis(1, at = c(0, 20000, 40000, 60000, 80000),\n     labels = paste0(c(\"0\", \"20.000\", \"40.000\", \"60.000\", \"80.000\"), \" \", \"$\"), cex.axis = 0.9)\naxis(2, at = c(0, 10, 20, 30, 40, 50, 60),\n     labels = paste0(as.character(c(0, 10, 20, 30, 40, 50, 60)), \" \", \"%\"), cex.axis = 0.9)\n\n\n\n\n\nYou can see that the data show a strong association between GDP per capita and the share of extreme poverty, which is intuitive. Moreover this association is not along a straight line.\nIf we would compute the correlation coefficient in these data, we would get a value of -0.4532214, a weak negative association.\nIn the following picture we show the data with smooth curve that approximates the shape of the data in red. Had we fitted a straight line to these data, you can see from the picture that the fit is poor. Though there seems to be a relatively strong association between GDP per capita and the share of people living in extreme poverty, it is not a linear association, i.e. an association along a straight line. The correlation coefficient only captures the clustering of data points along a straight line.\n\n\nCode\ndf &lt;- data.frame(x = data_2015$GDPpc, y = data_2015$Share)\nplot(df$x, df$y,\n     xlab = \"GDP per capita\",\n     ylab = \"Share in extreme poverty\",\n     main = \"Share of population living in extreme poverty vs. GDP per\ncapita, 2015\",\n     pch = 19,\n     cex = 0.5,\nxaxt = \"n\",\nyaxt = \"n\")\n\naxis(1, at = c(0, 20000, 40000, 60000, 80000),\n     labels = paste0(c(\"0\", \"20.000\", \"40.000\", \"60.000\", \"80.000\"), \" \", \"$\"), cex.axis = 0.9)\naxis(2, at = c(0, 10, 20, 30, 40, 50, 60),\n     labels = paste0(as.character(c(0, 10, 20, 30, 40, 50, 60)), \" \", \"%\"), cex.axis = 0.9)\n\nmod &lt;- lm(y ~ x, data = df)\n\nlines(stats::lowess(df, f= 1/7), lw = 2, col= \"red\")\n\nabline(mod, col = 4, lw = 2)\n\n\n\n\n\n\n\n7.2.2 Association is not causation\nIn the discussion of correlations we need to come back to an issue we had discussed in a previous lecture. It is important to keep in mind that correlations measure linear association between two variables but association is not the same as causation.\nLet me come back to an example we discussed earlier in section 3, where we looked at the correlation between average learning outcomes and total education expenditure by per capita documented in the study by Larry Hedges (1994)\n\n\nCode\nlibrary(JWL)\nplot(expenditure_outcome_dat$Expenditure, \n     expenditure_outcome_dat$Outcome, \n     main = (\"Average learning outcomes by total education expenditure per capita\"), \n     xlab = (\"Public and private per capita expenditure on education (PPP, constant 2011-intl $)\"), \n     ylab = (\"Average harmonized learning outcome in 2005 - 2015\"), pch = 16)\n\naxis(1, at = seq(500, 3500, by = 500))\n\nmod &lt;- lm(Outcome ~ Expenditure, data = expenditure_outcome_dat)\n\nabline(mod, col = 4, lw = 2)\n\n\n\n\n\nThe correlation in these data is 0.5684246. But does this evidence does not support the conclusion that there is a causal relation between increased educational spending and student outcomes. Comparing outcomes of students across countries or students across schools with different levels of spending does not tell us whether the different spending levels are causal for the variation in outcomes we observe. There may be many other differences between countries and students that are shaping this relationship.\n\n\n7.2.3 Ecological correlations\nThe previous example shows a case of correlations based on averages. We have seen by now various examples of such correlations. Correlations based on averages can be frequently found in social sciences such as sociology, political science or economics. However these correlations have to be assessed with care because they can overstate the strength of an association.\nLet me illustrate this point by a hypothetical example. Many studies have looked at the association between individual education and individual income, claiming a positive association between the level of education and individual income. Suppose a survey had collected data on income and education for individuals in three countries, labelled by three colors: Country red, country orange and country blue. Each individual is marked by one color symbolizing it’s country of residence. Let’s do not worry about the exact units. Say education is measured by some form of time of education and income in some monetary unit.\n\n\nCode\nset.seed(12) # set the random number generator seed\n\n  x &lt;- 0:24\n  y &lt;- 1153*x + rnorm(25, 20000, 8000) # create the toy data of duration of edcuation and income\n  \n  df &lt;- data.frame(Education = x, Income = y) # write into dataframe\n  \n  \n  df$State &lt;-  with(df, ave(seq_along(Income), Income, FUN = function(x)\n    sample(c(\"Red\", \"Blue\", \"Orange\"), size = length(x), replace = TRUE, prob = c(1/3, 1/3, 1/3)))) |&gt;    as.factor() # asign colors as factors to the education and income combination\n\n  color_pallete_function &lt;- colorRampPalette(\n  colors = c(\"red\", \"orange\", \"blue\"),\n  space = \"Lab\" # Option used when colors do not represent a quantitative scale\n  ) # define a state by a color\n  \n  num_colors &lt;- nlevels(df$State)\n  df_state_state &lt;- color_pallete_function(num_colors)\n  \n# Do the exercise for state averages\n  \n  col1 &lt;- tapply(df$Education, df$State, mean)\n  col2 &lt;- tapply(df$Income, df$State, mean)\n\ndf_avg &lt;- data.frame(Education = col1, Income = col2, State = as.factor(names(col1)))\n\nnum_colors_avg &lt;- nlevels(df_avg$State)\ndf_state_state_avg &lt;- color_pallete_function(num_colors_avg)\n\n# plot side by side\n\npar(mfrow = c(1,2))\n\n# plot for individual values\n  \n  plot(\n  x = df$Education,\n  y = df$Income,\n  xlab = \"Individual education\",\n  ylab = \"Individual income\",\n  pch = 20, # solid dots increase the readability of this data plot\n  col = df_state_state[df$State],\n  xaxt = \"n\",\n  yaxt = \"n\")\n  \n# plot for aggregated values\n\n plot(\n  x = df_avg$Education,\n  y = df_avg$Income,\n  xlab = \"Country average education\",\n  ylab = \"Country average income\",\n  pch = 20, # solid dots increase the readability of this data plot\n  col = df_state_state_avg[df_avg$State],\n  xaxt = \"n\",\n  yaxt = \"n\"\n)\n\n\n\n\n\nIn the averaged data the correlation increases from 0.848 to 0.996 or by about 17.5 %. So bear in mind that, while correlations based on averages are frequently encountered in the press, in reports and in the research literature, they tend to overstate the strength of an association."
  },
  {
    "objectID": "modelling_relationships_using_regression_part_2.html#exercises",
    "href": "modelling_relationships_using_regression_part_2.html#exercises",
    "title": "7  Modelling relationship: Prediction using regression",
    "section": "7.3 Exercises",
    "text": "7.3 Exercises\n\n7.3.1 Exercises\n\n\n\n\n\n\nExercise 1: Comparing correlations\n\n\n\nLook at the following four data sets. Rank them from weakest to strongest correlations.\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Interpreting correlation\n\n\n\nStudies find a negative correlation between hours spent watching television and scores on reading tests. Does watching television make people less able to read? Discuss briefly.\n\n\n\n\n\n\n\n\nExerxise 3: Rating learning facilitators\n\n\n\nIn a statistics course, a course with lectures and small discussion sections led by learning facilitators, in the last lecture, students were asked to fill out anonymous questionaires rating the effectiveness of their learning facilitators and the course on the scale:\n1: poor, 2: fair, 3: good, 4: very good, 5: excellent\nThe following statistics were then computed:\n\nThe average rating of the learning facilitator by the students in each section.\nThe average rating of the course by the students in each section\nThe average score on the final for the students in each section.\n\nResults are shown below - sections are identified by letter. Draw a scatter diagram for each pair of the variables - there are three pairs - and find the correlations.\n\n\nCode\nexample &lt;- data.frame(section = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"),\n                      avratlf = c(3.3, 2.9, 4.1, 3.3, 2.7, 3.4, 2.8, 2.1, 3.7, 3.2, 2.4),\n                      avratco = c(3.5, 3.2, 3.1, 3.3, 2.8, 3.5, 3.6, 2.8, 2.8, 3.3, 3.3),\n                      avsfin  = c(70,64,47,63,69,69,69,63,53,65,64))\n\nknitr::kable(example)\n\n\n\n\n\nsection\navratlf\navratco\navsfin\n\n\n\n\nA\n3.3\n3.5\n70\n\n\nB\n2.9\n3.2\n64\n\n\nC\n4.1\n3.1\n47\n\n\nD\n3.3\n3.3\n63\n\n\nE\n2.7\n2.8\n69\n\n\nF\n3.4\n3.5\n69\n\n\nG\n2.8\n3.6\n69\n\n\nH\n2.1\n2.8\n63\n\n\nI\n3.7\n2.8\n53\n\n\nJ\n3.2\n3.3\n65\n\n\nK\n2.4\n3.3\n64\n\n\n\n\n\nwhere avratlf means average rating of learning facilitator, avratco means average rating of course and avsfin means average score on final.\nThe data are section averages. Since the questionnaire was anonymous, it was not possible to link up student ratings with scores on an individual basis. Student ability may be a confounding factor. However, controlling for pretest resulst turned out to make no difference to the analysis. Each learning facilitator taught in onse section. True or false? Explain.\n\nOn the average those sections that liked their LF more did better on the final.\nThere was almost no relationship between the section’s average ratibg of the learning facilitator and the section’s average rating of the course.\nThere was almost no relationship betwee the section’s average rating of the course and the section’s average score on the final.\n\n\n\n\n\n\n\n\n\nExercise 4: Explain\n\n\n\nWhen we calculate the correlation coefficient, why do we convert data to standard units?\n\n\n\n\n7.3.2 Exercises R\n\n\n\n\n\n\nExercise 1: Write a conversion function from any data to standard units:\n\n\n\nWrite a function called convert_su which takes in a dataframe of elements called data and returns a dataframe of the values represented in standard units. To do a specific computation with the function take the dataset pearson from the JWLpackage. These are the data we used in the lecture.\n\n\n\n\n\n\n\n\nExercise 2: Write your own function to compute correlation.\n\n\n\nWrite a function called calculate_correlation which takes in a dataframe which is the output of convert_su and returns the correlation coefficient. Again test your function with the pearsondataset from the JWL package.\n\n\n\n\n\n\n\n\nExercise 3: Advertising channels and sales\n\n\n\nLoad the advertising dataset from the JWL package. The data show 200 samples of advertising expenditures via different advertising channels, TC, radio and newspaper as well as revenues from sales.\n\nVisualize the association between advertising expenditures and the value of sales for each advertising channel. Interpret the graphs in your own words.\nCompute the correlation between advertising expenditures and sales for the different advertising channels.\nBuild a linear model between advertising expenditures via TV and sales and display a scatterplot of the data as well as the regression line. Use lm() to compute the intercept and the slope of the regression line.\nPredict the sales from the model if we spend 0 on TV advertising? What are the predicted sales if we spend 300 on advertising?\n\n\n\n\n\n7.3.3 Exercises Project\n\n\n\n\nAdhikari, Ani, John DeNero, and David Wagner. “Computational and Inferential Thinking: The Foundations of Data Science.” https://inferentialthinking.com/chapters/intro.html.\n\n\nLarry Hedges, Rob Greenwald, Richard Laine. 1994. “Does Money Matter? A Meta-Analysis of Studies of the Effects of Differential School Inputs on Student Outcomes.” Educational Researcher 3 (23): 5–14."
  }
]