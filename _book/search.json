[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics",
    "section": "",
    "text": "Preface\nThese are lecture notes for my introductory statistics and data literacy course I am developing for Jesuit Worldwide Learning (JWL) together with my colleagues from Seitwerk. I would like to thank Peter Balleis, Mathias Beck, Martha Habash, Stefan Hengst and Anny Mayr for all their generous help and support in kickstarting this challenging project.\nThe goal of the notes is to develop the core contents of the course systematically and to support the production of the online units. I also write up in these notes material and instructions for teaching material, which is meant to be used in the local study centers. My vision is that after we are through with the production of the online units, the notes will be overhauled and can then when the course actually begins, be used by the students as a textbook and reading material.\nFor the time beeing it is more like a systematic notebook and a scenario. I will add questions and comments for the Seitwerk team during this phase as collapsible callout notes like this:\n\n\n\n\n\n\nComment or Remark for Seitwerk\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user.\n\n\n\nThis helps me to keep a better overview of the overall development of the text.\nI will put the developing material on our MS-teams collaboration platform as we go along. Data and graphics will be put on the platform as separate files. In addition, mainly for the interaction with my past and future self, I keep a versioning of the notes on a public github archive, which may also be accessed by colleagues, and of course by the Seitwerk team. The github repository’s address is:\nhttps://github.com/Martin-Summer-1090/Statistics_JWL"
  },
  {
    "objectID": "introduction.html#experiencing-the-world-through-data",
    "href": "introduction.html#experiencing-the-world-through-data",
    "title": "1  Introduction",
    "section": "1.1 Experiencing the world through data",
    "text": "1.1 Experiencing the world through data\n\n1.1.1 What are the major sources of electricity production in Kenya?\nWhat is the biggest source of electricity production in Kenya? Answering this question needs data, listing and recording various forms of electricity production in specific countries. An internationally acknowledged organisation, which collects and reports these data is the International Energy Agency (IEA).\n\n\n\n\n\n\nAccessing IEA data on the internet\n\n\n\nIf you have access to the internet, you can look up data and reports by the IEA on it’s website https://www.iea.org/1\n\n\n1 A screenshot I have taken in August 2022 shows the website like this. In the upperm right part of the website there is a link called data which will bring you to the data collected by IEA. Now let’s take a quiz and guess. What do you think is actually the biggest source of electricity production in Kenya?\n\nCoal\nRenewable energy\nNatural gas\n\n\n\n\n\n\n\nSeitwerk: Expand for reading comment.\n\n\n\n\n\nIt would be great to make this like an online quiz, where you can click the answer and get right or wrong. Renewable energy is the right answer.\n\n\n\nConsulting the IEA data, you will find that the correct answer is renewable energy. A very interesting website called gapminder, which analyzes the answers of many people to this question, finds that 61 % give a wrong answer to this question. Maybe they find it hard to imagine that 80 % of energy production in Kenya is already fossil free thanks to huge sources of geothermal- and hydropower.2 Even if you break the answers down by country you see that 38 % of people from Kenya get the answer wrong. Among the people from the UK, who answer this question even 72 % answer wrongly.2 Geothermal electricity generation uses the earth’s natural heating energy - geothermal energy. A country needs to be located on a geothermal hot spot to make effective use of this energy source for electricity generation. At such a hotspot there are high temperatures beneath the earth’s surface which naturally produces steam. This steam can be used to spin turbines connected to a generator. This mechanism then produces electricity. Hydropower uses the water cycle to generate electricity by using dams to alter the flow of a river. The kinetic energy of the water spins turbines connected to a generator which produces electricity.\nIf you are not familiar with the details of electricity production using geothermal- and hydropower or if you are not completely sure how to interpret a number like 38 %, don’t worry for now. The point here is that you see that one useful consequence of being able to access, read and interpret data is that it can help to establish facts about the world. In this way data can help us to perhaps correct misconceptions we might have had about these facts.\n\n\n\n\n\n\nThe gapminder webpage\n\n\n\nIf you have internet access you can reach the gapminder webpage at https://www.gapminder.org/. I encourage you to visit this page at an occasion when you have access to the internet. You will be surprised how often you might have a wrong guess about basic facts in the world.3\n\n\n3 Here is how the website looks like as of August 2022: In this course you will learn how to work with data and how to learn from these data in a systematic way.\n\n\n1.1.2 How many trees are there on the planet?\nLearning from data entails more than just establishing facts. This might not always be possible, either because you cannot access the relevant data or you cannot completely access them, since doing so would be way too expensive. When working with data you also need rigorous definitions of concepts, so that you can actually transform your experience about the world into data\nThink for a moment about the following interesting example, which I learned from a wonderful book by the British statistician David Spiegelhalter (Spiegelhalter 2019). The example shows that even just categorizing and labeling things in the world to measure them and turn them into data can be challenging. A very basic question raised in the introduction of this book is:\n\n\n\n\n\n\nQuestion:\n\n\n\nHow many trees are there on the planet?\n\n\nIt is clear that answering this question is more challenging than the task the IEA had to solve when listing energy sources by country around the world in a given time period. But before you go about to think how you might count all the trees on the planet, you have to answer an even more basic question, namely: What is a tree?\nSome of you might think this is a silly and obvious question, which every child can answer. But what some might consider a tree others will consider just a shrub. Turning experience into data requires rigorous definitions. It turns out that such definitions can be given for trees. 44 For example the forestry expert Michael Kuhns writes: “…Though no scientific definition exists to separate trees and shrubs, a useful definition for a tree is a woody plant having one erect perennial stem (trunk) at least three inches in diameter at a point 4-1/2 feet above the ground, a definitely formed crown of foliage, and a mature height of at least 13 feet. This definition works fine, though some trees may have more than one stem and young trees obviously don’t meet the size criteria. A shrub can then be defined as a woody plant with several perennial stems that may be erect or may lay close to the ground. It will usually have a height less than 13 feet and stems no more than about three inches in diameter.” (Kuhns, n.d.)\nBut even with the definition at hand you cannot just go around the planet and count every plant that meets the criteria. So this is what the researchers investigating that question did according to (Spiegelhalter 2019):\n“…They first took a series of of areas with a common type of landscape, known as a biome and counted the average number of trees per square kilometer. They then used satellite imaging to estimate the total area of the planet covered by each type of biome, carried out some complex statistical modelling, and eventually came up with an estimate of 3.04 trillion (3,040,000,000,000) trees on the planet. This sounds a lot, except that they reckoned there used to be twice this number.”\nNow imagine that if long expert discussions are needed to precisely define something so seemingly obvious as a tree, clearly more complex concepts such as unemployment or the definition of the total value of goods and services produced in a country in a given year, known as Gross Domestic Product or GDP, is even more challenging.\nThere is no automatism or mechanical receipt how we can turn experience into data and the statistics that we use and produce are constructed on the basis of judgement. It is a starting point for a deeper understanding of the world around us. This is one of the reasons why in this course statistics is not only referred to as science, which it arguably is to some degree, but also as an art.\nTo guard against the trap of mindlessly fall into mechanical thinking or automatism it is sometimes helpful to make a rough plausibility estimates about the order of magnitude you might expect as a result for a really big number, like the number of trees on the globe.\n\n\n\n\n\n\nComment for Seitwerk: Please uncollapse\n\n\n\n\n\nHere would be a good opportunity to engage students in an activity estimating a big number. Since this has an interactive part, which better works when in class together, one could set up the problem with some guidance in the online unit and then discuss solutions in class. It might be fun to add a competitive element by splitting the students in teams or pairs and rewarding the team/pair who comes closest to the true value. Let them first just guess and then lead them through the guestimation a bit more systematically: One way to lead students through this exercise might be: How many people are there in country X, how many children of school age, how many of them ride the bus, how many students can be transported by a bus etc. The estimate will have lots of uncertainty but be hopefully closer to the truth than most of the original unguided guesses. It is a first encounter for the students with propagation of uncertainty, an important topic in statistics more generally. The activity contains further interesting aspects like reliability of data sources and the design of data collection. I am not quite sure whether and how to build this in at this stage but maybe you have an idea or suggestion.\nA good example would for instance be: How many school buses are there in country X?\n\n\n\n\n\n1.1.3 What has happened to extreme poverty on the globe in the last 20 years?\nOne of the limitations of data as a source of knowledge about the world is that anything we choose to measure will differ across places, across persons or across time. When analyzing and trying to understand data we will always face the problem how we can extract meaningful insights from this apparent random variation. One challenge faced by statistics and one core topic in this course will thus be how we can distinguish in data the important relationships from the unimportant background variability.\nExploring and finding such meaningful relationships or patterns in data using the science of statistics and computational tools is one of the skills you will learn in this course.\nAn example of a pattern in data is if we can spot a trend, data values which are for example increasing or decreasing.\nConsider the following data from the World Bank, reporting the share of people in the world who are living in extreme poverty. Extreme poverty is defined by the World Bank, an international development finance organisation for low and middle income countries located in the US5 as the percentage of people in the world who have to live on less that $ 1.90 per day. Hans Rosling in his great book Factfullness (Rosling, Rosling, and Rosling-Rönnlund 2018), gives a concrete description of what it means in concrete terms to live on this income level, which Rosling calls level 1. I quote from his book:5 The Worldbank is an international financial institution founded along with the International Monetary Fund in the Bretton Wods conference in 1944. It is located in Washington D.C. and finances projects in low and middle income countries. It also collects and processes data globally to support its activities and conduct development research. The Worldbank makes its data public in print or through its website https://www.worldbank.org/en/home\n“… Your five children have to spend hours walking barefoot with your single plastic bucket, back and forth, to fetch water from a dirty mud hole and hour’s walk away. On their way home they gather firewood, and you prepare the same gray porridge that you have been eating at every meal, every day, for your whole life - except during the months when the meager soil yielded no crops and you went to bed hungry. One day your youngest daughter develops a nasty cough. Smoke from the indoor fire is weakening her lungs. You can’t afford antibiotics, and one month later she is dead. Yet you keep struggling on. If you are lucky and the yields are good, you can maybe sell some surplus crops and manage to earn more than 2 $ a day, which would move you to the next level…(Roughly 1 billion people live like this today).\nLet us have a look at a table showing the first 10 observations of this share\n\n\nCode\n# read poverty data from our project data folder\npovdat_by_country &lt;- read.csv(\"data/extreme_poverty/share-of-population-in-extreme-poverty.csv\")\n# select the years from 2000\npovdat_world &lt;- with(povdat_by_country, povdat_by_country[Entity == \"World\" & Year &gt;= 2000, ])\n# Keep only the year and the share\nplot_data &lt;- povdat_world[,c(3,4)]\n# Rename variables\nnames(plot_data) &lt;- c(\"Year\", \"Share\")\n\n# produce a table\nlibrary(knitr)\nkable(head(plot_data, n= 10), row.names = F, digits = 1)\n\n\n\n\nTable 1.1: Share of world polpulation living in extreme poverty, Source: World Bank\n\n\nYear\nShare\n\n\n\n\n2000\n27.8\n\n\n2001\n26.9\n\n\n2002\n25.7\n\n\n2003\n24.7\n\n\n2004\n22.9\n\n\n2005\n21.0\n\n\n2006\n20.3\n\n\n2007\n19.1\n\n\n2008\n18.4\n\n\n2009\n17.6\n\n\n\n\n\n\nCode\nwrite.csv(head(plot_data, n= 10), file = \"tables/table_1_1_world_poverty.csv\", row.names = FALSE)\n\n\nThe table shows that the share of people living in extreme poverty has been decreasing year after year for the first 10 years since the year 2000. This is a pattern which is called a downward trend.\nNote that we did not show the whole series of numbers. The data points in our data-set actually range until the year 2017. Printing them all in a table quickly produces very large and unwieldy number array which is awkward to read.\nExploring data and detecting patterns is usually easier when we use the power of the human visual system. Humans are very good in finding visual patterns. Looking for patterns is almost visceral for us. We can’t help but looking for patterns. This almost instinctive human urge can also be misleading and suggesting patterns to us where there are in fact none.\nIn data exploration we can make use of the power of visualization by plotting data and looking at them graphically. The modern computer has made this form of displaying data particularly easy and powerful and visualizing data in data exploration is another core skill you are going to learn in this course.\nSo let us visualize the world poverty data. In our plot we draw the year on the x-axes and the share of people living in extreme poverty on the y-axes. This will give us a point for each year. To facilitate the spotting of a trend, we connect the annual observations by a line.\n\n\nCode\nlibrary(ggplot2)\n\np &lt;- ggplot(plot_data, aes(x = Year, y = Share)) + \n     geom_line() +\n     geom_point() +\n     xlab(\"\")\nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world.png\")\np\n\n\n\n\n\nFigure 1.1: Share of world population living in extreme poverty from 2000 - 2017\n\n\n\n\nVisualizing trends in world extreme poverty as an example of data exploration. In this case the data pattern reveals a stunning fact. Over almost two decades we can see a sharp fall in the share of extremely poor people when looked at from a global perspective.\nOf course when we drill down to the level of individual countries this trend will not look the same everywhere and there might be countries where the share has actually increased. But overall we have seen a breathtaking steady decline. This is good news.66 When you have access to the internet you can have a closer look at these data at the very interesting website “our world in data” maintained by a consortium of Oxford University and University College London. See https://ourworldindata.org/extreme-poverty. The website has many interesting visualizations and options to select individual countries, country aggregates and make other selections of the data.\nBut does this trend mean that extreme poverty must disappear some years down the road? No, because nobody can tell whether the trend of the last two decades will go on also in the future.\nStatistics can help us to think more systematically about patterns and in making systematic guesses how a pattern might continue in the future. This is another core skill you will learn in this course: Making predictions, which means using available data and information to make informed guesses about data and information we do not have.\nLet us go back to the share of people in the world living in extreme poverty. As reported in our data the last actual observation for the global share in extreme poverty from the world bank is from 2017. There are more recent data for some regions but the global data since then are by now forecasts based on statistical techniques. The basic ideas of these techniques and how to apply them to data is a core skill you will learn in this course.\nNow what does the World bank predict for the share of extreme poverty in the world? Let us look at the data again graphically to visualize the prediction.\n\n\nCode\nobsdat &lt;- data.frame(Year = plot_data$Year, Scenario = rep(\"Observed\", 18), Share = plot_data$Share)\n\nadd_dat &lt;- data.frame(Year = 2018, Scenario = \"Observed\", Share = 8.6)\n\npred_dat_precovid &lt;- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Pre-Covid-19\", 4), Share = c(8.6, 8.4, 7.9, 7.5))\n\npred_dat_covidbase &lt;- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Covid-19-Baseline\", 4), Share = c(8.6, 8.4, 9.1, 8.9))\n\npred_dat_coviddown &lt;- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Covid-19-Downside\", 4), Share = c(8.6, 8.4, 9.4, 9.4))\n\ndat &lt;- rbind(obsdat, add_dat, pred_dat_precovid, pred_dat_covidbase, pred_dat_coviddown)\n\np &lt;- ggplot(dat, aes(x = Year, y = Share, group = Scenario, color = Scenario)) + \n  geom_line(alpha = 0.5)+\n  geom_point()+\n  scale_color_manual(values=c('green', 'red', 'black', 'blue'))\nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world-forecast.png\")\np\n\n\n\n\n\nFigure 1.2: Share of world population living in extreme poverty from 2000 - 2017 with predictions until 2021\n\n\n\n\nObserve that the line showing the share of extreme poverty in the world takes different distinct future paths as we make predictions. What does this mean?\nAt the root of the predictions is an abstract model7, how the share of poverty changes over time. If the underlying data would correspond to a world before Covid the falling trend in poverty would just continue to fall, as it has done continuously from 2000 onward. In the graph you can see this scenario if you follow the black and the blue line. But taking the pandemic and the consequences into account the prediction of the World Bank is that extreme poverty after a almost two decades downward trend will rise again. This you can see by following the green and the red line. How much, this rise actually will be in the end depends on data we can not yet know.7 If you have difficulties now to imagine what this means, don’t worry. We will learn in detail what a model is and how it can be interpret. In practice a model is usually an equation which provides a low-dimensional summary of a dataset. This summary is then used to make predictions.\nWhen we make informed guesses based on observed data on information we do not yet have there is uncertainty involved. Using the theory of probability in combination with statistics we can quantify this uncertainty. Quantifying the uncertainty attached to predictions is the third basic skill you will learn in this course.\n\n\n1.1.4 Does taking your time in college pay off?\nA newspaper in Germany reported that the more semesters needed to complete an academic program at the university the greater the starting salary in the first year of a job. The report was based on a study that used a random sample8 of 24 people who had recently completed an academic program.8 We will later in the course learn in detail what a random sample is. For the moment imagine that there is a mechanism which allows to select these 24 students at random from the large population of all university students in Germany. When a sample is random, every member in the sample has the same probability of beeing chosen from the population.\nInformation was collected on the number of semesters each person in the sample needed to complete the program and the starting salary, in thousand Euros, at the beginning of the job.\nThe data are shown in the following plot\n\n\nCode\ndat &lt;- read.csv(\"data/college_years_salaries/coll_sal.csv\")\n\nlibrary(ggplot2)\n\np &lt;- ggplot(dat, aes(x=Time, y=Salary)) +\n     geom_point()\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm and the starting salary in the first year in the job.\n\n\n\n\nWhat you see in this picture is a so called scatter-plot. It takes the data and plots all pairs of time in semesters needed to complete the academic university program and the starting salary in the job, where the first value is shown on the x-axis and the second on the y-axes. The points you draw like this are “scattered” all over the place, but it seems that “by and large” there is also some trend - shown as a blue line - like this:\n\n\nCode\np &lt;- ggplot(dat, aes(x=Time, y=Salary)) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = FALSE)\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary_with_trend.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm and the starting salary in the first year in the job with a linear trend fitted to data.\n\n\n\n\nLooking at the data, does this plot support the claim of the Newspaper?\nApparently the journalist writing the article saw a pattern, shown here as the blue line, which suggests that on average the salaries are really increasing with the semesters spent at the university. But is this pattern plausible? What do you think?\nAn independent researcher, who doubted the result, received the data from the newspaper and did a new analysis by separating the data into three groups based on the major of each person.\n\n\nCode\np &lt;- ggplot(dat, aes(x=Time, y=Salary, group = Major, color = Major)) +\n     geom_point()\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary_by_major.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm by major and the starting salary in the first year in the job.\n\n\n\n\nNow, looking at this plot, describe the relation for students with a major in business. How could the newspaper report be modified to describe the data?\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nThis example works particularly well for involving students. I am not quiet sure how to build this interactive element in the online unit. maybe you have an idea.\n\n\n\nYou see in this example that looking for patterns is not as easy as it seems. Again we see that there are no automatism. You will need the skills you learn in this course to gain competence in distinguishing actual patterns from spurious ones.\nWhy did I go through all these example with you in the beginning, pinning down the share of renewable energy in the electricity production in Kenya, estimating the number of trees on the planet, long term trends in world extreme poverty, and the relation between study time and beginning salaries for students in Germany? All these examples illustrate some special skills you will learn in this course. Hopefully it also convinced most of you that statistics and working with data is an exciting field and a way to engage with real world issues.\nSo these are the three core skills you will learn in this course:"
  },
  {
    "objectID": "introduction.html#the-three-basic-skills-you-will-learn-in-this-course",
    "href": "introduction.html#the-three-basic-skills-you-will-learn-in-this-course",
    "title": "1  Introduction",
    "section": "1.2 The three basic skills you will learn in this course",
    "text": "1.2 The three basic skills you will learn in this course\n\n\n\n\n\n\nThe three basic skills you will learn in this course\n\n\n\nAfter successfully completing this course students you will have learned three core skills:\n\nData exploration or finding patterns in data and information through visualisation and computation.\nMaking predictions, which means using available data and information to make informed guesses about data and information we do not have.\nTo quantify the uncertainty we have to attach to our predictions.\n\n\n\nThe course is split into 8 units in total. Units 1, 2 and 3 will be mostly be concerned with data exploration and with making comparisons based on data. You will also learn step by step how you can use the computer for data exploration and visualization. We assume no prior knowledge and start from scratch. We also do not strive for completeness. The idea is that you acquire the practically most important skills and get maturity to drill deeper for yourself after the course either in your further studies or on the job.\nUnit 4 and unit 5 will be predominantly be concerned with models and using models for prediction. Here you will learn how to spot trends in data, how you can discern actual information in data, so called signals, from random variation, called noise.\nUnits 6, 7 and 8 will focus on how to quantify uncertainty related to prediction and inference from data. Here is the place where probability theory combines with statistics to provide analytically and practically powerful tools which ground data analysis in a firm scientific foundation. This is also the most challenging part of the course and it will require lots of practice and participation from you to acquire this important skill.\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nIn your template you suggest that in the introduction I give a minute overview of the details of topics learned in the course. I would prefer to abstain from this, because it contains lot of terms the students will learn step by step and it is probably boring at this stage. I would rather prefer the big picture approach outlined here with the three basic skills as the guiding posts. If listing the details is a must, it would look roughly like this:\n\n\n\n\n\n\nUnit 1: Overview; Categorical Data and Proportions\n\n\n\nIn unit 1 we will give an overview of the course and we begin with the analysis and understanding of binary variables, variables that can be imagined as simple yes or no questions and how they can be summarized as proportions or percentages. You will learn about how the idea of expected frequency will promote the understanding of the meaning of these shares and how this provides a basic understanding of the importance of these numbers.\n\n\n\n\n\n\n\n\nUnit 2: Summarizing and Communication lots of data; From limited data to populations\n\n\n\nIn unit 2 we learn how to deal with lots of data, the typical situation we will face when we do statistics. When there are lots of data we need instruments and tools to summarize them and to get an overview. This overview is usually also very important for communicating the data and the information they might contain. We will also learn how we can use statistics to learn properties about large populations by only making limited observations on some appropriately chosen subset of individuals from this population. The gold standard in making such inferences possible is the concept of a so called random sample. But at each step of the sampling procedure bias can crop up and invalidate our results leading to wrong conclusions. The circumstances under which inference from samples to populations can be made and how we can make sure to minimize bias we need a firm understanding of the opportunities and limits of this important technique.\n\n\n\n\n\n\n\n\nUnit 3: What causes what?\n\n\n\nIn unit 3 we discuss when data analysis allows us to say something about what causes what. We learn about the important concept of randomized trials. We will also learn what an observational study is and how it differs from a randomized trial. This is an important concept we will learn through the discussion of real world examples.\n\n\n\n\n\n\n\n\nUnit 4: Modelling relationships using regression and algorithmic predictions\n\n\n\nIn unit 4 we will learn a key concept needed to make predictions. The technical term for this concept in statistics is regression. It is a simple mathematical model describing how a set of explanatory variables varies systematically with a response variable. We will learn to construct such models and to interpret them correctly. We will also cover related techniques which have become very important recently and entered the statistical toolbox from the field of computer science. These methods are known under the notion of algorithmic prediction or machine learning.\n\n\n\n\n\n\n\n\nUnit 5: How sure can we be about what is going on: Estimates and Intervals.\n\n\n\nIn unit 5 we encounter the first time tools for quantifying uncertainty. We learn how to determine and use uncertainty intervals by using a technique which is called the bootstrap. Being able to determine such intervals is extremely important in communicating statistics and for supporting a systematic and sound answer to the question: How sure can we be about an estimate.\n\n\n\n\n\n\n\n\nUnit 6: Probability: Quantifying uncertainty and variability\n\n\n\nIn unit 6 we deepen the knowledge how to quantify uncertainty by introducing basic ideas of probability theory. Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable.\n\n\n\n\n\n\n\n\nUnit 7: Putting Probability and Statistics together\n\n\n\nIn unit 7 we put statistics and probability theory together. This allows us to both simplify ideas and techniques how to quantify uncertainty. The combination of the two field makes the tools for quantifying uncertainty at the same time more powerful. Combining statistics and probability theory is at the heart of statistics as a science. It makes all the ideas developed in unit 1 to 6 very versatile and powerful.\n\n\n\n\n\n\n\n\nUnit 8: Answering questions and claiming discoveries\n\n\n\nIn unit 8 we learn how to leverage the knowledge of this course to answer questions and to claim discoveries. You will learn how statistics is used in the sciences and how it supports to develop our knowledge of the world. It pulls many ideas of the whole course together and when you have mastered this unit you have mastered all the basic skills we want to develop in this course, data exploration, prediction and quantifying uncertainty."
  },
  {
    "objectID": "introduction.html#on-the-use-of-the-computer",
    "href": "introduction.html#on-the-use-of-the-computer",
    "title": "1  Introduction",
    "section": "1.3 On the use of the computer",
    "text": "1.3 On the use of the computer\nOur approach to teach you basic ideas of statistics and data analysis will be very much problem and activity oriented. The application of specific statistical techniques will be only one component in a whole package of activities you will need to engage in when you work with data in real world applications. Preparing data appropriately for analysis as well as communicating the conclusions for your analysis will be important elements of the whole process of statistical analysis. Today these activities involve using a computer.\nIn this course you will also learn how to use the computer. It will play an important role for developing your skills. We will make no assumptions of prior knowledge of computers and programming and will introduce the use of the computer step by step.\nUsing a computer requires a language in which we can tell the computer what to do and which the computer can understand. The language of our choice for this course is called R. It is one of the most widely used and most powerful languages for data analysis and statistics. We will introduce you to the language and its use step by step as we go along and in parallel with the statistical concepts we develop."
  },
  {
    "objectID": "introduction.html#activities-in-the-study-center",
    "href": "introduction.html#activities-in-the-study-center",
    "title": "1  Introduction",
    "section": "1.4 Activities in the study center",
    "text": "1.4 Activities in the study center\n\n1.4.1 Visualizing the share of extremely poor people for different countries\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nHere we assume that students have a running R and R-Studio or R with Jupyter Notebook or Jupyter Lab installation on their laptops at the study center. We would give the code in a notebook with the code chunk shown in the source file, to play with.\nWhile we need not pin down all the details of the computational infrastructure yet, we need a discussion how to integrate the computer instructions into the course and the material I have to prepare for that. I would very much prefer a minimalist solution with R and some kind of notebook but not more.\n\n\n\nLets go back to figure Figure 1.1 for a moment. This figure has been created by the use of the computer.\nIn the following box you see the computer code that has read the data from a file and then plotted the share for the world and for a particular country, say China. Don’t worry if you do not (yet) understand the details of the code. Think of it as a language that tells the computer what to do with the data. You can edit the code and delete China and insert another country instead. If you click the green arrow at the upper right corner of the box the computer will run the code again and generate a new graphic.\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nThe details of this will of course depend on the kind of notebook we use. We can use in principle three options. Option 1: Have an installation of Rstudio (https://www.rstudio.com/). RStudio is the most popular (and free) IDE for development of R code. In R Studio we can open quarto notebooks (qdm) and run code interactively there. Pro: Works seamlessly with my notes and enhances production efficiency. Con: You need to explain students the IDE in addition to R, though no big deal it is an additional complication. Option 2: Jupyter Notebook with an R kernel (ipynp). Pro: If JWE decides to work more with Jupyter notebooks on a broader base, seamless integration for this strategy. Con: same as with R studio and just a tiny inch more awkward to produce for me. Option 3: Work in the R console only. Pro: No IDE or notebook. Con: perhaps a bit too difficult to use for students without Computer Science background.\n\n\n\nTry to play and experiment with the code in this way to see what happens. Soon you will know yourself how to make interesting and beautiful data visualizations yourself.\n\n\nCode\nlibrary(ggplot2)\n\ndemo_data &lt;- read.csv(\"data/extreme_poverty/share-of-population-in-extreme-poverty.csv\")\n\nnames(demo_data) &lt;- c(\"Country\", \"Code\", \"Year\", \"Share\")\n\npl_dat &lt;- with(demo_data, demo_data[Country %in% c(\"World\", \"China\") & Year &gt;= 2000, ])\n\np &lt;- ggplot(pl_dat, aes(x = Year, y = Share, color = Country)) +\n     geom_point() +\n     geom_line() +\n     xlab(\"\")\nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world-china.png\")\n\n\nSaving 7 x 5 in image\n\n\nCode\np\n\n\n\n\n\n\n\n1.4.2 Guessing ages\nThis is an exercise which needs the leadership of an instructor at the study center. It would be great to collect the data of the exercise in a readable file for later use in the course.\nWe have 10 photos of persons whose age is known to us but not to the students. We divide students into 10 groups A through J. Each group gets one of the photos. The students in each group are asked to estimate the age of the person in their photograph and write the guess into a form. Each group must come up with a single estimate.\nExplain that each group will be estimating the ages of all 10 photos and that groups are competing to get the lowest error. Each group passes its card to the next group (A to B, B to C, etc. J back to A) and estimates the age of the new photo. This is continued until each group has seen all photos.\nThe data are written in a table where the rows are groups and the columns are card numbers. We can discuss the expected accuracy of the guesses (within how many years do you think you can guess). Then start with card 1 and ask groups to give their guess, then reveal the true age and write it at the bottom margin of the first column.\nIntroduce the concept of error - guessed age minus actual age - and wrote the errors in place of the guessed ages. Then fill the whole table. Ask the students to compute the absolute average error.\nStudents get an idea about: uncertainty, empirical analysis and data display. There are many statistical ideas in this game and the data can be taken up throughout the course (variance, bias, experimental design, randomization, linear regression, two-way tables, statistical significance).\nTo illustrate the idea more precisely:\nFor each card your group is given, estimate the age of the person on the card and write your guess on the table below in the row corresponding to this numbered card. Later students are told the true ages and they can compute the error. The error is defined as estimated minus actual age.\n\nExample for group card, guessing ages\n\n\nCard\nEstimated\nActual\nError\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n5\n\n\n\n\n\n6\n\n\n\n\n\n7\n\n\n\n\n\n8\n\n\n\n\n\n9\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n1.4.3 Collect data from students.\nOne data collection exercise, which might be fun here is to collect the height and hand span from students in the class. Later when we teach regression we can compare to the data Pearson collected on university students over 100 years ago.\n\n\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nRosling, Hans, Ola Rosling, and Anna Rosling-Rönnlund. 2018. Factfullness, 10 Reasons Why We Are Wrong about the World - and Why Things Are Better Than You Think. Sceptre.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "categorical_data_and_proportions.html#communicating-counts-and-proportions",
    "href": "categorical_data_and_proportions.html#communicating-counts-and-proportions",
    "title": "2  Categorical Data and Proportions",
    "section": "2.1 Communicating Counts and Proportions",
    "text": "2.1 Communicating Counts and Proportions\nLet us follow the history of infant mortality around the globe. When we looked at the year 1860 rates of infant mortality were very high, not much better than it had been all the centuries before industrialization has set in in Europe.\nBut then about 100 years later we already see a significant reduction in the Western countries, with rates reduced still much further until now in the most affluent countries. Let us look at the country group for which we had data in 1860 already, again today in 2020, before we go to the global picture. We will use this example to discuss some important aspects of communication counts and proportions.\n\n\nCode\n# Select the country group used in the historical 1860 data\n\nrate_2020 &lt;- infant_mortality[infant_mortality$Country %in% rate_1860$Country & infant_mortality$Year == 2020, c(\"Country\", \"Continent\", \"Mortality\")]\n\n# display the table and write text file of table\n\nlibrary(knitr)\nkable(rate_2020, digits = 4, row.names = FALSE)\n\n\n\n\nTable 2.2: Infant mortality in some European countries in 2020.\n\n\nCountry\nContinent\nMortality\n\n\n\n\nAustria\nEurope\n0.0030\n\n\nBelgium\nEurope\n0.0034\n\n\nDenmark\nEurope\n0.0031\n\n\nFrance\nEurope\n0.0034\n\n\nGermany\nEurope\n0.0031\n\n\nNorway\nEurope\n0.0018\n\n\nSpain\nEurope\n0.0027\n\n\nSweden\nEurope\n0.0021\n\n\n\n\n\n\nCode\nwrite.csv(rate_2020, file = \"tables/table_2_2_infant_mortality_2020.csv\", row.names = FALSE)\n\n\nThis is a spectacular improvement for this group of countries. By 2020 we have much more data, covering more regions and we will follow the global story later. But let us stick for the moment with this group of European Countries, which we followed over a time span of 160 year.\nWithin a bit more than a century the mortality rate has been reduced from 30 % to below 0.3 %. This amounts to a reduction by a factor of about 100. Isn’t this a stunning achievement?\n\n\n\n\n\n\nNow you try\n\n\n\nThis is a good opportunity to practice and refresh your skills in manipulating percentages. Compute and report the reduction in the share of infant mortality for each of the 8 countries between 1860 and 2020 based on the numbers reported in Table 2.1 and Table 2.2\n\n\nIn a recent book (Smil 2020), the Canadian researcher Vaclav Smil has pointed out that such low rates are impossible without the combination of a number of critical conditions, such as good healthcare in general, appropriate prenatal, perinatal and neonatal5 care, proper maternal and infant nutrition, adequate sanitary living conditions as well as access to social support for disadvantages families. All of these factors require relevant government and private spending and on infrastructures that can be universally used and accessed. Infant mortality is thus a very powerful indicator of quality of life in a country.5 prenatal is a word rooted in ancient Latin and means before birth. Perinatal means around the time of birth and neonatal means the first month after birth of a child.\nWhen data, such as counts and proportions are reported in a table we should make careful considerations how the data are precisely presented.\nFor instance in the table about infant mortality in some European countries in 1860, we could have reported the same information by presenting survival rates instead of mortality rates. Such a choice in reporting is generally known as framing.\nFraming can have effects on the impact of communication. Depending on how you frame the communication of data, the same information might affect and engage your audience differently. The same table with survival rates instead of mortality rates would the look like this.\n\n\nCode\nsrate_1860 &lt;- rate_1860\nsrate_1860$Survival &lt;- 1 - rate_1860$Mortality\n\n# table for 1860 survival rates\n\nsrate_1860 &lt;- with(srate_1860, srate_1860[ , c(\"Country\", \"Continent\", \"Survival\")])\n\nlibrary(knitr)\nkable(srate_1860, digits = 4, row.names = FALSE)\n\n\n\n\nTable 2.3: Infant survival rates in some European countries in 1860\n\n\nCountry\nContinent\nSurvival\n\n\n\n\nAustria\nEurope\n0.763\n\n\nBelgium\nEurope\n0.861\n\n\nDenmark\nEurope\n0.864\n\n\nFrance\nEurope\n0.850\n\n\nGermany\nEurope\n0.740\n\n\nNorway\nEurope\n0.898\n\n\nSpain\nEurope\n0.826\n\n\nSweden\nEurope\n0.876\n\n\n\n\n\n\nCode\nwrite.csv(srate_1860, file = \"tables/table_2_3_infant_survival_1860.csv\", row.names = FALSE)\n\n\nTake for instance the data for Germany 1860. If we had reported a survival rate of 74 % it might sound to many better than if we had reported the equivalent information of a mortality rate of 26 %. So whenever you report counts or proportions be mindful of framing effects.\nRisk impression can often be made more clear if we report expected frequencies as well\nas percentages. In this way the risk can be imagined as an actual crowd of people. For example we could visualize the infant mortality rate in Germany by creating a picture of the mortality rate, the actual number of cases per 1000 life births. If there are relatively few cases, as in 2020 Germany, this arbitrary normalisation leads to an artificial number such as 3.1 deaths per 1000 life births. Of course this is an artifact of the normalisation because there is nothing as an event with 0.1 deaths. If we had normalized to 10000 this would amount to 31 in 10000 which sounds more like an actual count. So for the visualization we take an infant mortality rate as infant deaths per 1000 life births to be 3.\nSuch a visualization could then look for example like this:\n\n\nCode\nlibrary(waffle)\nwaffle(c(\"Death\" = 260, \"Survived\" = 740), rows = 20, colors = c(\"#FD6F6F\", \"#93FB98\"), use_glyph = \"child\", glyph_size = 4, equal = F)\n\n\n\n\n\nIn 1860 in Germany among 1000 newborns 260 infants died in the first year of their life.\n\n\n\n\nHere you visualize the numbers as differently colored crowds. There are 20 rows with 50 people symbols each. This multiplies to a crowd of 1000 people. It makes the magnitude of the mortality rate (as well as the survival rate) tangible. The share of infant death per 1000 life births in Germany in 1860 becomes now more tangible than reporting just a percentage in a table.\nWhen you plot the same graph for the data of 2020 the enormous improvement that took place within a time span of one and a half centuries in Germany become perhaps more obvious than just looking at the rates alone. The same kind of visualization for the 2020 data would then look like this\n\n\nCode\nlibrary(waffle)\nwaffle(c(\"Death\" = 3, \"Survived\" = 997), rows = 20, colors = c(\"#FD6F6F\", \"#93FB98\"), use_glyph = \"child\", glyph_size = 4, equal = F)\n\n\n\n\n\nIn 2020 in Germany among 1000 newborns 3 infants died in the first year of their life.\n\n\n\n\nFrom this perspective the improvement in infant mortality rates in Germany are truly spectacular.\nStill even at the low mortality rate this can mean a huge amount of individual tragedies. In 2020 Germany had 760.378 births. 0.31 % of this amounts to 2357 individual tragedies. Even if this rate could be brought down further, say to 0.28 % which sounds like a tiny improvement, it would mean 228 infant lives that could be saved.\n\n\n\n\n\n\nNow you try\n\n\n\nDavid Spiegelhalter (Spiegelhalter 2019) whose book we have encountered when we discussed the question of how many trees there are on the planet, describes an advertisement in the London Underground, saying that 99 % of young Londoners do not commit serious crimes. The add was presumably intended to reassure passengers that riding on the London Underground is very safe. Try to imagine what this statement would mean when you think about this information in terms of crowds of young Londoners, assuming that “young” means between 15 and 25. Try to exlpain how the same information presented differently may have a different impact. What kind of communication tools we have just heard about, have been applied here?\n\n\n\n\n\n\n\n\nSeitwerk: A remark on answer.\n\n\n\n\n\nTurn a positive frame into a negative one and then imagine actual crowds of people instead of percentages. This or an answer like this I would have in mind that studnets come up with if they reflect this example. I think an answer to such a reflection is best placed in the interactive part of the course or in the study center meet ups.\n\n\n\n\n\n\n\n\n\nBe mindful of framing\n\n\n\nA standard we should strive for when reporting data is providing impartial information. For this we should think carefully about our framing and should perhaps provide both positive and negative frames.\n\n\nBut even if we achieve this, we should consider other things, like the ordering of the rows. Let us discuss this aspect in the next session together with your first steps in R."
  },
  {
    "objectID": "categorical_data_and_proportions.html#a-first-acquaintance-with-r-visualizing-infant-mortality-rates",
    "href": "categorical_data_and_proportions.html#a-first-acquaintance-with-r-visualizing-infant-mortality-rates",
    "title": "2  Categorical Data and Proportions",
    "section": "2.2 A first acquaintance with R: Visualizing infant mortality rates",
    "text": "2.2 A first acquaintance with R: Visualizing infant mortality rates\nWhile a table might be the medium of choice for displaying data it is often more powerful to convey information visually. The example of how imagining actual crowds might give us sometimes a more tangible picture of the same information, was one illustration of this.\nToday when we visualize data as one important tool of data exploration, the tool of choice is a computer and an appropriate language that enables us to tell the computer what to do.\nFor us in this course this will be the R language and now we start to learn the first steps in this language.\n\n2.2.1 Starting an quitting R\nWe assume that the computer you work with has a recent version of R installed. R will work with the most common operating systems such as Windows, OSX or Linux.66 Installing R yourself is not complicated. To install R yourself you need a computer where you have the privileges to install software and you need an internet connection. You get the newest version of R at the website https://cran.r-project.org/. A screenshot of the website is here \nWe will use R in two ways: First from the R command line and then also by working with Jupyter Notebooks.77 Another very powerful and popular program to work with R and R code is RStudio, which you can find here https://www.rstudio.com/. RStudio is a so called IDE, an integrated development environment. If you have access to a computer with internet connection and the permissions to install software yourself, feel free to experiment also with this tool. The basic version is free of charge. Again you need a computer where you have the privileges to install software and you need an internet connection.\nThe notebooks, which we already encountered in an example in the introduction, will not only allow to write R code and send this code to R for execution. Notebooks also allows you to store commands, to comment them and make them available as files for later use. This is especially useful, once you work on longer and more complicated tasks. It is then essential that you can reproduce what you did in the past days, that you can write easily readable comments and that you can collaborate in a team, where you can share your work with others. This circle of collaborators includes your future and past self.\nFor a start we just work with the R command line or the R-console, as it is called, introducing the use of R via the notebooks a bit later.\nYou start R by either typing R into the terminal or by clicking the R icon on your computer. The R console shows a prompt, a symbol that looks like this &gt;. When you see the prompt in the R console, R is ready to receive commands.\nTo end your R session write quit() at the prompt &gt; of the console. Congratulations. Now that you can start and quit R we are ready to go.\n\n\n\n\n\n\nSeitwerk: A remark on a demo.\n\n\n\n\n\nThis would be perhaps best shown in a brief video. Of course when producing the video it is important to think about which operating system the students will use, and the R version that they will have available. If this is open, so the demo for windows, mac and linux. It does only take marginally more time and effort. Maybe this whole intro is perfect for a video demo.\n\n\n\n\n\n2.2.2 First steps\nHere is an easy command you can send to R. Just try to type 1 + 1 at the prompt\n\n1 + 1\n\n[1] 2\n\n\nSure enough, R gives you the result of the addition, which is 2. But what is [1]? This is just a row label. If there were more outputs to your command, then they would be labelled [2], [3], [4], and more. We will go into this aspect of R’s output display later in more detail.\nSo R can do all the usual computations. For instance if you knew that in Germany in 1860 there were 270 infant death per 1000 life birth you could compute the mortality rate in a decimal format by dividing the counts by 1000, because the convention is to report mortality as cases per 1000 life briths. The R command for division is /. So by typing\n\n270/1000\n\n[1] 0.27\n\n\nyou will get the mortality rate 0.27 or 27 %.\nYou can do all the usual arithmetic operations, like with a calculator in R. For instance subtraction\n\n3-2\n\n[1] 1\n\n\nOr multiplication\n\n10*10\n\n[1] 100\n\n\nYou can raise a number to the power of another, like\n\n3^2\n\n[1] 9\n\n\nand of course you can combine all of these operations:\n\n(1+3)^2 - 5*4 + 12^3 - (13/2)\n\n[1] 1717.5\n\n\nThe comma is represented in R as a dot .. So the above output reads 1717and one half or 0.5.\n\n\n\n\n\n\nNow you try\n\n\n\nUse R to transform the Mortality numbers reported in percent in Table 2.1 and Table 2.2 in mortality rates, i.e. the (approximate) number of infant-deaths per 1000 life births.\n\n\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance:\n5*\nIn this case R will show the expression 5* followed by a + instead of showing a new prompt. This means that the expression is incomplete. When R shows + after entering a command instead of the output and a new prompt, it means that it expects more input. If we complete the expression, the expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Errors are usually printed in red, and it instinctively might create a feeling of alarm. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n\n5%3\n\nError: &lt;text&gt;:1:2: unexpected input\n1: 5%3\n     ^\n\n\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, which is particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity.\nWith this knowledge we can already do a first example, by continuing our discussion on infant mortality data we started in this lecture. On the way we learn a few more things about R and the R language.\n\n\n2.2.3 Storing and reusing results\nWhen our operations become just a bit more complex than just typing in a simple arithmetic operation, it becomes useful if we can store answers and use these answers, which might be an intermediate result of some transformed data or something else. In R this problem is solved very easily. We assign the answer to a name we choose ourselves. Here is an example.\n\na &lt;- 1+1\n\nThe symbol &lt;- tells R pleas assign to the name a the result of the computation 1+1.88 The assignment operator is used so often that it is useful to type it using a keyboard shortcut instead of typing first &lt;and then -. The same result can be achieved by pressing the ALTkey followed by the - key. \nNow see why we have said that the assignement has stored your result. Enter on your keyboard the name you have just chosen:\n\na\n\n[1] 2\n\n\nAnd you can use the stored value to do further computations with it, like\n\na^2\n\n[1] 4\n\n\nWhen you assign a new value to the old name a, the old value will be overwritten by the new value.\nWhat names should you use? You could uses actually anything but you have to follow a few rules. A name in R must for example not begin with a number, a dot . or an underscore _. So for example var_1, var.1, var1 and VAR1 and myVar1 are all allowed names but 1var, .var and _var1 are not\n\n\n2.2.4 A first data visualization\nWe showed the infant mortality rates of a group of European countries before in Table 2.1 and Table 2.2.\nHumans are very visual creatures. Thus using our visual system to explore data and absorb information in these data visualizations can be very powerful. To deploy their power, we must - however - follow some principles, which we will learn step by step over this course.\nAssume we would like to display the information in our tables in a so called bar chart. A bar chart would combine in a plot bars for each country in the table with the bar length proportional to the mortality rate. This will give us a visual impression how the countries differ in one view, which might be more informative as just looking at the numbers themselves in a table.\n\n2.2.4.1 Functions\nR is not just a calculator and data storage device. What makes R very powerful is that it comes equipped with many functions which we can use to do things with data, like for instance producing plots like a bar chart.\nFunctions in R have a name followed by parenthesis. In the very first step we typed for example quit() at the prompt. This is a function and by typing its name followed by the parenthesis, R knows that it has to close the program and shut down.\nFunctions can also have arguments, which we can assign certain values to. For example, R has a function which would round numbers. This function is called round(). It has also arguments. You need to tell R which numbers to round and the number of digits the rounding should consider.\n\nround( x = 2.4356789123456, digits = 2)\n\n[1] 2.44\n\n\nThe first argument in the function round is x. We can give x a value, which we assign by =. The second argument is called digits and we assign to it the value 2. The output is then, not very surprisingly, 2.44.\nNote that R is programmed such that we could also have typed:\n\nround(2.4356789123456, 2)\n\n[1] 2.44\n\n\nR would have known automatically that the first value is assigned to xand the second to digits.\nWe will encounter a lot of R functions during this course. We will also learn how to access R documentation to know for so many different functions, what is their name, which arguments they accept as input and how we can use them.\n\n\n2.2.4.2 Visualizing the infant mortality data\nR has a built in function for plotting bar charts, which is called barplot(). Let is make use of this function to show the infant mortality rates of 1860 as a bar chart. The arguments taken by R are the data. Then we can add additional arguments which determine details of the plot display and appearance.\nLet us first store the data in an object with the name mrfor mortality rates of 1860.\n\nmr_1860 &lt;- c(0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)\n\nHere we see another important function of R which we will need all of the time, the c()function. This function concatenates values in a vector of values. So the output of the operation will be a vector \\((0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)\\) with the name mr_1860. Thus when we type\n\nmr_1860\n\n[1] 0.237 0.139 0.136 0.150 0.260 0.102 0.174 0.124\n\n\nR will print the whole vector as one object. This is why the counting label is [1] and not [8]. The c() function concatenates the number so a single object, a vector containing all eight numbers.99 In case you are worried about the technical term of a vector, don’t worry. We use this term here losely and not in a rigorous mathematical sense. You can think for the moment of a vector in R as a single object that can hold several data at once, like numbers, or characters. I avoided the term list in the text to avoid confusion with the list data structure which is a special data structure in R which we learn about in the course later.\nNow lets see what happens when we give mr_1860 as an argument to barplot.\n\nbarplot(mr_1860)\n\n\n\n\nWe see on the y axis the infant mortality rates from 0 to 0.25 and on the x axis a bar for each country with a length proportional to the infant mortality rate in this country.\nBut here it is difficult to connect the bars to the countries. So let us store the country names in another vector and call them ctr\n\nctr &lt;- c(\"Austria\", \"Belgium\", \"Denmark\", \"France\", \n         \"Germany\", \"Norway\", \"Spain\", \"Sweden\")\n\nNote that the names of the countries had to be written between quotation marks \" \". This is the way to tell R that the sequence of letters are characters. Characters are a specific data type representing text. Now we have a vector of words, the country names. We can give the country names as an argument to barplot()like this:\n\nbarplot(mr_1860, names.arg = ctr)\n\n\n\n\nNot too bad. But some country names are missing. It seems that the width of the bars is not wide enough that R is able to print all names.\nIt would be more convenient to flip the chart around and interchange the x and the y axes here. This can be done by another argument to barplot(). This argument is called horiz and it assumes a logical value. A logical is another R data type which allows us to express whether something is true or false. Logical true and false values are expressed as TRUE and FALSE in R.\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE)\n\n\n\n\nThis does not yet help much, because this flip of coordinates can only support a better display of the data if the country names are also printed horizontally.\nYou might guess it already: This can be controlled by another argument which is called in the case of this function las. If las gets value 1 we get what we want.\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE, las = 1)\n\n\n\n\nNow we have visualized the information we had displayed in a table before. It is not yet perfect because the names of countries with longer names are cut off a bit. This could be fixed by additional function arguments, but let us not go too much in the details of the barplot()function at this stage. We will learn a lot about powerful visualization techniques in R as we go along.\n\n\n\n\n\n\nNow you try\n\n\n\nUse R to redo the barplot visualization we just did for the 1860 data for the 2020 data.\n\n\nBefore we close this first encounter with R and data visualization, let me point out an important aspect of bar charts. The visual impression is powerful and truthful, if we choose the origin of the bars carefully. It is usually the best idea to start the bars at zero. So we see clearly the relative lengths and the magnitude of differences in the context of the entire dataset.\nChanging the origin with not enough care can visually exaggerate the differences between countries. This is a manipulative visualization, which should be avoided but which is encountered often. So be mindful about the choice of origin in a bar chart.\nLet me show you what I mean by telling R, for example, to start the plot of the data for the 1880 data at \\(0.08\\) instead as of \\(0\\).\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE, las = 1, \n        xlim = c(0.08, 0.275), xpd = F)\n\n\n\n\nDo you see that now the differences appear bigger? The choice of origin can have a big influence on the appearance of differences between the length of the bars. Always be mindful of this effect and reflect what happens if for some reason you have to choose a different origin for the bar chart than zero. Alberto Cairo, who is the author of an influential book on data visualization (Cairo 2016) recommends to always choose a “…logical and meaningful baseline”.\n\n\n\n\n\n\nBe mindful in choosing a logical and meaningful origin for barcharts\n\n\n\nWhen you compare porportions visually with a bar chart always think of choosing a logical and meaningful origin. In most cases this will be 0. If 0 is not possible, think about what would be a choice that gives a truthfull and not exaggerated display of differences.\n\n\nBefore we close this digression into visualization, let me briefly discuss another aspect of data presentation which you need to consider. The ordering of the rows in the data table, or in this case, the order of the bars in the barchart has to be carefully considered.\nIf you look at the bar chart we have produced, you see that the bars have an order corresponding to the alphabetical order of the countries, starting with A for Austria and ending with S for Sweden.\nNow consider we had ordered the data according to mortality rate like this:\n\n\nCode\nbarplot(rate_1860[order(-rate_1860$Mortality) , ]$Mortality, \n        names.arg = rate_1860[order(-rate_1860$Mortality) , ]$Country, horiz = TRUE, las = 1)\n\n\n\n\n\nNow the bar chart could suggests that the infant mortality rate is an important and meaningful way of comparing this particular group of countries.\nSuch ranking comparisons are very popular in the media but they can be misleading. They can be misleading because the differences could be there just by chance.\nThere could be also systematic differences between countries affecting infant mortality rates. For example, countries that are small with populations under 10 Million and that have very homogeneous populations and low birth rates tend to show lower infant mortality rates just because of these demographic features. So an ordering like the one presented in the graph might suggest a ranking that is in fact spurious and is not really substantial.\n\n\n\n\n\n\nBe mindful about order in displaying the data\n\n\n\nWhen you display proportions in a table or a bar chart be mindful of the ordering of data and avoid spurious rankings. Choose a particular ordering only if there is a meaningful and logical reason to do so."
  },
  {
    "objectID": "categorical_data_and_proportions.html#categorical-variables-causes-of-infant-mortality",
    "href": "categorical_data_and_proportions.html#categorical-variables-causes-of-infant-mortality",
    "title": "2  Categorical Data and Proportions",
    "section": "2.3 Categorical variables: Causes of infant mortality",
    "text": "2.3 Categorical variables: Causes of infant mortality\nWe have discussed binary data. These are data that can take two values, like death or alive, in the examples of infant mortality data we have studied so far. A generalization of binary variables are called categorical variables in statistics. Categorical variables are measures that can take two or more values, which can be either unordered, such as eye color, countries or study center locations at which JWL courses take place. They can also be ordered, like positions in a hierarchy.\nAn example of categorical data arises if we study the issue of infant mortality further and ask for the causes. Why do infants die in the first year of their life?\nHere are the causes that have been registered by the Global Burden of Disease Study in 2019 by the Institute for Health Metrics and Evaluation as well as the share of each cause in the overall cases.1010 See https://www.healthdata.org/gbd/2019\n\n\nCode\n# read data from JWL package\n\nlibrary(JWL)\ncauses &lt;- infant_mortality_causes\n# order by share\ncauses_ordered &lt;- causes[order(-causes$Share) , ]\n\ndat &lt;- causes_ordered[, c(\"Entity\", \"Share\")]\nnames(dat) &lt;- c(\"Cause\", \"Share\")\n\nlibrary(knitr)\nkable(dat, digits = 3, row.names = FALSE)\n\n\n\n\n\nCause\nShare\n\n\n\n\nPreterm birth\n0.211\n\n\nEncephalopathy due to birth asphyxia and trauma\n0.180\n\n\nLower respiratory infections\n0.161\n\n\nBirth defects\n0.128\n\n\nDiarrheal diseases\n0.092\n\n\nHeart anomalies\n0.048\n\n\nMalaria\n0.044\n\n\nSyphilis\n0.026\n\n\nMeningitis\n0.020\n\n\nWhooping cough\n0.016\n\n\nNutritional deficiencies\n0.015\n\n\nDigestive anomalies\n0.013\n\n\nSudden infant death syndrome\n0.009\n\n\nTuberculosis\n0.008\n\n\nMeasles\n0.006\n\n\nHIV/AIDS\n0.006\n\n\nDigestive diseases\n0.005\n\n\nTetanus\n0.005\n\n\nEncephalitis\n0.003\n\n\nAcute hepatitis\n0.002\n\n\nDiabetes and kidney diseases\n0.002\n\n\n\n\n\nCode\nwrite.csv(dat, file = \"tables/table_2_4_infant_mortality_causes.csv\", row.names = FALSE)\n\n\n\n\n\n\n\n\nNow you try\n\n\n\nUse R to visualize this table in a barchart. Don’t worry if some of the causes are cut off at the left of the graph. We will learn during the course how to better control the appearance of a visualization and control for details like this.\n\n\n\n\n\n\n\n\nThis is how an answer can look like.\n\n\n\n\n\nSince students have not yet learned subsetting, they need to use the table and retype the values and the causes manually. This is intended because it will familiarize them with the c() function, the difference between numerical and character data type and is an excellent opportunity to review what we did on simple R bar-charts. In the example cause below I use - of course - the convenience of subsetting\n\n\nCode\nbarplot(dat$Share, \n        names.arg = dat$Cause, horiz = TRUE, las = 1, cex.names = 0.3)\n\n\n\n\n\n\n\n\nIn the news but also in many publications you will often find proportions and how they add up to a total represented in so called pie charts. A pie chart for the causes of infant mortality visualizing our data will look like this\n\n\nCode\npie(dat$Share, labels = dat$Cause, radius = 1, cex = 0.5)\n\n\n\n\n\nNow compare this to a display of the same information using the bar chart. If you have done the exercise before correctly, it will look like this\n\n\nCode\nbarplot(dat$Share, \n        names.arg = dat$Cause, horiz = TRUE, las = 1, cex.names = 0.5)\n\n\n\n\n\nApart from some imperfections of displaying the labels in a easily readable way, why does the bar chart work much better for comparing the proportions of causes than the pie chart?\nThe reason is that the bar chart is supported better by by our visual perception capacity. In the case of the bar chart we have to compare lengths, which our visual system can do well. For decoding the pie we have to compare areas of slices or angles formed by the slices at the circle center. But our visual system does not work well for such tasks. It works even worse, if the display is a three dimensioal pie chart, as offered by various apps. In this case the pie would look like this:\n\n\nCode\nlibrary(plotrix)\npie3D(dat$Share, labels = dat$Cause, radius = 1, labelcex = 0.5)\n\n\n\n\n\nNow it is even worse, because our visual system now has to additionally decode perspective on top of area, something it is naturally also not good at.\nThere are ways to visually display data that work because they are naturally supported by our ability of visual perception and cognition, while others are not.\n\n\n\n\n\n\nDon’t use pie charts to visually compare proportions\n\n\n\nWhen you display proportions choose bar charts and avoid pie charts. Bar charts are naturally supported by the ability of our visual system to compare lengths, while pie charts require the comparison of areas and angles, a task our visual system is not so good at."
  },
  {
    "objectID": "categorical_data_and_proportions.html#compairing-pairs-of-proportions",
    "href": "categorical_data_and_proportions.html#compairing-pairs-of-proportions",
    "title": "2  Categorical Data and Proportions",
    "section": "2.4 Compairing pairs of proportions",
    "text": "2.4 Compairing pairs of proportions\nOften proportions or percentages are used to communicate risks by comparing pairs of proportions.\nWhen pairs of proportions are compared, we need to understand how such comparisons can be made in a meaningful way. Especially in the media it is popular to often run spectacular headlines of how specific behaviors affect your likelihood of developing a disease.\nMuch of the spectacle in the headlines is due to the exclusive reporting of relative risks. It is important to understand that such statements need to be put into context by also reporting baseline of absolute risk.\nLet me illustrate the issue using the example of consuming processed meat and the risk of developing bowel cancer.\nFrom epidemiological research it is known that the chance of a person of developing bowel cancer is based on factors such as meat consumption, the level of physical activity, the body weight or income.\nThe relative risk is the risk of developing bowel cancer in a group of people compared to another group of people with different behaviors, different physical conditions of different environments.\nFor instance when we compare meat eaters versus vegetarians, a statement about relative risk would be that the consumption of processed meat increases the risk of developing bowel cancer by 18 %. This sounds spectacular but it does not tell the full story.\nTo see this go back to our previous example of imagining crowds of people instead of percentages.\n\n\nCode\nwaffle(c(2, 8), rows = 2, use_glyph = \"male\", \n       colors = c(\"#FD6F6F\", \"#93FB98\"), glyph_size = 16, \n       equal = F, legend_pos = \"\")\n\n\n\n\n\nHere we have an absolute risk of two out of 10 persons developing bowel cancer. If the relative risk would increase by 50 % this would mean in terms of absolute risk that now we have\n\n\nCode\nwaffle(c(2, 1, 7), rows = 2, use_glyph = \"male\", \n       colors = c(\"#FD6F6F\", \"#3A9ABD\", \"#93FB98\"), glyph_size = 16, equal = F, legend_pos = \"\", size = 0.2)\n\n\n\n\n\nThe risk increases now to three out of 10.\n\n\n\n\n\n\nNow you try\n\n\n\nAssume the absolute risk is 4 out of 10 and the relative risk increase is 50 %. How many out of 10 are now at risk?\n\n\nThese examples illustrate that you need baseline or absolute risks as a vital information to understand what an 18 % increase in risk really means.\nGoing back to the example of processed meat consumption and bowel cancer risk, the estimated lifetime risk of developing bowel cancer is 5.6 %. Expressed as a relative frequency this is about 6 out of 100 or in a chart\n\n\nCode\nwaffle(c(6, 94), rows = 5, use_glyph = \"male\", \n       colors = c(\"#FD6F6F\", \"#93FB98\"), glyph_size = 8, \n       equal = F, legend_pos = \"\")\n\n\n\n\n\nThe estimated life time risk of developing bowel cancer if you eat 50 g of processed meat per day increases your relative risk by 18 %. At this heigthened risk level the new absolute risk is now 6.6 % or about 7 out of 100\n\n\nCode\nwaffle(c(6, 1, 93), rows = 5, use_glyph = \"male\", \n       colors = c(\"#FD6F6F\", \"#3A9ABD\", \"#93FB98\"), glyph_size = 8, \n       equal = F, legend_pos = \"\")\n\n\n\n\n\nIn the research literature proportions are often expressed by the odds ratio. This ratio expresses the chance of an event happening relative to the chance of an event not happening. In the example of processed meat and bowel cancer this would be \\(6/94\\), because 6 out of hundered people develop bowel cancer in their life time and 94 out of hundred do not. While odds are very common in the research literture they are not a very intuitive way to communicate the comparison of proportions. Spiegelhalter, from whom I took this example therefore recommends not to use odds ratios outside a scientific context, since it easily invites misunderstanding."
  },
  {
    "objectID": "categorical_data_and_proportions.html#activities-in-the-study-center",
    "href": "categorical_data_and_proportions.html#activities-in-the-study-center",
    "title": "2  Categorical Data and Proportions",
    "section": "2.5 Activities in the study center",
    "text": "2.5 Activities in the study center\n\n2.5.1 The story of infant mortality around the globe\nFor this activity we continue to look into the issue of infant mortality around the globe by working with a Jupyter notebook. To work on this assignment, please open the notebook infant_mortality_global.ipynb with the jupyter notebook. The notebook contains some text and some code which you can interactively execute from the notebook, as we already did it in the assignments in the Introduction, when we looked at the share of poverty in various countries. In this notebook we continue to dive deeper into the story of infant mortality around the globe. The things you learn when you work through this assignment will probably surprise you."
  },
  {
    "objectID": "categorical_data_and_proportions.html#a-data-collection-exercise-for-the-next-unit",
    "href": "categorical_data_and_proportions.html#a-data-collection-exercise-for-the-next-unit",
    "title": "2  Categorical Data and Proportions",
    "section": "2.6 A data collection exercise for the next unit",
    "text": "2.6 A data collection exercise for the next unit\nMaybe handedness score?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAntony, Volk, and Atkinson Jeremy. 2013. “Infant and Child Death in the Human Environment of Evolutionary Adaptation.” Evolution and Human Behavior 34: 182–92.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for Communication. New Riders.\n\n\nRoser, Max. 2019. “Child Mortality Is an Everyday Tragedy of Enormous Scale That Rarely Makes the Headlines.” https://ourworldindata.org/child-mortality-everyday-tragedy-no-headlines.\n\n\nSmil, Vaclav. 2020. Numbers Don’t Lie: 71 Things You Need to Know about the World. Penguin Books.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#understanding-variation-in-a-single-variable-using-histograms",
    "href": "summarizing_and_communicating_lots_of_data.html#understanding-variation-in-a-single-variable-using-histograms",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.1 Understanding variation in a single variable using histograms",
    "text": "3.1 Understanding variation in a single variable using histograms\n\n3.1.1 Constructing a Histogram\nTo summarize data, statisticians often use a graph which is called a histogram. In this section we will discuss all you have to know about histograms and how to use them. Let us start by an example, where we have about 100 data points, which is a lot but not that large that we can not handle them by hand.\nThe data we want to look at come from measurements of the annual flow of the river Nile at Aswan in Egypt from 1871 to 1970. The units of these measurement in which the annual flow is recorded are 100 millions of cubic meters, i.e. \\(10^8 m^3\\).\nThis is one of the data sets that is bundled with the R distribution and is available to all users of R. They are stored in an R object called Nile.11 When you type data() at the R console, you get a list of all datasets that are available with the current distribution of R.\nThis is how the data look like, when we print them to the R console using the R command print(). The R function options() with the argument widthjust controls how the numbers are printed. Here I made sure that they will fit in the width of the page.\n\noptions(width = 70)\nprint(Nile)\n\nTime Series:\nStart = 1871 \nEnd = 1970 \nFrequency = 1 \n  [1] 1120 1160  963 1210 1160 1160  813 1230 1370 1140  995  935 1110\n [14]  994 1020  960 1180  799  958 1140 1100 1210 1150 1250 1260 1220\n [27] 1030 1100  774  840  874  694  940  833  701  916  692 1020 1050\n [40]  969  831  726  456  824  702 1120 1100  832  764  821  768  845\n [53]  864  862  698  845  744  796 1040  759  781  865  845  944  984\n [66]  897  822 1010  771  676  649  846  812  742  801 1040  860  874\n [79]  848  890  744  749  838 1050  918  986  797  923  975  815 1020\n [92]  906  901 1170  912  746  919  718  714  740\n\n\nWe start the construction of a histogram by choosing for the horizontal axes ranges of numerical values - in our case of the river flow data - which are called bins or classes. There is no fixed rule as to how to choose the size of these ranges. These ranges should neither be too fine, nor too coarse. While there are lists of mechanical rules, which you can for example find on Wikipedia2, it is usually best to use your domain knowledge and some experimentation to find out the bin size that works best for your data.2 See https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width\nFor this example, assume we had chosen a bin size of 1003. When you study the list, you will find that the lowest value is at 456 while the highest value is at 1370. This is already quite tedious to find out by eyeballing the numbers with the small number of values we have chosen for this example. It is impossible to do for really large data sets.3 Note that this will mean \\(100\\times 10^8 m^3\\) per year.\nNow lets make a distribution table like this:\n\n\n\nFlow-bin\nFrequency\n\n\n\n\n400 - 500\n1\n\n\n500 - 600\n0\n\n\n600 - 700\n5\n\n\n700 - 800\n20\n\n\n800 - 900\n25\n\n\n900 - 1000\n19\n\n\n1000 - 1100\n12\n\n\n1100 - 1200\n11\n\n\n1200 - 1300\n6\n\n\n1300 - 1400\n1\n\n\n\nIn the column Flow-bin we have recorded the bins in steps of 100 and in the right column, Frequency, we have recorded the count of values that are in this bin.\nWhen we make such a tabulation we have to agree on an endpoint convention. This is important, since when a flow value would for instance be measured as exactly 500, in which bin should it be counted: 400-500 or 500-600? You, the constructor of the histogram, has to take this decision. Let us agree on the convention that when a value falls exactly at the endpoint of the bin, we put it in the next bin. In practice you will usually do a histogram by computer. The code of the computer program has to specify an endpoint convention, so the computer knows what to do when a value coincides with an endpoint.\nOn the Frequency axes you put the frequency scale: Counts of values. Then for each bin, you plot a bar, which has the width of the bin and the height of the frequency.\nDo this for all the bins you have tabulated and you are ready.\nThe histogram provides a certain aggregation of the data because it sorts the 100 data points into 10 bins, in our example. While loosing some local information on individual data points the global information conveyed by the summary gives us a pretty good idea of the overall pattern of variation on the Nile river flow data.\nWe can see, for instance, that the most frequent flow is between 800 and 900 and that the variation is fairly symmetric around this bin. In the extremes this most frequent value can half or almost double, so there is quite some spread in the data.\n\n\n\nConstructing the river flow histogram\n\n\nIf we had just plotted all individual data points, we also got a picture, though you probably agree that it is not particularly useful.\n\n\nCode\nplot(as.numeric(Nile), xlab = \"Observation\", ylab = \"Annual Flow\", pch = 16)\n\n\n\n\n\nHistograms are such a common tool in statistics to explore the variation in one variable and the shape, how it is roughly distributed that every statistical software has functions to produce histograms. In R, the language we use in this course, there is also such a function. The function name is called hist() and it takes the data as an argument. This is the second graphic function of R you encounter in this course after we played with the barplot()function in the last lecture.\nTo produce a histogram from the river flow data, we type at the console\n\nhist(Nile)\n\n\n\n\n\n\n\n\n\n\nNow you try\n\n\n\nLet us check your understanding of histograms by a little quiz now. The histogram below shows the distribution of the final score in a certain class.\n\nWhich block represents the people who scored between 60 and 80?\nTen percent scored between 20 and 40 about what percentage scored between 40 and 60?\nAbout what percentage scored over 60?\n\n\n\n\nFinal Score\n\n\n\n\n\n\n3.1.2 The relative frequency scale: Absolute versus relative frequency\nSometimes it might be useful, to choose a different scale for the y axes of your histogram. Instead of absolute frequencies (or counts) it might be useful to show relative frequencies, the proportion of occurrences in each bin. The type of scale you choose will depend on what kind of comparisons you want to emphasize about your data.\nLet us look at this issue by an example. The numbers we want to look at report primary energy consumption per capita in kwh per person per year for different countries around the world.4 The energy numbers refer to primary energy – the energy input before the transformation to forms of energy for end-use (such as electricity or petrol for transport).4 A kilowatt-hour, known as Kwh is a way to measure how much energy is used. A kilowatt-hour is the amount of energy used if a 100 watt appliance is kept running for an hour. For instance, if you turned on a 100 watt bulb for one hour you are using a kilowatt-hour of energy. What’s the difference between kilowatt vs. kilowatt-hour? A kilowatt is 1,000 watts, which is a measure of power. A kilowatt-hour is a measure of the amount of energy a certain machine needs to run for one hour. So, if you have a 1,000 watt drill, it takes 1,000 watts (or one kW) to make it work. If you run that drill for one hour, you’ll have used up one kilowatt of energy for that hour, or one kWh. Obviously, every appliance will use a different amount of power. Here are some of the usages for some items in a home: 50″ LED Television: around 0.016 kWh per hour, electric water heater: 380-500 kWh per month\nLet us look at the year 2019.\n\n\nCode\nlibrary(JWL)\n\n\ndat &lt;- with(energy_consumption_per_capita, energy_consumption_per_capita[Year == 2019, ])\n\nhist_info &lt;- hist(dat$Cons, plot = FALSE)         # Store output of hist function\nhist_info$counts &lt;- hist_info$counts /    # Compute relative frequency values\n  sum(hist_info$counts) * 100\nplot(hist_info, freq = TRUE, xlab = \"Primary energy consumption in kilowatt-hours per person per year.\", ylab = \"Percent\", main = \"Primary energy Consuption per Capita 2019\")              # Plot histogram with percentages\n\n\n\n\n\nIn this histogram you see the distribution of per capita primary energy consumption for the year 2019 for countries around the globe. But now the y axes shows relative frequencies instead of counts.\nFor example, you see from the graph that roughly 55 % of countries have a primary energy consumption smaller that 20.000 kilowatt hours per person in this year. The next larger bucket contains already roughly half or 24 %. The biggest buckets are then a very small fraction of countries in the world. This means that there is a relatively small share of countries, around 20 %, which have a large primary energy consumption per capita. One says in the language of statistics that the distribution of per capita primary energy consumption is skewed. When you have a histogram with a relative frequency scale the lengths of the bars must sum to 1 (or to 100 depending whether the relative frequency is expressed in decimal fractions or in percentages).\n\n\n3.1.3 Exercises: Now you try\n\nA histogram of monthly wages for part-time employees is shown below (relative frequencies are marked in parenthesis). Nobody earned more than $1000 a month. The block over the class interval from 200 to 300 is missing. How tall must it be?\n\n 2. Three people plot histograms for the weights of subjects in a study, using the relative frequency scale. Only one is right. Which one and why?\n\n\nCode\n#| echo = false\n\nlibrary(JWL)\n\ndat &lt;- height_weight\n\ndata &lt;- dat[dat$state == 1 & dat$sex == 1 & dat$age &gt; 18, ]\n\nhist_inf &lt;- hist(data$height, plot = FALSE)         # Store output of hist function\nhist_inf$density &lt;- hist_inf$counts /    # Compute density values\n  sum(hist_inf$counts) * 100\n\npng(file=\"pictures/hight_version1.png\")\nplot(hist_inf, freq = FALSE, xlab = \" \", ylab = \" \", main = \" \")\n\n\npng(file=\"pictures/hight_version2.png\")\nplot(hist_inf, freq = FALSE, xlab = \"hight (cm) \", ylab = \"Percent per 5 cm \", main = \" \")\n\n\npng(file=\"pictures/hight_version3.png\")\nplot(hist_inf, freq = FALSE, xlab = \"hight (cm) \", ylab = \"5 cm per percent\", main = \" \")\n\n\n\n\n\n\n\n\n\n(a) Version 1\n\n\n\n\n\n\n\n(b) Version 2\n\n\n\n\n\n\n\n(c) Version 3\n\n\n\n\nFigure 3.1: Three versions of a hight histogram of males over age 18\n\n\n\nAn investigator draws a histogram for some height data, using the metric system. She is working in centimeters (cm). She wants to draw the histogram in a so called density scale, i.e. in a scale that the area of all the bars sum to 1. The vertical axes shows relative frequency and the top of the vertical axes is 10 percent per cm. Now she wants to convert to millimeter (mm). There are 10 millimeter to the centimeter. On the horizontal axis, she has to change 175 cm to ? mm, 200 cam to ? mm. On the vertical axis she has to change 10 percent per cm to ? percent per mm, and 5 percent per cm to ? percent per mm.\nIn a Public Health Service study, a histogram was plotted showing the number of cigarettes per day smoked by each subject (male current smokers), as shown in the histogram below. The density is marked in parentheses. The class interval include the right endpoint, not the left.\n\nThe percentage who smoked 10 cigarettes or less per day is around:\n1.5% 15% 30% 50%\nThe percentage who smoked more than a pack a day, but not more than 2 packs, is around\n1.5% 15 % 30% 50% (There are 20 cigarettes in a pack)\nThe percentage who smoked more than a pack a day is around\n1.5%, 15%, 30%, 50%\nThe percent who smoked more than 3 packs a day is around\n0.25 of 1%, 0.5 of 1%, 10 %\nThe percent who smoked 15 cigarettes per day is around\n0.35 of 1%, 0.5 of 1%, 1.5%, 3.5%, 10%\n\n\n\n\n\nFigure 3.2: Number of cigarettes\n\n\n\n\n3.1.4 Best practices for histograms\nWhen you summarize lots of data by a histogram there are some things you should consider carefully. Let us go through the most important best practice principles for histograms.\n\n3.1.4.1 Bin size\nWhen doing exploratory data work it is usually a good idea not to look at a single histogram but at several histograms of the same data by changing the bin size. There is no clear rule about the optimal bin size. It often depends on context and field knowledge.\nIf the bins are to fine, then the data will be be very noisy and give no overview because they show too many individual points. On the other hand if the bins are too wide, they will not show you the overall variability in the data very well and you fail to get a good idea about the distribution.\nLet us illustrate this point using the river flow data of the Nile.\nIn the first case we have chosen 100 bins, which is too fine. There is almost one bar for every single data point. In this way we have a lot of spurious peaks and throughs and can not see the variation pattern in the data very clearly\n\n\nCode\nhist(Nile, breaks = seq(min(Nile), max(Nile), length.out = 100))\n\n\n\n\n\nNow here we have the other extreme, lets assume we have only 3 bins. This would give us a pattern like this.\n\n\nCode\nhist(Nile, breaks = seq(min(Nile), max(Nile), length.out = 3))\n\n\n\n\n\nHere the histogram is too coarse and we do not see the variation pattern either.\nThe computer usually has a built in rule of thumb for the histogram which will work well in most of the cases. Still for individual datasets it is sometimes better to choose a different bin size that more adequately mirrors the variation in the data.\n\n\n3.1.4.2 Choose boundaries that can be clearly interpreted\nTick marks and labels should fall on the bin boundaries. As in the examples discussed so far, they need not be there for every tick but it is enough if they are there between every few bars. Bin labels should also have not many significant digits, so they are easy to read. So bin sized which divide 10 and 20 evenly are easier to read than bin sizes that do not. So always take caution not to arbitrarily split bin sizes. Otherwise you can end up with off bin boundaries.\nFor example, if we just took the maximum and the minimum of the Nile river flow data and arbitrarily divided them into 7 bins, we would get the difficult to read bin boundaries\n\n\nCode\nseq(min(Nile), max(Nile), length.out = 7)\n\n\n[1]  456.0000  608.3333  760.6667  913.0000 1065.3333 1217.6667\n[7] 1370.0000\n\n\ninstead of the more easily readable boundaries\n\n\nCode\nseq(400,1200,100)\n\n\n[1]  400  500  600  700  800  900 1000 1100 1200\n\n\n\n\n3.1.4.3 What’s the difference between a histogram and a bar chart?\nA histogram depicts the frequency distribution of a continuous, quantitative variable, such as height, weight, time, energy consumption etc. These are variables that can take on any value and these values can be ordered from smallest to highest.\nWhen we have a categorical variable, like we encountered them in section 2, we need to use a bar chart. The bars of the bar chart typically will have a small gap between the bars, emphasizing the discrete nature of the variable. The categories in a bar chart usually have no natural ordering. As we discussed in section 2, we have even to be conscious how we display the categories to avoid suggesting an order that is in fact not there in the data."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#sec-moreR",
    "href": "summarizing_and_communicating_lots_of_data.html#sec-moreR",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.2 Next steps in R: Reading Data, understanding R objects and selecting and modifying values",
    "text": "3.2 Next steps in R: Reading Data, understanding R objects and selecting and modifying values\nBefore we go on with learning the tools to summarize and communicate lots of data let us gain more skills for handling the tool that will actually enable you to handle large data sets yourself by making use of R. Building on what we learned in the last unit, lets now push your knowledge of R bit further.\n\n3.2.1 Reading data in R\nBefore we can do anything with data, we need first to learn how to load data into R and how to save them. We will discuss now how to do this for the case of comma separated text files or so called csv files. R provides functions for reading and writing from almost any other format, like data stored in Excel files and many more data formats that are used today. Since in all those different formats are read by R following the same principles as in the csv case, it is sufficient if we discuss here the the case of csv files only.\nWe have discussed a data set of per capita primary energy use in countries around the globe to produce a histogram of these data for the year 2019. How did I get these data into R?\nFirst of all I could access these data because helpful people at Oxford University in the UK who maintain and run the website “Our world in data”, which we have encountered before, store these data on their website. In the concrete case of the energy data, they can currently be found at https://ourworldindata.org/grapher/per-capita-energy-use where you can download the datafile from the webpage and save it locally somewhere on your computer.\nI have taken a screenshot here\n\n\n\nOur World in Data website energy\n\n\nIn the lower right corner you see a box called Data and a download button. This button allows you to download the dataset to your machine. The file is called per-capita-energy-use.csv. From the extension of the file csv, you can see that it is a comma separated text file. This is a plain text file following certain formatting rules. In particular individual data points are separated by a comma55 The standard format for csv can be looked up here https://www.ietf.org/rfc/rfc4180.txt. Despite this standardization it can occur that different files use different conventions for the notation of the decimal comma sign. In the most common specification this symbols is a dot (.) and in others it is a comma (,). For such speical cases R provides special functions, which we will explain in the text.\nI have stored the file in a sub-folder to the directory in which I am writing these lecture notes. If you decide to download this file, you will save it somewhere on your machine where you find it appropriate. Perhaps you have a folder for this course and in this folder you have a sub-folder where you store all the data sets we are using in the course.\nTo read a csv file, R provides the function read.csv(). If the csv file comes with a European instead of an US decimal format (, instead of . for the decimal sign.) you need to use the function read.csv2(). Please check out the documentation of these functions by typing ?read.csv at the R prompt.\nIn the simplest form you read the data and store them in an object you can work with in R. How to store data in an R object, we have already learned in the previous lecture. You invent a name and assign the values to this name using the assignment operator &lt;-.\nLet’s call the object in which we save our data energy_consumption, then by calling the function read.csv() with the path to your file as an argument will read the data from your local folder and store them in the object we have created. This allows us to refer to the data for doing further computations.\n\nenergy_consumption &lt;- read.csv(\"data/energy_use_per_capita/per-capita-energy-use.csv\")\n\nThe function needs as an argument the file name. If the file is in a sub-folder of the current directory you need to also specify the path. To specify the correct path to the file you need to know in which part of your directory tree you are currently working.\nIn my case I am working in the project folder for my lecture notes, which has a sub-folder called data. The data sub-folder has a further sub-folder called energy_use_per_capita in my case and thus I specify the path relative to this location.\nTo find out what is your current R working directory, R provides the function getwd(). If I type this in my case, I will get\n\ngetwd()\n\n[1] \"/home/martinsummer/R/Statistics_JWL\"\n\n\nthe path of my project folder for this lecture notes. So if I type the string \"data/energy_use_per_capita/per-capita-energy-use.csv\" this specifies the path relative to my working directory.\nIf you read the file on your computer, you need to specify the path appropriately from where you are working in R at the moment to where you have stored the csv file.\nNow read.csv() has many additional arguments, which provide you with lots of flexibility. I encourage you to check it out and play with it using the help function and the examples given therein by typing ?read.csv at the prompt.\nWe have now read the primary energy consumption data and written it to the R-object energy_consumption. Lets inspect the object a bit to see what we’ve got.\nI use the function head() with the parameter value n = 10. This will show me the first 10 rows of the data-file. So the value I give to the argument n contros how many rows will be displayed.\n\noptions(width = 120)\nhead(energy_consumption, n = 10)\n\n        Entity Code Year Primary.energy.consumption.per.capita..kWh.person.\n1  Afghanistan  AFG 1980                                           583.2944\n2  Afghanistan  AFG 1981                                           666.3782\n3  Afghanistan  AFG 1982                                           725.6599\n4  Afghanistan  AFG 1983                                           912.1396\n5  Afghanistan  AFG 1984                                           941.3926\n6  Afghanistan  AFG 1985                                           939.6124\n7  Afghanistan  AFG 1986                                           976.6691\n8  Afghanistan  AFG 1987                                          1592.7023\n9  Afghanistan  AFG 1988                                          2805.6096\n10 Afghanistan  AFG 1989                                          2700.4739\n\n\nThis gives you an idea what the data look like. There are four variables, called Entity, Code, Year and Primary.energy.consumption.per.capita..kWh.person. The last variable name is very informative but also very long and unpractical. We will learn how to change variable names soon. Because of the long name, I had to use the function options() before heads() to tell R to use a sufficiently wide display. Don’t worry for this detail at the moment.\n\n\n3.2.2 R objects\nThe most basic type of R objects are atomic vectors. Objects in R are built from atomic vectors.\nThe energy consumption data-file we have just loaded is an example of such a more complex structure built from atomic vectors. We have already encountered a few of those in our previous lecture.\n\n3.2.2.1 Atomic vectors\nAn atomic vector is just a simple vector of data. For example remember when we typed the infant mortality data for eight European countries for 1860 we typed\n\nmr_1860 &lt;- c(0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)\n\nIn this case mr_1860 is an atomic vector.\nR has a function, which allows you to check whether an object is an atomic vector or not. This function is called is.vector(). It takes the object name as an argument and returns TRUE if the object is an atomic vector and FALSE if it is not.\nFor example:\n\nis.vector(mr_1860)\n\n[1] TRUE\n\n\ndoes indeed return TRUE.\nEach atomic vector stores values in a one-dimensional vector, and each atomic vector can only store one type of data. The length of the atomic vector can be determined by the function length() This function takes an R object, which is an atomic vector, as an argument and returns the number of elements in this vector. Here is the example of the die\n\nlength(mr_1860)\n\n[1] 8\n\n\nwhich is 8 as it should be. An atomic vector could also have only one element, in which case lenght()would return 1.\n\n\n3.2.2.2 Data types\nNow altogether R has implemented six basic types of atomic vectors:\n\ndouble\nintegers\ncharacters\nlogical\ncomplex\nraw\n\nWe will not encounter complex and raw data-types in this course, so let us skip those and discuss only the first 4 types, double, integer, character and logical.\nIf yo u go back to our energy consumption data and look at the first three lines\n\n\nCode\noptions(width = 120)\nhead(energy_consumption, n=3)\n\n\n       Entity Code Year Primary.energy.consumption.per.capita..kWh.person.\n1 Afghanistan  AFG 1980                                           583.2944\n2 Afghanistan  AFG 1981                                           666.3782\n3 Afghanistan  AFG 1982                                           725.6599\n\n\nyou will see the different variables in the object energy_consumption. Note that I had to fudge a bit with the display because of the unwieldy and long name of the last variable. We are soon going to fix this.\nThe variables Entity and Code are both of type character. A character vector stores strings of text, which have to be put between quotation marks \"\". Strings are the individual elements of a character vector.\nNote that a string can be more than just letters. If you type, for instance the number 1 with quotation marks, like \"1\" R would interpret the value as a string not as a number. Sometimes one can get confused in R because both objects and characters appear as text in R code. Object names are without quotation marks strings always are between quotation marks.\nCharacter is the natural data type for country names - here “Afghanistan” and the international abbreviation “AFG” also called an ISO-country code66 ISO is the short name for International Organization for Standardization. International Organization for Standardization came into existence in the year 1946 in London. This organization was formed after a delegation of 65 members from 25 countries, met to discuss the future of International Standardization. In 1947, ISO was officially formed with 67 technical committees consisting of a group of experts focusing on a specific subject. ISO founders decided to give it an acronym ISO, which was based on the Greek word ‘isos’, which means ‘equal’.\nThe variable Year encodes the year of a particular record or observation. Its type is integer, since years are integer values like 1980 or 2022. If yo want to specify a number explicitly as integer in R you have to type it as, say, 1980L, the number followed without space by a big L.\nThe variable with the very long name Primary.energy.consumption.per.capita..kWh.person. is of type double. This is the data type in R for encoding numbers that are decimal fractions, like 12.451\nNow why should we care for distinguishing integers from doubles? This has to do with the way a computer does computations. Sometimes a difference in precision can have surprising effects.\nIn your computer 64bits of memory7 are allocated for each double in an R program. While this allows for a very precise representation of numbers not all numbers can be exactly represented with 64-bits. The famous candidates are \\(\\pi\\), which has an infinite sequence of digits and must therefore be rounded by the computer. Usually the rounding error introduced into your computations will go unnoticed but sometimes surprises can occur.7 A bit is a binary digit, the smallest increment of data on a computer. A bit can hold only one of two values: 0 or 1, corresponding to the electrical values of off or on, respectively. So 64 bits are sequences of 0 or 1 with a length of 64\nTake for instance:\n\nsqrt(2)^2 - 2\n\n[1] 4.440892e-16\n\n\nWhy is that? The square root of 2 can not be expresses precisely because, as already the old Greeks knew, it is not a rational number.88 Let me invite you to a short digression into the history of science and the history of ideas. The discovery that \\(\\sqrt{2}\\) can not be rational was a shock discovery to the ancient Greeks. The Greek mathematician Pythagoras and his followers were fascinated by and devoted to whole numbers. They detected the fundamental role played by ratios of whole numbers for musical harmony. For example dividing a vibrating string in two half raises the pitch by an octave, dividing the string in three raises the pitch by one fifths and so on. This discovery gave them the clue that the physical world as a whole might have an underlying mathematical structure governed by whole-number patterns. It was thus quite a shock when they found out that one of their foundational discoveries the Pythagorean theorem logically implied that there were ratios of lengths that were incommensurable, that is, not measurable as integer multiples of the same unit. The ratio between such lengths is therefore not a ratio of whole numbers. This is why the Greeks called these numbers irrational. Some of you will remember from school that the Pythagorean theorem says that in a right angled triangle with lengths \\(a\\), \\(b\\) and \\(c\\), where the sides with length \\(a\\) and \\(b\\) from a right angle must fulfill the equation \\(a^2 + b^2 = c^2\\) or expressed in a picture  The incommensurable lengths disovered by one member of the pythagorean school was the side and the diagonal of the unit square. By the Pythagorean theorem in a square with side length of 1 it must be the case that \\(\\text{(lenght of diagonal)}^2 = 1 + 1 = 2\\). Thus the length of the diagonal must be \\(\\sqrt{2}\\).  Hence if the diagonal and side are in the ratio \\(m/n\\) (where \\(m\\) and \\(n\\) can be assumed to have no common divisor), we have \\[\\begin{equation}\nm^2/n^2 = 2\n\\end{equation}\\] thus \\[\\begin{equation}\nm^2 = 2 n^2\n\\end{equation}\\] This equation implies that \\(m^2\\) must be an even number. So \\(m\\) must be even too, say \\(m = 2 p\\). But if \\[\\begin{equation}\nm = 2 p\n\\end{equation}\\] then we have \\[\\begin{equation}\n2 n^2 = m^2 = 4 p^2\n\\end{equation}\\] hence \\[\\begin{equation}\nn^2 = 2 p^2\n\\end{equation}\\] which similarly implies that \\(n\\) is even. But we began the chain of deductions by the assumption that \\(m\\) and \\(n\\) have no common divisor. So if both \\(m\\) and \\(n\\) were even they would have a common divisor, namely 2. We have arrived at a contradiction. This means the length of the diagonal of the unit square can not be a rational number. Legend has it that th first Pythagorean to make the result public was drowned at sea. But even if the Pythagoreans could not accept that \\(\\sqrt{2}\\) was a number, no one could deny that it was the length of the diagonal of the unit square.\nAnd you have a small rounding error. Let me explain to those of you who are puzzled by the meaning of this output. R displays the result of its computation in scientific notation. 4.440892e-16 means \\(4.44089 \\times 10^{-16}\\).\nFor those of you who forgot how exponential notation works, let me remind you that we write \\(10^{-1}\\) for \\(1/10\\) and \\(10^{-2}\\) means \\(1/10^2\\) or \\(1/100\\). Thus \\(10^{-16}\\) means \\(1/10.000.000.000.000.000\\), a very small number but still different from 0. This is the error introduced by rounding \\(\\sqrt{2}\\). Such errors are called floating point errors in computer science lingo and computing with such numbers is called floating-point-arithmetic.\nWith integers floating point errors are avoided, but for many applications this is not an option. Luckily for most cases floating-point arithmetic provides sufficient precision for most of the applications we encounter in practice.\nThe last data type in the list, we want to discuss here, are: Logicals. Logical vectors store TRUE and FALSE; logical values. They are extremely useful for doing comparisons and - as we will see shortly - also for selecting values from a data set.\nHow logical data types work can best be understood by an example. If you type, for instance:\n\n0 &gt; 1\n\n[1] FALSE\n\n\nR tells you that this statement is false, by printing the logical value FALSE as an output.\nWhenever you type TRUE of FALSE without quotation marks, R will treat the input as logical data. Note, as an aside, if you typed \"TRUE\"and \"FALSE\" in quotation marks, R would treat this input as a string co characters, a different data type. When communicating with a computer, you need to be very precise or the machine will not understand what you want to tell it do.\nFor instance, the following statement yields:\n\n\nCode\n#| code-fold: false\nlogic &lt;- c(TRUE, FALSE, TRUE)\nlogic\n\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\n3.2.2.3 Attributes\nOne important R-fact which you need to know about atomic vectors is that atomic vectors can have attributes. Attributes won’t affect the values of an object but can hold and store object metadata.\nNormally we do not look at these metadata, but many R functions check for attributes and then do special things with the object depending on these attributes. Attributes can be checked with the function attributes() using an R object as an argument. This will show you all the attributes that are attached to an R object.\nThe most common attributes for atomic vectors as well as R objects built from atomic vectors are names, dimensions and classes. Each of these attributes has its own helper function that you can use to also assign attributes to the object. For what we need now we discuss names and dimensions and discuss classes, which is a more advanced topic later.\nLet us look how this works with the enegrgy_consumptiondata and check whether they have a names attribute:\n\nnames(energy_consumption)\n\n[1] \"Entity\"                                             \"Code\"                                              \n[3] \"Year\"                                               \"Primary.energy.consumption.per.capita..kWh.person.\"\n\n\nWe can now see the variable names. We can use the function names()also as a helper function to assign other names to our variables. This is a tool that could help us to get rid of the very long and unwieldy name \"Primary.energy.consumption.per.capita..kWh.person.\". For example:\n\nnames(energy_consumption) &lt;- c(\"Entity\", \"Code\", \"Year\", \"Cons\")\n\nwill overwrite the name attribute by this new vector of names. When you now look for the names attribute, you will see\n\n\nCode\nnames(energy_consumption)\n\n\n[1] \"Entity\" \"Code\"   \"Year\"   \"Cons\"  \n\n\nOne very important attribute, we will encounter all the time is dimension, with the helper function dim(). For example we can look at our data object energy_consumption again to get:\n\n\nCode\ndim(energy_consumption)\n\n\n[1] 10215     4\n\n\nwhich returns two numbers, which mean that the object has 10215 rows and 4 columns.\n\n\n3.2.2.4 Factors\nR stores categorical data, such as nationality, sex etc. by using aspeical data type called factors. If you take for instance, sex, it can have only two values - male or female - and these values may have their idiosyncratic order, for example that females go always first.\nTo make a factor in R you have to pass an atomic vector to the factor() function. This function works by recoding the values in the vector as integers and store the results in an integer vector. R also adds a level attribute which contains the set of labels and their order and a class attribute that says the vector is a factor. Example:\n\nsex &lt;- factor(c(\"m\", \"f\", \"f\", \"m\"))\ntypeof(sex)\n\n[1] \"integer\"\n\nattributes(sex)\n\n$levels\n[1] \"f\" \"m\"\n\n$class\n[1] \"factor\"\n\n\nFactors can be confusing since they look like characters but behave like integers.\nNote that R will often try to convert character strings to factors when you load and create data. I recommend that you do not allow R to make factors unless you explicitly ask for it. This can usually be controlled by an argument to whatever the data reader function is. For instance you can give the read.csv() function the argument stringsAsFactors = FALSE.\nR has an internal coercion behavior for data types, which you should know about if you work with R. With this knowledge you can do many useful things.\nIf a character string is present in an atomic vector, R will automatically convert every other component in this vector to a character string. If a vector contains only logicals and numbers, R will convert the logicals to numbers. In this case every TRUE becomes a 1 and every FALSE becomes a 0.\nR also uses the coercion rules, when we do math with logicals, like for example\n\nsum(c(TRUE, TRUE, FALSE, FALSE))\n\n[1] 2\n\n\nWhat happens here is that R coerces the vector c(TRUE, TRUE, FALSE, FALSE) to the vector c(1, 1, 0, 0) and sums the components.\n\n\n\n3.2.3 Data frames and R lists\nGoing back to our data set on the enegry consumption data, we see that this data set stores values of different types, characters, integers and doubles. How does R achieve this?\nThe answer is that this is achieved by a data structure called a list. List are like atomic vectors, because the group data into a one-dimensional set. However, lists do not group together individual values. List group together R objects, such as atomic vectors or even other lists.\nFor example, you can create a list, which contains a numeric vector of length 31 in its first element, a character vector of length 1 in its second element and a new list of length 2 in its third. This is done by using the list()function of R, like this:\n\nlist_example &lt;- list(100:130, \"R\", list(TRUE, FALSE))\nlist_example\n\n[[1]]\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n[30] 129 130\n\n[[2]]\n[1] \"R\"\n\n[[3]]\n[[3]][[1]]\n[1] TRUE\n\n[[3]][[2]]\n[1] FALSE\n\n\nThe double bracketed indices tell you which element of the list is being displayed. The single bracketed indices tell you which sub-element of the list is being displayed. For example 100 is the first sub element of the first element in the list. “R” is the first sub element of the second list element.\nThere is lots to say about lists. But this is an advanced topics. We mentioned it here to introduce one of the most important data structres for our course the R dataframe.\n\n\n3.2.4 Data Frames\nData Frames are the two dimensional version of a list. They are by far the most useful storage structure for data analysis. Indeed, our dataset on energy consumption data we have loaded before is an instance of a data frame. Data frames group vectors together in a two dimensional table. As a consequence each variable can have a different type, i.e. each column of the data frame can contain a different data type. Within a column, however, we can have only one data type. The energy consumption data are a typical example of a dataframe.\n\n\nCode\nhead(energy_consumption, n = 10)\n\n\n        Entity Code Year      Cons\n1  Afghanistan  AFG 1980  583.2944\n2  Afghanistan  AFG 1981  666.3782\n3  Afghanistan  AFG 1982  725.6599\n4  Afghanistan  AFG 1983  912.1396\n5  Afghanistan  AFG 1984  941.3926\n6  Afghanistan  AFG 1985  939.6124\n7  Afghanistan  AFG 1986  976.6691\n8  Afghanistan  AFG 1987 1592.7023\n9  Afghanistan  AFG 1988 2805.6096\n10 Afghanistan  AFG 1989 2700.4739\n\n\nEvery column in this tabular array of data can be considered as a vector.\n\n\n3.2.5 Selecting data from R objects: A toolbox.\nAsking questions about a dataframe and in particular about our energy data requires that we are able to adress particular values in the dataframe. We now learn the most important techniques to do so.\nR has a notation system to address individual values. You write the object name first, followed by a pair of had brackets []. Between the brackets goes a , separating row and column indices. The notation is thus like energy_consumption[,].\nWhen it comes to writing the indices you have six different ways to do this, all of them very simple. You can use:\n\nPositive integers\nNegative integers\nZero\nBlank spaces\nLogical values\nNames\n\nThe simplest are positive integers. When you want to extract the energy consumption in kwh per person in one year - the value of the variable Cons - you would adress for instance the 3rd value in the Cons column, which is in our case the 4th column as\n\nenergy_consumption[3,4]\n\n[1] 725.6599\n\n\nYou can - of course extract more than one value. If you write for instance\n\nenergy_consumption[1:5,4]\n\n[1] 583.2944 666.3782 725.6599 912.1396 941.3926\n\n\nyou will get the first 5 values of the consumption numbers in the dataframe. The colon operator : used here is a very useful R operator. It creates sequences of whole numbers. Thus if you create a vector with the name, say n10 containing the sequence of the first 10 whole numbers, you would write\n\nn10 &lt;- 1:10\n\nNow clearly the indexing rules work in the same way as with dataframes, only that now you have only 1 dimension. Say you want to extract the first three numbers from n10 you would write\n\nn10[1:3]\n\n[1] 1 2 3\n\n\nNote that R’s notation system is not limited to data frames. The same syntax can be used to select values from any R object, provided you supply an index for each dimension of the object. Two things have to be kept in mind. In R indexing begins at 1. In some other programming languages indexing begins at 0. The indexing convention in R is just like in linear algebra. The second thing to note is that if you select two or more columns from a data frame, R will return a new data frame, like in\n\nenergy_consumption[1:5, 3:4]\n\n  Year     Cons\n1 1980 583.2944\n2 1981 666.3782\n3 1982 725.6599\n4 1983 912.1396\n5 1984 941.3926\n\n\nR will always return a dataframe.\nHowever, if you select a single column, R will return a vector:\n\nenergy_consumption[1:5, 4]\n\n[1] 583.2944 666.3782 725.6599 912.1396 941.3926\n\n\nif you prefer to get returned a data frame in this case, you have to add the argument drop = FALSE, like:\n\n\nCode\nenergy_consumption[1:5, 4, drop = FALSE]\n\n\n      Cons\n1 583.2944\n2 666.3782\n3 725.6599\n4 912.1396\n5 941.3926\n\n\nNegative integers work exactly opposite to positive integers. If you type:\n\nhead(energy_consumption[-1,4], n = 10)\n\n [1]  666.3782  725.6599  912.1396  941.3926  939.6124  976.6691 1592.7023 2805.6096 2700.4739 2557.5864\n\n\nR will return the fourth column of the data frame except the first row. We just display the first 10 values using the head() function.\nIf you try to pair a negative and a positive integer in an index, R will return an error. However, you can use both negative and positive integers if you use them in different indices.\nZero is neither positive nor negative, If you use 0 as an index, R will return nothing from a dimension with index 0. The following syntax for instance creates an empty object\n\nenergy_consumption[0,0]\n\ndata frame with 0 columns and 0 rows\n\n\nBlank spaces are used if you want to ask R to select every value in a dimension. So for instance, if you type:\n\n\nCode\n#|code-fold: false\nsel &lt;- energy_consumption[ , 4]\n\n\nR will select the entire column of energy consumption. You can check that the length of this vector is\n\n\nCode\n#|code-fold: false\nlength(sel)\n\n\n[1] 10215\n\n\nas expected.\nLogical Values can also be used for subsetting. If you type for instance\n\nenergy_consumption[1, c(F,F,F,T)]\n\n[1] 583.2944\n\n\nR will select the energy consumption value of the first observation. Note that here we used the R convention that TRUEand T as well as FALSEand F have an equivalent meaning.\nFinally, you can ask for the elements, you want by name. On our case, you could select the first energy_consumption value by the syntax\n\nenergy_consumption[1, \"Cons\"]\n\n[1] 583.2944\n\n\nFinally, note that two types of object in R obey an optional second system of notation. You can extract values from data frames and lists with th $syntax.\nIt works as follows: For example\n\ntest &lt;- energy_consumption$Cons\ntest[1:4]\n\n[1] 583.2944 666.3782 725.6599 912.1396\n\n\nwould select the column of energy consumption numbers in our dataframe, save them in an object names testand then subsequently extract the first four values from the object.\n\n\n3.2.6 Application: Reproduce our histogram of primary per capita energy consumption around the globe in 2019 by a worked example.\nThis was a lot of abstract and dry instruction about some data extraction tools. It will require lost of practice and concrete examples before you get some natural acquaintance with these techniques which belong to the everday routines of data analysis.\nTo see the concept in action, let me take you step by step through an example by showing you how you can produce a histogram, like we discussed in the first section of this chapter starting from the raw data.\nWe have already read the data from the csv file and stored it in an object we have called energy_consumption. We would like to plot a histogram of the annual per capita energy consumption in different countries around the globe for the year 2019.\nLet us look again at the first 10 observations:\n\nhead(energy_consumption, n = 10)\n\n        Entity Code Year      Cons\n1  Afghanistan  AFG 1980  583.2944\n2  Afghanistan  AFG 1981  666.3782\n3  Afghanistan  AFG 1982  725.6599\n4  Afghanistan  AFG 1983  912.1396\n5  Afghanistan  AFG 1984  941.3926\n6  Afghanistan  AFG 1985  939.6124\n7  Afghanistan  AFG 1986  976.6691\n8  Afghanistan  AFG 1987 1592.7023\n9  Afghanistan  AFG 1988 2805.6096\n10 Afghanistan  AFG 1989 2700.4739\n\n\nHow would we extract the observations that refer to the year 2019 only? Here is the first practical use case of logical data types, which will immediately show you their power. Let me suggest the following code and then explain step by step what it does.\n\ndat &lt;- energy_consumption[energy_consumption$Year == 2019, ]\nhead(dat, n = 10)\n\n                 Entity Code Year       Cons\n40          Afghanistan  AFG 2019   945.6454\n95               Africa      2019  4245.7188\n137             Albania  ALB 2019 11266.2578\n192             Algeria  DZA 2019 16140.1963\n234      American Samoa  ASM 2019 26023.6699\n274              Angola  AGO 2019  3429.5928\n314 Antigua and Barbuda  ATG 2019 31384.8633\n369           Argentina  ARG 2019 20785.9141\n399             Armenia  ARM 2019 15538.2012\n433               Aruba  ABW 2019 51178.5859\n\n\nLet me explain what is going on here: We have used the indexing rules and logical subsetting. In the first index we have written a logical condition, namely energy_consumption$Year == 2019. This means that R selects all the rows for which the year variable of energy_consumption is equal to 2019. The logical sign for identical in R is ==. So what R does then is checking for each value of the Year variable whether it is identical to 2019 - in which case the logical comparison results in TRUE or not, in which case the result would be FALSE. Then by logical subsetting all rows for which the comparison vector is TRUE will be selected. Note that we had to tell R that it needs to compare the Year variable. Year is selected in a data frame by energy_consumption$Year. We have then stored all the data in a vector I have called dat and then looked at the first 10 rows. You see that only the observations where the Year is 2019 have been kept, as we had intended.\nNow we see from the output that the Entity variable apparently not only contains individual countries but also whole regions or continents, like Africa in our example. It seems that the regions have no ISO code but instead an empty character.\nLet’s try to apply the logical subsetting logic and our knowledge of character types to filter out Entities which have no value in the Codevariable. One way to achieve this would be, for example the code:\n\ndat_countries &lt;- dat[dat$Code!=\"\", ]\nhead(dat_countries, n = 10)\n\n                 Entity Code Year       Cons\n40          Afghanistan  AFG 2019   945.6454\n137             Albania  ALB 2019 11266.2578\n192             Algeria  DZA 2019 16140.1963\n234      American Samoa  ASM 2019 26023.6699\n274              Angola  AGO 2019  3429.5928\n314 Antigua and Barbuda  ATG 2019 31384.8633\n369           Argentina  ARG 2019 20785.9141\n399             Armenia  ARM 2019 15538.2012\n433               Aruba  ABW 2019 51178.5859\n545           Australia  AUS 2019 66614.8828\n\n\nNow Africa is out. Of course this is no proof that I have now only entities with ISO-codes left but it is an indication. We can check this. But let me first expalin what is going on here.\nI now work with the object dat, the object we have created before by filtering for the observations in which the Yearvariable is equal to 2019. Now we tell R compare the entries in the Codevariable, i.e. in dat$code with the condition that they are not equal to an empty string. The symbol for not identical in R is !=. We know that the variable Code is of type character hence all codes are strings within quotation marks. An empty string is thus written as \" \".\nHow could we check - by the way that the Code variable does not contain any empty string anymore? Here is how:\n\nsum(dat_countries$Code == \"\")\n\n[1] 0\n\n\nHere I have used two rules we have dicussed before, logical data types and coercion. First we ask R to check whether any Code variable in dat_countiesstill has any empty string. This will create a vector of logicals like (showing only the first five entries)\n\nhead(dat_countries$Code == \"\", n = 5)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\nNow I use coercion rules by summing over this vector. The function sum() in R is a function for summation. For example, if I have the vector c(1,2,3,4,5,6,7,8,9,10) and type\n\nsum(c(1,2,3,4,5,6,7,8,9,10))\n\n[1] 55\n\n\nR will add up the numbers.\nNow if I have a vector of logicals and apply a numerical function to it, R will by its coercion rules, force TRUE into 1 and FALSE into 0. The result that sum(dat_countries$Code == \"\") equals 0 means that the vector for our logical test contained FALSE only. So indeed there is no empty Code variable anymore.\nNow we are ready to do a histogram of the per capita primary annual energy consumption by\n\nhist(dat_countries$Cons, \n     xlab = \"Primary energy consumption in kilowatt-hours per person per year.\",\n     main = \"Primary energy Consuption per Capita 2019\")\n\n\n\n\nI have here used the xlaband mainarguments with a character value to have a nicer description of the histogram. Otherwise the x axis would have been labeled dat_countries$Cons and the title would have been Histogram of dat_countries$Cons. Note that we have here a histogram in the absolute frequency scale."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#the-average-and-the-standard-deviation",
    "href": "summarizing_and_communicating_lots_of_data.html#the-average-and-the-standard-deviation",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.3 The average and the standard deviation",
    "text": "3.3 The average and the standard deviation\nWith a histogram we can summarize a large amount of data and get some insights about the variation in the data. Often we can summarize data much more drastically by just one number describing the center of the histogram and the spread around this center. When I write center an spread here, these are just ordinary words with no special technical meaning.\nWhen we do statistics we need precise definitions and we will study and learn these definitions in this section. The average is often used to find the center. Another measure to find the center is the median. The standard deviation measures spread around the average. The interquartile range is another measure of spreads.\nBefore we go into these definitions, let me show for a start two histograms, both have the same center, but the second one is more spread out, there are more observations farther away from the center.\n\n\n\n\n\n\n\n(a) Histogram 1\n\n\n\n\n\n\n\n(b) Histogram 2\n\n\n\n\nFigure 3.3: Histogram 1 and Histogram 2 have the same center but Histogram 2 is more spread out\n\n\nThese distributions can be summarized by the center and the spread. But what about a situation like this?\n\n\n\nFigure 3.4: A bimodal distribution\n\n\nIn some cases such distribution can occur naturally. Think, for example, about the distribution of the elevation data of surface area of the earth. Most of the surface area of the earth is taken by the sea floor at about three miles below the sea level or the continental planes around sea level. If we would report only the center an the spread of this histogram, plotted in the picture as the red and blue vertical lines, we would miss these peaks.\n\n3.3.1 The average\nHuman growth depends on good nutrition and the availability of medical services to effectively treat illness. Thus human height in a population has for a long time been a subject of study. The average human height in a population varies with the general living standards. In statistical terminology this is expressed by saying that human height and growth are positively correlated. This fact makes data on human height especially interesting for historians who study the history of living conditions. Because humans tend to get taller when they have good living conditions, human height can reveal some information on living conditions, height data can be an indirect measure of living conditions. This is especially interesting for periods when very little or no data were collected and recorded.\nAs we see from the class activity in the introductory unit where we collected data in your class on height and hand span, we learned that usually heights differ quite a bit within a group. This is also true in a population. The many different height data, which are usually collected in survey studies on health are usually summarized by studying average height. Height data are thus also interesting for us as one specific context to study height data.\n\n\n\n\n\n\nAverage\n\n\n\nThe average of a list of numbers is defined as their sum, divided by how many numbers their are in the list.\n\n\nFor example the average of the list \\(L\\)\n\nL &lt;- c(9,1,2,2,0)\n\nwould be computed as:\n\\[\\begin{equation}\n\\frac{(9+1+2+2+0)}{5} = \\frac{14}{5} = 2.8\n\\end{equation}\\]\nBy now it will be no surprise for you that R provides a function for computing averages. This function is known by the alternative name for the average, called the mean.\nHere is how you would compute the mean using R.\n\nmean(L)\n\n[1] 2.8\n\n\nwhich is indeed what we should get as a result.\nLet’s go back to the issue of human height data. Let us first look at a sample of height data and how they look like. This is a data set from the statistics Online Computational Resource (SOCR) of the university of California Los Angeles.9 for details.] The dataset contains 25,000 synthetic records of human heights and weights of 18 years old children.9 See http://www.socr.ucla.edu/\nLet us look at the histogram first. But before we do this, let me briefly explain a feature of R that will be very useful for us in the remainder of this course.\n\n3.3.1.1 A brief digression: Installing and using R packages\nWhen we install R we get a version the program which is called base-r. This is the term for the basic version, which comes with any R installation. The base version contains all the important functions. The hist()function is part of base-r for example, but also mean(), lenght() etc.\nBase R can be extended by adding so called packages. A package contains additional functions for doing computations or other things like graphics with R written by other people or by yourself, once you have learned how to write a package. Packages have to be installed and then they need to be loaded to make the functions of the package available to R. You have to install a new package only once, but you have to load it in every new session of R if you want to use it.\nThe first package, you will encounter in this course is a package that I have prepared when preparing the course. It contains all the data sets we use here. When the package is loaded these data sets are available for use. The package that contains our data is called JWL. You need to install it first. As you might guess there is an R function to install packages. The function is called install.packages(). This function works if the package is beeing made available via the R CRAN-server at https://cran.r-project.org/. Our package is a package that I have assembled for this course and it was installed from a private archive. It is not on CRAN. For you taking this course the package is already installed.\nTo be able to make the funcions and the data contained in the package available to R you need to laod the package first. This is done with the function library(). Now assume we want to load the package JWL we need to type\n\nlibrary(JWL)\n\nNow all the datasets bundled in the JWL-package are available for use to you. This includes the data sets on human height data I have prepared for this course.\nThe data set is called socr_height_weight and you can study the details and description of the data by typing ?socr_height_weight at the prompt.\nLet us start by first saving the socr_height_weight data in an R object which we call - say - ´dat.\n\ndat &lt;- socr_height_weight\n\nWe can inspect the R object, using the R function str(). This function compactly displays the internal structure of an R object.\n\nstr(dat)\n\n'data.frame':   25000 obs. of  3 variables:\n $ Index : num  1 2 3 4 5 6 7 8 9 10 ...\n $ Height: num  65.8 71.5 69.4 68.2 67.8 ...\n $ Weight: num  113 136 153 142 144 ...\n\n\nWe see that dat is a data frame with three variables Index, Height and Weight. All variables are of type numeric.\nFrom the help information on the dataset we know that the units of the Height variable is inches. Inches are used in the English speaking countries but in many other parts of the world the more common units are metric units. Human height in metric units would be measured in cm rather than in inches. This is an excellent opportunity to practice some of the R knowlegde you have learned in this unit.\n\n\n\n\n\n\nNow you try\n\n\n\nThe variable Height in the dataframe dat is measured in inches. 1 inch is 2.54 cm. Transform the Height variable such that it shows the Height in cm rather than in inches,\n\n\n\n\nCode\n# the hight data are in inches and the weight data are in pounds. Convert to metric\n# units cm\n\ndat$Height &lt;- dat$Height*2.54\n\n\nOnce you have done this little transformation of units, let us plot a histogram first. Let us also lable the x-axes as “Height in cm” and write as a title of the histogram “Height of 18 year old humans”\n\n\n\n\n\n\nNow you try\n\n\n\nUse R to plot a histogram of the Height variabe in the dat dataframe. Label the x-axes as “Height in cm” and write as a title of the histogram “Height of 18 year old humans”\n\n\n\n\nCode\nhist(dat$Height, xlab = \"Height in cm\", main = \"Height of 18 year old humans\")\n\n\n\n\n\nThe distribution looks symmetric. If we summarize these data by taking an average in one number we will capture the center of the distribution fairly well. Let us compute the average, using the mean funcion.\n\n\n\n\n\n\nNow you try\n\n\n\nUse R to compute the average or mean of the Height variable in the dat dataframe.\n\n\n\n\nCode\nmean(dat$Height)\n\n\n[1] 172.7025\n\n\nThe average height in this dataset is about 173 cm. The average is a very powerful way of communicating data by compressing many observations - in this example 25000 - into ons single number, the mean.\nThis compression is, however, only achieved by loosing some information on individual differences. For example in our dataset the average height is 173 cm. But there are about 4 % who are larger than 180.9 and there are also about 4 % who are smaller than 164.2. With 25000 individuals these are 1000 individuals who are beyond these thresholds. This diversity is hidden in the aggregation.\nThis is a good opportunity to show you a cool feature or trick in R how we could in one line compute such percentages. Assume we want to compute the percentage of individuals in our data who are larger than the mean of 172.7025. How can we do this?\nLogicals are a powerful data type to make such a computation. Say we have 10 numbers, given by\n\nx &lt;- c(4,5,1,2,4,2,0,10,11,6)\n\nNow we could ask R which entries of x are larger than 2 by typing\n\nx &gt; 2\n\n [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nThe output is a vector of logicals where R compares each entry in xwith 2 and if it is larger it returns TRUE and otherwise FALSE (this includes all entries which are exactly 2).\n\n\n\n\n\n\nNow you try\n\n\n\nReplace every TRUE value with 1 and every FALSE value with 0 and compute the mean. What do you get? Compute the percentage of values in xwhich are larger than 2. What did you get? Explain!\n\n\nNote that, when you apply the function mean() to a vector of data of type logical the coercion rules of R will coerce TRUE to 1 and FALSE to 0 automatically. So if you apply mean() to x you will get the result you have computed before.\nThis insight can be used to compute the percentage of values that fulfill a certain condition in R. Here the condition is that the Height is larger than the mean.\n\n\n\n\n\n\nNow you try\n\n\n\nCompute the percentage of Heights in dat which are larger and the percentage of Heights that are smaller than the mean of 172.7025.\n\n\n\n\nCode\nmean(dat$Height &gt; 172.7025)\n\n\n[1] 0.5006\n\n\nCode\nmean(dat$Height &lt; 172.7025)\n\n\n[1] 0.4994\n\n\nThe value of a distribution where 50 % of all values are larger than this given value and 50 % are smaller is another one number summary you can give for a large set of data which describes the center of a distribution in another way. It is called the median. R also has a built in function for the median, which is called - as you might already guess - median(). Let us compute the median height in our data.\n\nmedian(dat$Height)\n\n[1] 172.7091\n\n\nIn this case the median and the mean give the same value up to the third digit. They are for practical reasons identical. The reason is that in the case of the measure of Height the distribution is highly symmetric. In practice you will encounter many situations where the distribution of a variable is not even approximately symmetric.\nA typical case of a distribution that us typically not symmetric but skewed - as it is called in statistical terminology - is income. The following histogram which is scaled in relative frequencies shows the data for the global income distribution in the year 2013 as reported by two economists Hellebrand and Mauro (2015) from the Peterson Institute, a US based research institution.10 The unit of measurement is US dollars in so called purchasing power parity, a concept economists use to make income figures comparable in terms of the amount of goods they can buy.1110 We have used the data they provide to simulate a sample of 100.000 observations consisten with the numbers they report. Thus our data used here are a simulation reproducing the characteristics of the data reported in this paper. We will soon learn how to do simulations using R. For the moment we need you to trust that we have done this correctly.11 When reporting income figures we could translate all national figures gathered from around the world into one common currency (for instance, US dollars) using exchange rates from currency markets. But because market exchange rates do not always reflect the different price levels between countries, economists often opt for a different alternative. They create a hypothetical currency, called ‘international dollars’, and use this as a common unit of measure. The idea is that a given amount of international dollars should buy roughly the same amount – and quality – of goods and services in any country. This way of measuring monetary amounts is called dollars in purchasing power parity or PPP.\n\n\nCode\nlibrary(readxl)\naux &lt;- read_excel(\"~/R/Statistics_JWL/data/income/wp15-7.xlsx\", sheet = 5, range = \"B1:D1401\")\ninc &lt;- aux[-1 ,c(1,3)]\nnames(inc) &lt;- c(\"Income\", \"Percent\")\ninc$Income &lt;- as.numeric(unlist(inc$Income))\ninc$Percent &lt;- as.numeric(unlist(inc$Percent))\n\ninc_trunc &lt;- inc$Income[inc$Income&lt;=14000]\n\ndata &lt;- sample(inc_trunc, 10^5, replace = TRUE, prob = inc$Percent[0:701])\n\nhist(data, breaks = 27, xlab = \"Income in purchasing parity power dollars\", main = \"Global income distribution\")\n\n\n\n\n\nNow the mean and the median of these data would give us:\n\n\nCode\nmean(data)\n\n\n[1] 3012.174\n\n\nCode\nmedian(data)\n\n\n[1] 1700\n\n\nThere is a big difference in both numbers as you can see even better when we show the mean (red line) and the median (blue) income value of our data in the graph.\n\n\nCode\nhist(data, breaks = 27, xlab = \"Income in purchasing parity power dollars\", main = \"Global income distribution\")\nabline(v = mean(data),                       # Add red line for mean\n       col = \"red\")\nabline(v = median(data),                     # Add blue line for median\n       col = \"blue\")\n\n\n\n\n\nWhen the distribution is not symmetric the mean does not capture the center of the distribution well. We have stored the data of our histogram in an object we have chosen to call data. Let us verify that the median is actually the value above that is in the middle of all values and compare this with the mean for these skewed distribution.\nLet’s check the median first\n\nmean(data &gt; 1700)\n\n[1] 0.49616\n\nmean(data &lt; 1700)\n\n[1] 0.49998\n\n\nNow let us do the same check for the mean\n\nmean(data &gt; 3012.174)\n\n[1] 0.32447\n\nmean(data &lt; 3012.174)\n\n[1] 0.67553\n\n\nYou see that the mean now does not capture the center of the distribution well. Only 32 % have incomes above the mean income and 68 % have incomes below. The point where 50% are above and 50 % are below is actually at 1700, as expressed by the median.\nThere is another important difference between the mean and the median. Let us illustrate this difference with the example of primary energy consumption across the countries in the world. This distribution turned out to be highly skewed. Remember the histogram\n\n\nCode\nhist(dat_countries$Cons, \n     xlab = \"Primary energy consumption in kilowatt-hours per person per year.\",\n     main = \"Primary energy Consuption per Capita 2019\")\n\n\n\n\n\nThe mean is:\n\n\nCode\nmean(dat_countries$Cons)\n\n\n[1] 26767.75\n\n\nand the median is\n\n\nCode\nmedian(dat_countries$Cons)\n\n\n[1] 16525.11\n\n\nNow let us assume we add a fictitious large value to the data set, say 400.000 kwh per person per year and recalculate the mean.\n\nmean(c(dat_countries$Cons, 400000))\n\n[1] 28479.82\n\n\nNow the mean per capita consumption is quite a bit larger actually - just from this one extreme observation - it is larger by more than 6 %.\nWhat happens to the median?\n\nmedian(c(dat_countries$Cons, 400000))\n\n[1] 16525.53\n\n\nIt stays about the same.\nThe take away here is that the mean is a summary measure that is sensitive to outliers. The median on the other hand is a more robust measure. One or a a few outliers can not move the median.\n\n\n\n3.3.2 Exercises for the Average\nExercise 1:\n\nThe number 3 and 5 are marked by crosses on the horizontal line below. Find the average of these two numbers and mark it by an arrow.\n\nRepeat (a) for the list 3,5,5\n\nTwo numbers are shown below by crosses on a horizontal axis. Draw an arrow pointing to their average.\n\n\nExercise 2:\nA list has 10 entries. Each entry is either 1, 2 or 3. What must the list be of the average is 1? if the average is 3? Can the average be 4?\nExercise 3:\nWhich of the following lists has a bigger average? Or are they the same? try to answer without doing arithmetic.\n\n10, 7, 8, 3, 5, 9 (ii) 10, 7, 8, 3, 5, 9, 11\n\nExercise 4:\nTen people in a room have an average height of 1.69 m. An 11th person is 1.96 enters the room. Find the average height of all 11 people.\nExercise 5:\nTwenty-one people in a room have an average height of 1.68. A 22nd person who is 1.96 enters the room. Find the average height of all 22 people. Compare with exercise 4.\nExercise 6:\nTwenty-one people in a room have a height of 1.68. A 22nd person enters the room. How tall would he have to be to raise the average height by 1 inch.\nExercise 7:\nIf you go back to figure Figure 3.4, where in the Histogram would be mountains? Where would you find planes? Where would the trenches in the sea floor show?\nExercise 8:\nDiastolic blood pressure is considered a better indicator of heart trouble than systolic pressure. The figure below shows age-specific average diastolic blood pressure for men age 20 and over in a health survey from the US (HANES5 (2003-04)). True or fales: The data show that as men age, their diastolic blood pressure increases until age 45 or so. and then decreases. If false, how do you explain the pattern in the graph? (Blood pressure is measured in “mm” that is millimeter of mercury)\n\nExercise 9:\nAverage hourly earnings in the US are computed each month by the Bureau fo Labor statistics using payroll data from commercial establishments. The Bureau figures the total wages paid out to non-supervisory personnel, and divides by the total hours worked. During recessions, average hourly earnings typically go up. When the recession ends, average hourly earnings often start going down. How can this be?\n\n\n3.3.3 The standard deviation\nWhen summarizing and communicating lots of data, it is useful not only to report at which value the center of the distribution is. It is often helpful to also think about the way how the values spread around the average. The quantity which measures this spread is called the standard deviation. It can be interpreted as an average deviation. In this section you will learn to interpret the standard deviation in the context of real data and then learn how it is computed both by hand and by using the computer.\nLe us go back to our data on the height of 18 year old humans. There were 25.000 observations in our sample. The average height was\n\n\nCode\nmean(dat$Height)\n\n\n[1] 172.7025\n\n\nThis average tells us that most of the humans measured in the sample had a height of around 1 m and 73 cm. But there were deviations from this average. Some humans were taller and others were smaller. We can ask how big these deviations are? In answering this question we need the concept of the standard deviation.\n\n\n\n\n\n\nStandard deviation\n\n\n\nThe standard deviation says how far away numbers on a list are from their average. Most entries on the list will usually be somewhat around one standard deviation from the average. Very few will be more than two or three standard deviations away.\n\n\nIn R you can compute the standard deviation by using the function sd(). So let us compute the standard deviation of our height data using the sd()function.\n\nsd(dat$Height)\n\n[1] 4.830264\n\n\nSo the standard deviation is at about 4.8 cm. That means that many humans differed from the average height by about 1, 2, 3, 4 or about 5 cm. Let us use R to compute the percent of observation that are within one standard deviation from the average.\nLet us use the trick of combining logical subsetting and the coercion rules of R to compute this percentage here:\n\naux &lt;- dat$Height &gt;= mean(dat$Height) - sd(dat$Height) & dat$Height &lt;= mean(dat$Height) + sd(dat$Height)\n\nmean(aux)\n\n[1] 0.68356\n\n\nLet me explain what we have computed here. First we have checked for all the entries in the vector dat$Height whether they were larger than the average minus one standard deviation and smaller than the average plus one standard deviation. These are all the values that are within one standard deviation away from the average. For all values that are in this range R returns the value TRUE and for all the other values that are outside of this range, R returns the value FALSE. We have stored the vector recording all these logical values in a new object called aux. Now we computed the mean of aux. When the mean function of R gets a vector of logical values it coerces all the TRUE values to 1 and al the FALSE values to 0. When we take the mean we get the share of values for which the condition is TRUE. In our case this is around 68 %. The other 32 % are farther away.\n\n\n\n\n\n\nNow you try\n\n\n\n\nCompute the percentage of observations of our height data that are within two standard deviations from the average.\nCompute the percentage of observations of our height data that are within three standard deviations from the average.\n\n\n\nThis is a rule of thumb that applies to many (but not all) data sets. For many distributions roughly 68 % of observations are within one standard deviation from the average. 95 % are within two standard deviations from the average and 99.7 % are within three standard deviations from the average. There is a deeper mathematical reason why this rule applies to many symmetric distributions. For the moment we can use this information as a sort of rile of thumb for symmetric, bell shaped distributions. We will learn later in the course where this rule comes from. Let us illustrate the rules graphically using the histogram.\n\n\nCode\nhist(dat$Height, col = color_list, xlab = \"Height in cm\", main = \"Height of 18 year old humans\")\nabline(v = mean(dat$Height),                       # Add red line for mean\n       col = \"red\")\n\n\n\n\n\nThe next figure shows the same histogram. Now the area within two standard deviations from the mean is colored differently\n\n\nCode\nhist(dat$Height, col = color_list, xlab = \"Height in cm\", main = \"Height of 18 year old humans\")\nabline(v = mean(dat$Height),                       # Add red line for mean\n       col = \"red\")\n\n\n\n\n\nLet us note a final point about this rule of thumb for symmetric distribution, you frequently encounter in practice. Often it is common to report a distribution of a variable not in its orginal units - height in cm - for example but in a standardized form where the units are changed such that the mean is at 0 and the standard deviation is at 1.\nHow is this possible? Let us consider the following example.\n\n\n\n\n\n\nExample\n\n\n\nConsider the following 10 values, which we write into a vector \\(x\\) and compute its mean \\(\\mu\\) and its standard deviation \\(\\sigma\\):\n\nx &lt;- c(9,14,9,11,11,10,15,8,11,13)\n\nmean(x)\n\n[1] 11.1\n\nsd(x)\n\n[1] 2.282786\n\n\nThus \\(\\mu = 11.1\\) and \\(\\sigma = 2.282786\\). Now let us change the units according to the formula \\[\\begin{equation*}\nz_i = \\frac{x_i - \\mu}{\\sigma}\n\\end{equation*}\\] and compute the mean of \\(z_i\\) and its standard deviation. We can use R to do that:\n\nz &lt;- (x - mean(x))/sd(x)\n\nmean(z)\n\n[1] 1.422473e-16\n\nsd(z)\n\n[1] 1\n\n\nNow you see that the mean is 0 and the standard deviation is 1. The mean in the R computation is not exactly 0, but this comes from the rounding errors made by the computer. For practical purposed \\(1.4 \\times 10^{-16}\\) is as good as zero. This must be the case as a consequence of how we changed the units. The mathematically inclined among you might try to derive this fact more generally using the definition of the mean and the standard deviation.\nNow see what happens when we standardize our height data\n\n\nCode\n#|code-fold: false\n\ndat$Z &lt;- (dat$Height - mean(dat$Height))/sd(dat$Height)\n\n\nWe want to see how many of the transformed values fall within 1 standard deviation, 2 standard deviations or three standard deviations from the mean.\n\ncheck1 &lt;- dat$Z &gt;= mean(dat$Z) - sd(dat$Z) & dat$Z &lt;= mean(dat$Z) + sd(dat$Z)\ncheck2 &lt;- dat$Z &gt;= mean(dat$Z) - 2*sd(dat$Z) & dat$Z &lt;= mean(dat$Z) + 2*sd(dat$Z)\ncheck3 &lt;- dat$Z &gt;= mean(dat$Z) - 3*sd(dat$Z) & dat$Z &lt;= mean(dat$Z) + 3*sd(dat$Z)\n\nmean(check1)\n\n[1] 0.68356\n\nmean(check2)\n\n[1] 0.9546\n\nmean(check3)\n\n[1] 0.99796\n\n\n\n\nSo the rule of thumb applies all the same: About 65 % of observations are within one standard deviation from the mean, 95 % within 2 and 99.7 within three standard devaitions. The transformation has only changed the units in which we measure our values not the way they are distributed.\n\n\n3.3.4 Exercises for the standard deviation\nExercise 1:\nThe socr_height_weight about the height and weight of 18 year old humans, we had used in the lecture before the average height in inches was about 173 cm and the standard deviation was about 5 cm.\n\nOne individual was 188 cm. He was above average by how many standard deviations?\nAnother individual was 174.66 cm. She was above average by how many standard deviations?\nA third individual was 1.5 standard deviations below the average height. He was how many cm?\nIf an individual was within 2.25 standard deviations of average height, the shortest height for this individual would be how many cm? The highest height would be how many cm?\n\nExercise 2:\n\nHere are the heights of 4 individuals. 150 cm, 130 cm, 180 cm, 172 cm. Match the heights with the description. A description may be used twice.\n\nunusually short, about average, unusually tall\n\nAbout what percentage of individuals in the data had heights between 170.2 and 173.2 ? Between 165.5 and 179.2?\n\nExercise 3:\nEach of the following lists has an average of 50. For which one is the spread of the numbers around the average biggest? Smallest?\n\n0, 20, 40, 50, 60, 80, 100\n0, 48, 49, 50, 51, 52, 100\n0,1,2,50,98, 99, 100\n\nExercise 4:\nEach of the following lists has an average of 50. For each one, guess whether the standard deviation is around 1, 2, or 10. (This does not require any arithmetic.)\n\n49, 51, 49, 51, 49, 51, 49, 51, 49, 51\n48, 52, 48, 52, 48, 52, 48, 52, 48, 52\n48, 51, 49, 52, 47, 52, 46, 51, 53, 51\n54, 49, 46, 49, 51, 53, 50, 50, 49, 49\n60, 36, 31, 50, 48, 50, 54, 56, 62, 53\n\nExercise 5:\nBelow are three sketches of three stylized histograms (we call them “sketches” because they display a schematic shape of a hypothetical histogram. This is why these sketches do not look like real histograms.) Match the sketch with the description. Some descriptions will be left over. Give your reasoning in each case. The symbol \\(\\approx\\) is the mathematical notation for approximately.\n\nave \\(\\approx\\) 3.5, sd \\(\\approx\\) 1 (iv) ave \\(\\approx\\) 2.5, sd \\(\\approx\\) 1\nave \\(\\approx\\) 3.5, sd \\(\\approx\\) 0.5 (v) ave \\(\\approx\\) 2.5, sd \\(\\approx\\) 0.5\nave \\(\\approx\\) 3.4, sd \\(\\approx\\) 2 (vi) ave \\(\\approx\\) 4.5, sd \\(\\approx\\) 0.5\n\n\n\n\n\n\n\n\n(a) Histogram 1\n\n\n\n\n\n\n\n(b) Histogram 2\n\n\n\n\n\n\n\n(c) Histogram 3\n\n\n\n\nFigure 3.5: Three stylized histograms\n\n\nExercise 6:\nOne investigator takes a sample of 100 men age 18 - 24 in a certain town. Another one takes a sample of 1000 such men.\n\nWhich investigator will get a bigger average for the heights of the men in his sample? Or should the average be about the same?\nWhich investigator will get a bigger standard deviation for the heights of the men in his sample? or should the standard deviation be about the same for both investigators?\nWhich investigator is likely to get the tallest of the sample men? Or are the chances about the same for both investigators?\nWhich investigator is likely to get the shortest of the sample men? Or are the chances about the same for both investigators?\n\n\n\n3.3.5 Computing the standard deviation\nUsually we will compute the standard deviation using the computer and only in rare cases will we ever compute a standard deviation by hand. Still it is important that you understand how the computation works and what is actually been computed. Here is how.\nExample 1: Find the standard deviation of the list 20, 10, 15, 15\nStep 1: We first need to find the average, which is \\[\\begin{equation}\n\\frac{20 + 10 + 15 + 15}{4} = 15\n\\end{equation}\\]\nStep 2: We next need to find the deviation from the average. In order to do so, we just subtract the average from each entry \\[\\begin{eqnarray}\n(20-15)&=5\\\\\n(10-15)&=&-5\\\\\n(15-15)&=&0\\\\\n(15-15)&=&0\n\\end{eqnarray}\\]\nStep 3: Now we have to square each one of these differences and take the square root, which is often called the root mean square\n\\[\\begin{eqnarray}\nsd&=&\\sqrt{\\frac{5^2 + (-5)^2 + 0^2 + 0^2}{4}} \\\\\n&=& \\sqrt{\\frac{25 + 25 + 0 + 0}{4}} \\\\\n&=& \\sqrt{\\frac{50}{4}}\\\\\n&=& \\sqrt{12.4}\\\\\n&\\approx& 3.5\n\\end{eqnarray}\\]\nThe standard deviation has the same units as the data. For example, when we measure height in cm then at the squaring step the units change to \\(cm^2\\) but the suqre root returns \\(cm\\) again.\n\n\n\n\n\n\nNow you try\n\n\n\n\nGuess which of the following two lists has the larger standard deviation. Check your guess by computing the standard deviation for both lists.\n\n\n9,9,10,10,12\n7,8,10,11,11,13\n\n\nCan the standard deviation ever be negative?\nFor a list of positive numbers can the standard deviation ever be larger than the average?\n\n\n\n\n\n3.3.6 Interquartile range and the boxplot\nNote that for skewed distributions the standard deviation is also not a very good measure of the spread, because it measures the variation about the mean and the mean does not capture the center of the distribution very well in this case.\nMoreover the standard deviation - like the mean - is sensitive to outliers and is therefore not a robust measure of the spread of a distribution.\nA measure that is more appropriate in this case is the inter-quartile range or IQR. It is the distance between the 25th and the 75th percentile of the data and contains the central half of the data. The 25th percentile is the range of the data where 25 % of observations have a lower value and 75 have a higher one. The median, for example, could equivalently be termed the 50 % percentile.\nR had a function to compute the inter-quartile range, which is called IQR(). For example in the case of our income data, which we showed are skewed, we would get, for example a mean and a standard deviation of\n\n\nCode\nmean(data)\n\n\n[1] 3012.174\n\n\nCode\nsd(data)\n\n\n[1] 3160.662\n\n\nand a median and inter quartile range of\n\n\nCode\nmedian(data)\n\n\n[1] 1700\n\n\nCode\nIQR(data)\n\n\n[1] 3220\n\n\nAlternatively you could use the R function summary(), which computes in one go the minimum and maximum value of your data, as well as the median the mean and the 25 and 75 percentile of the data. You can compute the IQR from this.\nLet us illustrate the use of summary() with the income data discussed in this section. To compute the describtives in one go for these data, you would tell R:\n\nsummary(data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     20     820    1700    3012    4040   14000 \n\n\nThe summary()function will give you the minimium, the first quartile, the median and the mean, the 4th quartile and the maximum. The interquartile range can be computed from these data as \\(4040 - 820\\) which is equal to \\(3220\\). When you use IQR()you will get\n\nIQR(data)\n\n[1] 3220\n\n\nas indeed it should be.\nNote that is you summarize our human height data we get:\n\nsummary(dat$Height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  153.1   169.4   172.7   172.7   176.0   190.9 \n\n\nSince the median and the mean give you the same value, you can see already from these data that the distribution of the data is symmetric, as we have already seen before.\n\n\n\n\n\n\nNow you try\n\n\n\nFor the list of numbers:\n2, -1, 1, -1, 1, -2, 1.5, -0.2, 1.37, -1.37, 3\n\nCompute the mean, the standard deviation, the median and the inter quartile range.\nDraw a histogram\n\n\n\nA standard graphical tool that summarizes the data in an anology to the summary()function is the boxplot. The boxplot summarizes the variation of data graphically through quartiles. It draws a box, where one side corresponds to the lower quartile, while the opposie side corresponds to the upper quartile. The median is displayed as a line crossing the box. The boxplot also contains lines, which are called the whiskers extending the box indication variability outside the two quartiles. These observations are at the extremes of the variation of the data. Data points that differ significantly from the other data, often calles outliers are shown as individual points.\nLet us show some of the datasets we have discussed in this section to illustrate this very useful tool which gives you a compact and very informative summary of a huge data set. Let us start with Nile river data we had looked at in the beginning of the section. T R function for making a boxplot is called boxplot() and is used like this:\n\nboxplot(Nile)\n\n\n\n\nHere the box shows you the maximum and the minimum flow (the whiskers), the box surrounding the first and the fourth quartile as well as the median as the black line within the box. If you look up the help function of boxplot() you can learn about various ways to control the look of the plot.\nYou can display the boxplot vertically or horizontally, whatever seems more convenient in the given context. Assume for instance we would like to show the Nile river data in one graph displaying both the boxplot and the histogram at the same time, then it would be more informative to have the boxplot as a horizontal graph.\n\n\nCode\nmat &lt;- matrix(c(1, 2), nrow = 2, ncol = 1, byrow = TRUE) # first and second plot\nlayout(mat = mat, heights = c(1, 1)) # First and second row relative heights\nboxplot(Nile, horizontal = TRUE)\nhist(Nile)\n\n\n\n\n\nBut you could, of course, also choose a different layout. Lets take the primary energy consumption data we used before\n\n\nCode\nmat &lt;- matrix(c(2, 1), nrow = 1, ncol = 2, byrow = TRUE) # first and second plot\nlayout(mat = mat, widths = c(1, 2)) # First and second row relative heights\nplot(hist_info, freq = FALSE, xlab = \"\", ylab = \"Percent\", main = \" \")\nboxplot(hist_info$density)\n\n\n\n\n\nThis graph show you in one picture the extreme asymmetry of primary energy consumption across the globe. Very few countries are heavy consumers of primary energy whereas the a huge share of countries consume very little in comparison. In the context of the debate on climate change, this observation might be an interesting starting point for discussing the sharing of the burdens of getting out of fossile fuels between the countries in the world.\nLet us finally look at the Nile river data once more combining the display of the raw data, the histogram as well as the boxplot. This shows you at the same time the raw data, and two ways of summarizing them. When the datapoints are huge, the display of the raw data will not show you very much, because the points will be so packed that you can not distinguish different datapoints easily. Withe the Nile river flow data, where we have 100 observations the situation is different. So this is how the picture of all three graphs combined looks like.\n\n\nCode\n# Data\n\nlayout(matrix(c(2, 0, 1, 3),\n              nrow = 2, ncol = 2,\n              byrow = TRUE),\n       widths = c(3, 1),\n       heights  = c(1, 3), respect = TRUE)\n\n# Top and right margin of the main plot\npar(mar = c(5.1, 4.1, 0, 0))\nplot(as.numeric(Nile), xlab = \"\", ylab = \"Flow\")\n\n# Left margin of the histogram\npar(mar = c(0, 4.1, 0, 0))\nhist(Nile, main = \"\", bty = \"n\",\n     axes = FALSE, ylab = \"\")\n\n# Bottom margin of the boxplot\npar(mar = c(5.1, 0, 0, 0))\n\n# Boxplot without plot region box\npar(bty = \"n\")\n\n# Boxplot without axes\nboxplot(Nile, axes = FALSE)"
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#describing-differences-between-groups-of-numbers",
    "href": "summarizing_and_communicating_lots_of_data.html#describing-differences-between-groups-of-numbers",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.4 Describing differences between groups of numbers",
    "text": "3.4 Describing differences between groups of numbers\nGroups of numbers are often compared by using summary statistics. Summary statistics can answer the question: How do the groups differ on average, how do they differ with respect to how spread out the observations are.\nBy this stage of the chapter on summarizing and communicating lost of data it should be clear that when we conduct such comparisons, we have to be mindful of the fact that summary measures hide information that depending on the way the data are distributed some summary measures are more appropriate than others.\nLet us use data on human height to illustrate these points. Our data includes a set of anthropometric data from a textbook by the anthropologist Richard McElrath (McElrath (2020)). If we summarize the height data of men and women in this dataset using the R summary function we get\n\n\nCode\nmh &lt;- height_weight$height[height_weight$sex == 1 & height_weight$age &gt;= 18] |&gt; na.omit()\nfh &lt;- height_weight$height[height_weight$sex == 0 & height_weight$age &gt;= 18] |&gt; na.omit()\n\nlapply(list(Male = mh, Female = fh), summary)\n\n\n$Male\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  139.7   157.5   160.7   160.4   163.8   179.1 \n\n$Female\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  136.5   146.1   149.9   149.5   153.0   162.6 \n\n\nThe summary statistics shows as a few facts about these data. First of all we see that both for the male height data as well as for the female height data the mean and the median are about the same and thus the distributions are roughly symmetric. The mean thus actually captures the “center” of the distribution in a meaningful way. On average men are taller than women. The variation in terms of the inter quartile range is about the same between 6 and 7 cm.\nNote that this does not mean that all men are taller than women. Let’s look at the full data in terms of the histogram.\n\n\nCode\nhist(mh, col=rgb(1,0,0,.5), border=NA, xlab = \"Height in cm\", main = \"Comparative histogram male and female height\", breaks = 14)\nhist(fh, col=rgb(0,0,1,.5), border=NA, breaks = 9, add=TRUE)\n\n\n\n\n\nWe have colored the female height distribution in blue and the male height distribution in red and overlayed the histograms. Now you can see that men tend to be taller than women as expressed in the differences in the averages there is also a substantial amount of overlap in the distributions. For each male in this overlap region you will find a female with a matching height, and some who are taller than a given male individual. About 10 % of women in this dataset have a male twin in height.\nIf you describe data and compare them by summary measures there is no substitute for looking at the data properly. It is usually a good idea to look at the distribution as well as on the summary measures."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#describing-relationships-between-data",
    "href": "summarizing_and_communicating_lots_of_data.html#describing-relationships-between-data",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.5 Describing Relationships between data",
    "text": "3.5 Describing Relationships between data\nAre taller people also heavier? This is another way in which we can look at large sets of data and describe their pattern. We try to describe the relationship between data. A standard tool to do so is the so called scatterplot.\nLet us show how a scatterplot works by using the data on height and weight of humans older than 18 years. If you give two numeric vectors of equal length to the function plot() in R R will produce a scatterplot.\n\n\nCode\nH &lt;- socr_height_weight$Height*2.54\nW &lt;- socr_height_weight$Weight*0.4535924\n\nplot(H,W, xlab = \"Height in cm\", ylab = \"Weight in kg\", main = \"Relation between height and weight of adult humans\")\n\n\n\n\n\nWhat you see here is a dense cloud of 25.000 pairs of height and weight measurements. For any given height you see a whole range of observations with different weights. And for any given weight you see many observations with differing heights. But the cloud as a whole shows an increasing relation. This makes also intuitive sense. People who are taller tend to be also heavier.\nIt is sometimes convenient to summarize such increasing (or decreasing) relationships between two variables or the pairs of numbers shown in a scatterplot with a single number. The general choice for doing this is the Pearson correlation coefficient or simply the correlation.\nThe Pearson correlation coefficient is a number that can be between -1 and 1. It epresses how close to a straight line the data-points in the scatterplot fall. If the correlation coerfficient is 1 all points lie on an upward sloping straight line. If the coeficcient is - 1 all the points fall on a downward sloping straight line. A correlation coefficient of 0 can come from points scattered at random or any other pattern in which there is no systematic upward or downward trend.\nR provides a function for the Pearson correlation coefficient, which is called cor(). For our height-weight pairs data used for the scatterplot this coefficient computes as\n\ncor(H,W)\n\n[1] 0.5028585\n\n\nsuggesting an association of increasing weight with increasing height.\nA correlation coefficient is simply a one number summary of association or co-variation. It can not be used to conclude that there is definitely an underlying relationship between height and weight - in this example - nor can it say anything about why such a relationship may exist."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#describing-trends",
    "href": "summarizing_and_communicating_lots_of_data.html#describing-trends",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.6 Describing Trends",
    "text": "3.6 Describing Trends\nLet us come back to the data on human height. One reason why data on height have always been interesting to researchers is that average height is strongly correlated with living standards in a population. This is because poor nutrition and poor medical support limit human growth. Because better living conditions tend to make humans taller they are an indirect measure of living stanrdards. Height is - of course - not a direct measure of well beeing. The variation of height in a given population is largely determined by genetic fatcors.\nLooking at the trend of average height over time helps us to indirectly track progress especially with respect to conditions of nutrition and disease prevention. The time trends in average human height are therefore data that can tell interesting stories or open our interest for further exploration.\nLet us use a dataset on the average height of men and women accross countries around the world from 1896 to 1996 which we retrieve from the site our world in data which we have now encountered several times by now12.12 https://ourworldindata.org/human-height\nLet us study, for instance the data for the world as a whole\n\n\nCode\nmh &lt;- height_men\nfh &lt;- height_women\n\nmh_w &lt;- height_men[height_men$Entity == \"World\", ]\nfh_w &lt;- height_women[height_women$Entity == \"World\", ]\n\nx &lt;- mh_w$Year\ny1 &lt;- mh_w$Height\ny2 &lt;- fh_w$Height\n\n#plot the first data series using plot()\nplot(x, y1, type=\"l\", col=\"blue\", ylab=\"y\", lty=1, ylim=range( c(150, 180) ) )\n\n#add second data series to the same chart using points() and lines()\n\nlines(x, y2, type = \"l\", col=\"red\",lty=2)"
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#summary",
    "href": "summarizing_and_communicating_lots_of_data.html#summary",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.7 Summary",
    "text": "3.7 Summary"
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#computer-exercises",
    "href": "summarizing_and_communicating_lots_of_data.html#computer-exercises",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.8 Computer exercises",
    "text": "3.8 Computer exercises\nOpen the notebook exercises_summarizing_and_communicating_lots_of_data.ipynb and work on the exercises. Write your solutions into the notebook. You will need the computer to do this work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHellebrand, Tomas, and Paolo Mauro. 2015. “The Future of Worldwide Income Distribution.” Brooking Institution Working Paper Series 7 (15). file:///home/martinsummer/R/Statistics_JWL/data/income/Hellebrand_Mauro_wp15-7.pdf.\n\n\nMcElrath, Richard. 2020. Statistical Resthinking. CRC Press. https://github.com/rmcelreath/rethinking."
  },
  {
    "objectID": "from_limited_data_to_populations.html#learning-from-data-and-the-process-of-inductive-inference.",
    "href": "from_limited_data_to_populations.html#learning-from-data-and-the-process-of-inductive-inference.",
    "title": "4  From limited data to populations",
    "section": "4.1 Learning from data and the process of inductive inference.",
    "text": "4.1 Learning from data and the process of inductive inference.\nIn the preceding chapters we have looked at examples where we had given data. Analysis of this data was sufficient if we just appropriately summarized them or displayed the data in an insightful way.\nSometimes this is really all there is to do. For instance when we studied the time trend in infant mortality we just looked at the time series of mortality data and a plot showed us the complete answer. The data we previously plotted for Ghana and Kenya - for example - show a steady decrease since 1965 starting from a share of roughly 12 % down below 2 % in 2020. These are data from public registers and not samples. The data are based on a complete set of observations in a country.\n\n\nCode\nlibrary(ggplot2)\nlibrary(JWL)\n\npl_dat &lt;- with(infant_mortality_data, infant_mortality_data[Country %in% c(\"Ghana\",\"Kenya\") & \n                                                            Year &gt;= 1965, ])\n\np &lt;- ggplot(pl_dat, aes(x = Year, y = Mortality, color = Country)) +\n     geom_point() +\n     geom_line() +\n     xlab(\"\") +\n     scale_y_continuous(labels = scales::percent)\n\np\n\n\n\n\n\nBut sometimes we want to say more about the data. We would like - for instance - to make predictions on how, for example, the trend in this share is going to look like in the future where we do not yet have data.\nOr maybe we would like to say something more basic. For example why did the share show a long term downward trend in those countries.\nSuch generalisations, where we try to learn something about the world outside of our observations, based on these observations are called in statistics inductive inference. Inductive inference is a challenging idea and it had been the topic of many philosophical and methodological controversies among scholars in the past.\nDeductive reasoning derives particular conclusions from general premises using the rules of logic. In this way if the assumptions hold and the reasoning is done correctly, i.e. the rules of logic are properly applied the conclusions is certain and irrefutable. A toy example of deductive reasoning would for instance be:\n\nMajor Premise: All plants perform photosynthesis.\nMinor Premise: A cactus is a plant.\nConclusion: A cactus performs photosynthesis.\n\nModern mathematical reasoning is all built on deduction, allowing to come up with actual proofs of certain statements given a set of axioms or assumptions. A proof is possible because deduction is logically certain.\nInductive reasoning works differently. It starts from particular instances and tries to work out generalizations from there. It goes from data to hypothesis. A simple toy example for inductive reasoning would be:\n\nData: I see fireflies in my garden every summer.\nHypothesis: This summer I will probably see fireflies in my garden.\n\nInduction does not allow to proof a hypothesis because it is generally uncertain.\n\n\n\n\n\n\nNow you try\n\n\n\nThink of another example for\n\ndeductive reasoning\ninductive reasoning.\n\n\n\nLet us go back to the entire process of going from the raw data to the statements about the entire country or region in the DHS survey.\nWhen we go from the sample to the true data about the units in the sample, the step from stage 1 to stage 2, we have to think careful about issues or problems of measurement. We want to know whether our data are reliable. They should have a low variability from occasion to occasion and should measure the same thing when we repeat a measurement. They should also measure what we intend or want to measure. This is called validity. We do not want our data to have a systematic bias.\nHere is the idea of a random in a stylized picture. The big circle is the frame for the population from which you take a sample. The unit of obervation in the DHS and in this example is a household. The read households are picked at random from the population and put into the sample.\nWhen we take again the DHS as an example, where people are asked many questions and quite a few measurements are taken on individuals, questions should be such that people give the same or a similar answer each time they are asked this question. To some extend this can be tested but these tests are not perfect. We also need to assume that people answer honestly to questions. With measurements things are similar. We want that we get the same or a very similar result if we repeat a measurement and we need to assume that the survey field workers who take the measurements do their job diligently.\nIf questions would be biased towards a particular answer a survey would also not be valid. This needs special care in developing questions. In chapter 2 we have learned about framing effects. Framing effects are often used in marketing. For example in meat packaging it is well known that how you report certain facts about the packaged meat influences sales. If the same piece of meat is one time packaged with the text “75 % lean meat” and the other time with the text “25 % fat”, which is logically the same information, several studies showed that on average the first package is preferred. This and other details have to be considered when elicting information from the sample.\nNow going from the second stage - from the sample - to the third stage - the study population requires particular care. We have to be confident that the sample observed accurately reflects characteristics of the larger group we are ultimately interested in and from which the sample has been taken. Technically this is also often called internal validity.\nHere we come to a crucial idea how bias can be avoided, the idea of random sampling. Let us explain exactly what a random sample is. Using R will support us in developing this understanding. As an example data set of our population we use as an illustration in the following discussion, let us take the dataset on the height and weight of adult humans, we used in chapter 3."
  },
  {
    "objectID": "from_limited_data_to_populations.html#why-does-random-sampling-help-avoiding-bias",
    "href": "from_limited_data_to_populations.html#why-does-random-sampling-help-avoiding-bias",
    "title": "4  From limited data to populations",
    "section": "4.2 Why does random sampling help avoiding bias?",
    "text": "4.2 Why does random sampling help avoiding bias?\n\n\nCode\nlibrary(JWL)\ndat &lt;- socr_height_weight\ndat$Height &lt;- dat$Height*2.54\ndat$Weight &lt;- dat$Weight*0.4535924\n\n\nRemember that this was a data set with 25.000 observations and three variables, an index for each individual a number of height in cm and weight in kg.2. Let’s look at the first 10 rows in this dataset.2 Note that the original data set socr_height_weight measures height in inches and weight in lbs. To get cm from inches, we have to multiply by 2.54 and to get kg from lbs we have to multiply the numbers by 0.4535924. In the data we use here we use units of cm and kg\n\n\nCode\nshow &lt;- head(dat, n= 10)\nrownames(show) &lt;- NULL\nknitr::kable(head(show, n= 10))\n\n\n\n\n\nIndex\nHeight\nWeight\n\n\n\n\n1\n167.0896\n51.25254\n\n\n2\n181.6486\n61.90960\n\n\n3\n176.2728\n69.41184\n\n\n4\n173.2702\n64.56226\n\n\n5\n172.1810\n65.45207\n\n\n6\n174.4925\n55.92903\n\n\n7\n177.2972\n64.18092\n\n\n8\n177.8374\n61.89826\n\n\n9\n172.4727\n50.97122\n\n\n10\n169.6272\n54.73372\n\n\n\n\n\nEach row of this table represents an individual. Each individual here is an adult person age 18 or more for which a measure of height and weight has been taken. So if we sample from the rows of this table, we get values for the index, the height and the weight of individuals.\nWe could now take samples of rows from the table by selecting every 50th row. Here is a way of how we could implement this in R. If you go back to our last section and remember what we have learned about subsetting, you might get an idea how to approach such a problem.\nLet us first create a sequence of numbers starting from 1 and going until the last row in steps of length 5. This can be done without problems because 25000 rows are divisible by 50 and this procedure will select exactly 500 rows. Here is the R code:\n\nidx &lt;- seq(from = 1, to = nrow(dat), by = 5)\nsample_1 &lt;- dat[idx, ]\nrownames(sample_1) &lt;- NULL\nknitr::kable(head(sample_1, n = 5))\n\n\n\n\nIndex\nHeight\nWeight\n\n\n\n\n1\n167.0896\n51.25254\n\n\n6\n174.4925\n55.92903\n\n\n11\n168.8787\n57.81108\n\n\n16\n180.5727\n63.50180\n\n\n21\n172.2978\n64.08385\n\n\n\n\n\nLet me explain. seq() is an R function which creates a regular sequence of numbers. The arguments specify where the sequence begins and where it ends as well as the step length it takes. We write this sequence into a variable idx. Since we do not want row names we delete these. Then we apply the subsetting rules we have learned in the last section to select all rows with index idx with all its variables.\n\n\n\n\n\n\nNow you try\n\n\n\nYou can try this code yourself and apply it to another situation where you would for example select every 100th obervation in the dataframe.\n\n\nWhile this is a sample from our population, this is not a random sample because there is no chance involved in selecting the rows. Random sampling is a sampling method that uses a random mechanism. This means that the probability of each unit in the population to become part of the sample is known.\n\n4.2.1 A virtual game of chance: Rolling a die\nWe have not yet discussed the concept of probability. We will learn about probability later in the course. But we have all am intuitive notion of probability from simple games of chance, such as from rolling a die. If we throw a die the probability of each of the points shown on the die’s faces - 1, 2, 3, 4, 5 or 6 - is \\(\\frac{1}{6}\\). This is the chance or probability we would attribute before throwing the die that it shows a particular number. When we can give a probability for each outcome of throwing a die, we have a probability distribution, which is in this particular example very simple, because the probability we have attributed to the different outcomes is always \\(\\frac{1}{6}\\). The probability of each number showing up is the same, \\(\\frac{1}{6}\\). We can show this in the form of a bar-plot, where each bar stands for an outcome and its height showing the probability of this outcome.\n\n\nCode\ndie_out &lt;- c(rep(1,10), rep(2,10), rep(3,10),rep(4,10), rep(5,10), rep(6,10))\n\n# create a histogram with 6 columns\nhist(die_out, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nIn contrast to this theoretical probability distribution an empirical distribution is the distribution of observed data. We encountered many such distributions in the previous sections and and we have learned how to summarize them graphically by histograms.\nThe computer allows us to create empirical distributions of results from throwing a virtual die. It is a simulation of a real situation where you would actually roll a six sided die physically.\nThis is an example which allows us to introduce some new functions and concepts in R, which we will need in the rest of the course.\nFirst we create a virtual die by defining an appropriate R object. This die should represent a physical die, which you know from games of chance.\n\n\n\nA die\n\n\nThe essential feature of the die is that it has six faces each showing different points starting from 1 to 6. In R we implement this by creating a vector of integers 1 to 6, like this\n\ndie &lt;- 1:6\n\nHere we have created the vector of integers 1,2,3,4,5,6 and saved these numbers in an object calles die.\nNow R has a built in function, called sample() which can pick values at random from an object. We can tell R by using this function for instance that it should randomly pick a number from the six possible numbers of die.\nThis is how it works:\n\nsample(x=die, size = 1)\n\n[1] 6\n\n\nThe R-function sample() takes an object as argument. The second argument, called size, specifies the number of random picks or draws from the object. If we give the value 1 to size it is as if we threw the die once. We have now implemented in the computer an equivalent to physically throwing a die.\n\n\n\n\n\n\nNow you try\n\n\n\nIn the stylized picture of a random sample, we had 24 households in the population from which we randomly picked a sample of 8. Use the sample function to pick randomly 8 numbers out of the numbers 1 to 24.\n\n\nOne feature that makes R so very powerful is that you can not only use built in functions, like mean, hist sample etc. but you can also write your own functions. We could for instance write a function which rolls a die if we call it.\n\n\n4.2.2 A brief digression: Writing R functions yourself\nEach function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object.\nThe syntax is given like this:\nmy_function &lt;- function() {}\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named roll_die\n\nroll_die &lt;- function(){die &lt;- 1:6 \n                         sample(die, size = 1)}\n\nNow we have written the function and saved it as an R-object we can call it like this. Let’s call it three times for example:\n\nroll_die()\n\n[1] 1\n\nroll_die()\n\n[1] 2\n\nroll_die()\n\n[1] 2\n\n\nNote that in our function roll_die() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like roll_die(). If we only call the name roll_die, R will display the code in the function body.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\nLet me finally say a few things about arguments in a function.\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”.\n\nroll_die2 &lt;- function(){sample(ball, size = 1)}\n\nIf we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call roll_die2 if we make ball an argument of the function. Lets do this:\n\nroll_die2 &lt;- function(ball){sample(ball, size = 1)}\n\nNow the function will work as long as we supply ball when we call the function.\n\nroll_die2(ball = 1:6)\n\n[1] 1\n\n\nNote that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument\n\nroll_die2 &lt;- function(ball= 1:6){sample(ball, size = 1)}\n\nNow if we type:\n\nroll_die2()\n\n[1] 6\n\n\neverything works, just as intended.\n\n\n4.2.3 Going back to simulating die rolls\nNow we are interested in an empirical distribution of points if we roll the die many times. A built in R function that would help us to do this. This function is called replicate(). It needs two arguments, how many times it should do something and of course what it should do precisely.\nSo if we roll our virtual die 10 times, we would tell R to do this:\n\nr10 &lt;- replicate(10, roll_die())\n\nwhich gives us the sequence of results from these 10 rolls. Now let’s plot the empirical distribution.\n\n\nCode\nhist(r10, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nThis does look quite different from the theoretical distribution, where each bar had the same length.\nWhen we increase the sample size the empirical distribution starts to look more similar to the theoretical distribution. Lets roll our die 100 times\n\nr100 &lt;- replicate(100, roll_die())\n\n\n\nCode\nhist(r100, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nLooks better. What about 1000 rolls?\n\nr1000 &lt;- replicate(1000, roll_die())\n\n\n\nCode\nhist(r1000, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nAs we increase the number of rolls in the simulation, the area of each bar gets closer to 16.67% (\\(\\frac{1}{6}\\)), which is the area of each bar in the probability histogram.\nLet’s do a last example with 100.000 rolls to show this\n\nr100000 &lt;- replicate(100000, roll_die())\n\n\n\nCode\nhist(r100000, \n     breaks = seq(0.5, 6.5, by = 1), \n     col = \"lightblue\",  \n     freq = FALSE,\n     xlab = \"Face\", \n     ylab = \"Percent per unit\", \n     main = \"Probability Distribution Fair Die\",\n     axes = F)         \n\n# add labels below each column\naxis(1, at = 1:6, labels = 1:6)\naxis(2, at =c(0,0.08, 0.16), labels = c(\"0 %\", \"8 %\", \"16 %\"))\n\n\n\n\n\nNow we are almost there.\n\n\n4.2.4 The law of averages\nWhat we have just observed is a demonstration of a famous result of probability theory. We are going to learn about probabilities later in the course. This result, sometimes referred to as the law of averages.33 Technically the result is called the weak law of large numbers and was discovered by the Mathematician Jacob Bernoulli in 1713\nThe law says that if a chance experiment - such as throwing a fair die - is repeated independently and under identical conditions (this means that every repetition is performed in the same way regardless of the other repetitions.), then as we repeat the experiment long enough, the relative frequency of each event\n- in our case an event would be that for example the outcome of the die throw is 1 - gets closer to the theoretical probability of the event.\nIn the example of the die, we just studied by using the simulation capacities of R, we saw that the proportion that the die will land on a face showing 6 will happen in about 1/6 of all rolls.\nThis law also holds when a random sample is drawn from units of a large population. Let us use the table of heights in cm and weights in kg of 25.000 adults we studied in the last unit. Here we show the fist 10 rows.\n\n\nCode\nlibrary(JWL)\ndata &lt;- socr_height_weight\n# transfrom to metric units from inches to cm and from lbs to kg\ndata$Height &lt;- data$Height*2.54\ndata$Weight &lt;- data$Weight*0.4535924\nhead(data, n=10)\n\n\n   Index   Height   Weight\n2      1 167.0896 51.25254\n3      2 181.6486 61.90960\n4      3 176.2728 69.41184\n5      4 173.2702 64.56226\n6      5 172.1810 65.45207\n7      6 174.4925 55.92903\n8      7 177.2972 64.18092\n9      8 177.8374 61.89826\n10     9 172.4727 50.97122\n11    10 169.6272 54.73372\n\n\nLet us remind ourselves, how the histogram of the height-data looked like.\n\n\nCode\nh &lt;- hist(data$Height, \n     breaks = 30, \n     plot = F)\n\nh$counts &lt;- h$counts/sum(h$counts)*100\n\nplot(h, xlab = \"Height\", ylab = \"Percent\", main = \"Height distribution of adults\", axes = F)\naxis(1, at = c(160, 170, 180, 190), labels = c(\"160 cm\", \"170 cm\", \"180 cm\", \"190 cm\"))\naxis(2, at =c(0,2,4,6,8), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nLet us now think about the 25.000 adults in our data as a population. We draw a random sample from it. When we draw from the population we draw with replacement. This means when a particular individual is selected at one draw, it could in principle be selected also at the next one.\nLet us write an R function for this:\n\nemp_distr_height &lt;- function(n){\n  \n  data[sample(x = 1:nrow(data), size = n, replace = T), ]\n\n  }\n\nThis function draws n random row from our dataframe. Note that n is an argument for the function. The random draws are a dataframe with N row indices, say 10, and then this gives the row index to the dataframe according to the subsetting rules we have learned in the previous section data[rowindex, ]. Since there is no index in the second slot the entire rows will be selected.\nNow in analogy to the dice example we can see, that when we increase the number of draws into our sample we come closer to the distribution of the population. By this mechanism a random sample generates a subpopulation with similar distributional properties when the sample is large enough. How much is large enough? For this we need probability theory which we will encounter later in the course.\nAs with the die, let us start with 10 draws:\n\nhs10 &lt;- emp_distr_height(10)\n\n\n\nCode\nh &lt;- hist(hs10$Height, \n     breaks = 30, \n     plot = F)\n\nh$counts &lt;- h$counts/sum(h$counts)*100\n\nplot(h, xlab = \"Height\", ylab = \"Percent\", main = \"Height distribution of adults\", axes = F)\naxis(1, at = c(160, 170, 180, 190), labels = c(\"160 cm\", \"170 cm\", \"180 cm\", \"190 cm\"))\naxis(2, at =c(0,2,4,6,8), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nThis does not look very similar to the population distribution.\nWhat about a sample with size 100?\n\nhs100 &lt;- emp_distr_height(100)\n\n\n\nCode\nh &lt;- hist(hs100$Height, \n     breaks = 30, \n     plot = F)\n\nh$counts &lt;- h$counts/sum(h$counts)*100\n\nplot(h, xlab = \"Height\", ylab = \"Percent\", main = \"Height distribution of adults\", axes = F)\naxis(1, at = c(160, 170, 180, 190), labels = c(\"160 cm\", \"170 cm\", \"180 cm\", \"190 cm\"))\naxis(2, at =c(0,2,4,6,8), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nNow lets increase to 2500, 10 % of the population.\n\nhs2500 &lt;- emp_distr_height(2500)\n\n\n\nCode\nh &lt;- hist(hs2500$Height, \n     breaks = 30, \n     plot = F)\n\nh$counts &lt;- h$counts/sum(h$counts)*100\n\nplot(h, xlab = \"Height\", ylab = \"Percent\", main = \"Height distribution of adults\", axes = F)\naxis(1, at = c(160, 170, 180, 190), labels = c(\"160 cm\", \"170 cm\", \"180 cm\", \"190 cm\"))\naxis(2, at =c(0,2,4,6,8), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nThis looks already pretty close. You should by now see what random sampling does for the process of inductive inference. For a large enough random sample, the empirical distribution of the sample resembles the histogram of the population with high probability. Thus if you report summaries of the sample, like its mean and its standard deviation, you will be close to the true values in the population. You will not be exactly there. But since the random mechanism is based on a probability model, where the chance of each unit ending up in the sample is known, we can quantify the uncertainty and the error we make.\nHow probability arguments work more precisely, we will learn later in the course. For now this brief discussion should have convinced you of the importance of random sampling for making inferences from sample to population and given you an intuition how this works. As a side effect of this discussion you have also learned how to write R functions.\n\n\n\n\n\n\nNow you try\n\n\n\nUse the emp_distr_height()function we have just written to draw a random sample of n=5000. Compute the mean and the standard deviation of the sample distribution. Compute - or look up in chapter 3 - the mean and the standard deviation of the population data and compare.\n\n\nNow that we have discussed random sampling, why it is important and how it works, let us finally discuss the last step in the inductive inference chain, going from the study population to the target population. Even if we have a perfect random sample, it might be the case that we were not able to ask the people who we were ultimately interested in. When we can make conclusions on the target population from the study population, we say that we have not only internal but also external validity.\nSpiegelhalter (2019), whom we have already cited before gives an example where this problem is particularly extreme. Often scientist who are ultimately interested in humans could onl study animals, as it is often the case in pharmaceutical research.\nA bit less extreme but related is a second example Spiegelhalter (2019) gives is the case of drugs tested say only on adults but it is then used also by children. These are problems which cannot entiely be solved by statistical methods but need caution and particular care and caution."
  },
  {
    "objectID": "from_limited_data_to_populations.html#when-all-data-are-available",
    "href": "from_limited_data_to_populations.html#when-all-data-are-available",
    "title": "4  From limited data to populations",
    "section": "4.3 When all data are available",
    "text": "4.3 When all data are available\nWhile surveys are a very important instrument to learn from data about the reality we live in, not all data that are available to us today are surveys. Mainly by the internet and technological development, the omnipresence of sensors and digital devices, we have today many data that are not random samples or which are not based on any sampling at all.\nMany data, especially from online commerce are available to platform firms and online shops, for example. The new field of data science git a big push from the attempt to get hold of data created through interactions on the internet and to gain information from these data.\nThe public sector also collects and stores lots of administrative data in a systematic manner. These are for example data needed to run a social security or pension system, police records, health records, demographic data, land registers and business registers, also data on crime, voters and education. These data are used for policy making, research and statistical analysis. In this case you have all the data.\nHaving all the data means that the stage of the induction process where we go from a sample to a study population collapses because the sample and the study population are the same.\nStill problems can occur. Spiegelhalter (2019) mentions the problem that the police systematically misses cases which the police does not record as a crime or which have not been reported by a victim. These problems make the step of drawing conclusions from the study population to the target population difficult. It is not only police records on crimes. You have analogous problems in all other register data as well. Take population register, as one example. Often certain groups such as homeless people or undocumented immigrants who may be hesitant to register or provide personal information are under-counted. Or you can have inaccuracies in property ownership data in land registers due to incomplete or outdated records, fraudulent transactions, or illegal land occupations. Or in health registers you could have missing or incomplete data on certain health conditions, especially those that are stigmatized or poorly understood, or patients who do not seek medical attention due to lack of access or fear of discrimination.\nOf course if we have all the data it is a straightforward process to produce statistics that describe what we have measured. But for broader conclusions we need to be very careful and we need to think about systematic biases and how we can potentially control for them. This always needs our special attention because systematic bias can jeopardize or even invalidate the reliability of our claims."
  },
  {
    "objectID": "from_limited_data_to_populations.html#the-normal-approximation-for-data",
    "href": "from_limited_data_to_populations.html#the-normal-approximation-for-data",
    "title": "4  From limited data to populations",
    "section": "4.4 The normal approximation for data",
    "text": "4.4 The normal approximation for data\nWe will now discuss an old and powerful concept that helps us in our ultimate aim to describe the target distribution. This is called the normal distribution or often also called the “bell curve”, because of its characteristic shape. We already encountered this distribution in the last exercise of chapter 3, when we worked with the DHS data.\n\n4.4.1 The normal curve\nThe normal curve was discovered first around 1720 by Abraham de Moivre4 In 1809, the German mathematician Car Friedrich Gauss5, made significant contributions to the concept of the normal curve, which is until today often also referred as a Gaussian curve. He used the curve to model errors in astronomical observations. Around 1870 the Belgian mathematician Adolphe Quetelet6 had the idea of using the curve as an ideal histogram to which historams of empirical distributions could be compared.4 Abraham de Moivre (1667-1754) was a French mathematician known for his contributions to the development of probability theory and the normal distribution as well as for his work on complex numbers. He is perhaps best known for his discovery of the formula that bears his name, which relates complex numbers to trigonometric functions. De Moivre was also a prominent member of the scientific community in 18th-century Europe and corresponded with many of the leading mathematicians and scientists of his time. 5 Carl Friedrich Gauss (1777-1855) was a German mathematician and physicist who made significant contributions to a wide range of fields, including algebra, number theory, astronomy, and physics. Gauss is considered one of the greatest mathematicians of all time and is known for his contributions to the development of calculus, the discovery of the normal distribution, and his work on the theory of functions. Gauss also made important contributions to the field of astronomy, including the discovery of the asteroid Ceres and the development of a method for calculating the orbits of celestial bodies. 6 Adolphe Quetelet (1796-1874) was a Belgian mathematician, astronomer, and statistician who is known for his work in developing the concept of the “average man” and the use of statistical methods in the social sciences. He is also credited with the development of the body mass index (BMI), which is still widely used today as a measure of obesity. Quetelet was a prominent figure in the scientific community of his time and was a founding member of the International Statistical Institute. \nThe normal curve has a slightly intimidating equation, which looks as follows \\[\\begin{equation}\ny = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left( \\frac{x - \\mu}{\\sigma}\\right)^2}\n\\end{equation}\\]\nBut don’t be afraid. You do not have to be able to manipulate this equation. This is something that R. We will learn how to do this.77 The mathematical formula for the normal curve is very interesting, since it contains the most famous numbers in mathematics. We have already encountered \\(\\sqrt{2}\\), the diagonal of the unit square, which is the first so called irrational number detected by the Pythagoreans in antiquity. For them it was quite a shock to discover that there are numbers which can not be represented as a fraction of whole numbers. There is the famous number \\(\\pi\\) also known since antiquity. This number represents the ratio of the circumference of any circle to its diameter and is also an irrational number. Finally there is the number \\(e\\) or sometimes also known as Euler’s number. It is the limit of the sequence \\((1+\\frac{1}{n})^n\\) and plays a fundamental role all over mathematics.\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 1000)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\", lwd = 2, xlab = \"Standard units\", ylab = \"Percent per standard unit\", main = \"The Normal Curve\", col = \"red\")\n\n\n\n\n\nThere are several features of this graph that are important to note:\n\nThe graph is symmetric about 0. The part of the curve to the right is a mirror image of the part of the curve to the left.\nThe total area under the curve equals 1 or 100 %.\nThe curve is always above the horizontal axis. It appears to get equal to zero between 3 and 4 but this is only because the distance from the horizontal axis becomes too tiny to visualize. Only about 0.00006 of the area under the curve is outside of this interval.\n\nIt will be important and helpful to find areas under the normal curve between specified values. For instance:\n\nthe area under the normal curve between +1 and - 1 standard deviations from the mean is about 68 %.\nthe area under the normal curve between -2 and +2 standard deviations is about 95 %.\nthe area under the normal curve between -3 and +3 standard deviations is about 99.7%\n\nDoes this look familiar? If you followed the course until now, you will remember that these are the numbers we encountered with the data on human height. This is because for these data the empirical distribution is such that the respective histogram is very closely approximated by a normal curve.\n\n\n4.4.2 Units and standard units\nOften it is convenient to express the units for the normal curve not in the direct unit in which observations are measures - as for example cm for human height - but in units of standard deviations from the mean.\n\n\n\n\n\n\nConversion to standard units\n\n\n\nA value is converted to standard units by checking how many standard deviations it is above or below the average. Values above the average are given a plus sign, values below the average are given a minus sign.\n\n\nTake an example from our human height data and draw 4 random values from these data, using the emp_distr_height() function we wrote earlier in this chapter say\n\nexample &lt;- emp_distr_height(4)\nexample$Height\n\n[1] 172.3432 168.8504 181.4084 176.1493\n\n\nLet us convert these values to standard units. Let us compute again the mean and the standard deviation of the height data.\n\nm &lt;- mean(data$Height)\nstd &lt;- sd(data$Height)\n\nm\n\n[1] 172.7025\n\nstd\n\n[1] 4.830264\n\n\n172.3432 is -0.0744 from the average. In standard units therefore 172.3432 is -0.0744.\n\n\n\n\n\n\nNow you try\n\n\n\nConvert the other values to standard units. Find the height which is 2.4 in standard units.\n\n\nStandard units are used in the DHS data on nutrition of children and adults which we worked with in the last exercise of chapter 3. Standard units are also often referred to as the z-score. Let us go back to these data and look at them more closely again.\nLet us go back to the histogram of human height we already discussed in the chapter 3.\n\n\nCode\nh &lt;- hist(data$Height, plot = F, breaks = 30)\n\n hist_breaks &lt;- h$breaks\n color_list &lt;- rep('#C6E2FF', length(hist_breaks))\n color_list[hist_breaks &lt;= m + std] &lt;- '#6C7B8B'\n color_list[hist_breaks &lt;= m - std] &lt;- '#C6E2FF'\n\npar(mar = c(5, 4, 4, 4) + 0.3)  \nplot(h, col = color_list, freq = FALSE, xlab = \" \", ylab = \"Percent per cm \", main = \"Height distribution of adults\", axes = F)\n\npar(new = TRUE) \n## Define x and y coordinates for normal curve\n x &lt;- seq(min(data$Height), max(data$Height), length.out = 100)\n y &lt;- dnorm(x, m, std)\n \n# Add normal curve to histogram plot\n lines(x, y, col = \"red\", lwd = 2,  xlab = \" \", ylab = \" \")\n axis(side = 4, at =c(0,0.02,0.04,0.06,0.08), labels = c(\"0%\", \"9.7 %\", \"19.3 %\", \"29.0 %\", \"38.6 %\"), ylab = \"Percent per standard unit\")\n \n\naxis(1, at = c(153.38, 158.21, 163.04, 167.87, 172.70, 177.53, 182.36, 187.19, 192.02), \n     labels = c(\"153.38 cm\", \"158.21 cm\", \"163.04 cm\", \"167.87 cm\", \"172.70 cm\", \"177.53 cm\", \"182.36 cm\", \"187.19 cm\", \"192.02 cm\"))\n\naxis(1, at = c(153.38, 158.21, 163.04, 167.87, 172.70, 177.53, 182.36, 187.19, 192.02), \n     labels = c(\"-4\", \"-3\", \"-2\", \"-1\", \"0\", \"1\", \"2\", \"3\", \"4\"), line = 2.5)\n\n\n\naxis(2, at =c(0,0.02,0.04,0.06,0.08), labels = c(\"0 %\", \"2 %\", \"4 %\", \"6 %\", \"8 %\"))\n\n\n\n\n\nThere are two x-axis in this figure. On the upper x-axis you see the height in cm. On the lower x-axis you see the height in standard units. Standard unit 1 is at 172.70 + 4.83 or at 177.53 and standard unit -1 is at 172.70 - 4.83 or 167.87.\nThere are two vertical axis in this figure, one on the left and one on the right. The histogram is drawn relative to the left y-axis, which is in percent per cm. The normal curve is drawn relative to the right one. To see how the scale on the right y-axis matches up, take the top value on each of the two axes: 38.6 % per standard unit matches 8% per cm because there are 4.8 cm to the standard unit. Spreading 38.6 % over one standard deviation is the same as spreading 38.6 % over 4.8cm and that comes to 8% per cm.\nWe explained before that the area under the normal curve between - 1 and 1 standard deviation from the mean is 68 %. In histogram the area between these values is colored in gray. The historgam follows the normal curve very closely. Sometimes it is a bit above, sometimes a bit below the curve but these deviations balance out. Indeed, if you go back to the previous chapter you see that in the empirical distribution we have indeed 68% of observations exactly between these bounds. So the shaded area in the histogram is to a very good approximation equal to the area under the normal curve and this is where the 68 % come from.\n\n\n4.4.3 Exercises: Now you try\n\nOn a cetrain exam, the average of the scores was 50 and the standard deviation was 10.\n\nConvert each of the following scores to standard units: 60, 45, 75.\nFind the scores which in standard units are: 0, 1.5, -2.8\n\nConvert each entry on the following list to standard units (using the average and standard deviation of the list): 13, 9, 11, 7, 10. Find the average and the standard devition of the converted list."
  },
  {
    "objectID": "from_limited_data_to_populations.html#finding-areas-under-the-normal-curve-with-r",
    "href": "from_limited_data_to_populations.html#finding-areas-under-the-normal-curve-with-r",
    "title": "4  From limited data to populations",
    "section": "4.5 Finding areas under the normal curve with R",
    "text": "4.5 Finding areas under the normal curve with R\nBefore the computer became a widely available tool, statisticians used standard tables for finding areas under the curve. Today this is done with a computer. R has built in standard function you can use for this purpose. In this section I show you how.\nExample 1:\nSay we want to find the area under the normal curve between 0 an 1. Now one way to find this area is to use you knowledge that 68 % of the area under the nomal curve are between - 1 and 1. By the symmetry of the normal curve thus the area between 0 and 1 must be half of this area or 34 %.\nWhen you use R to do this calculation the built in function is called pnorm(). This function gives you the area under the normal curve smaller or equal to a given value \\(x\\). The default value for the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\) is \\(\\mu = 0\\) and \\(\\sigma = 1\\).\nSo what we would do is to compute:\n\npnorm(1) - pnorm(0)\n\n[1] 0.3413447\n\n\nwhich gives 0.34 or 34 %.\n\n\n\nExample 1: Area under the normal curve between 0 and 1\n\n\nExample 2:\nFind the area between -2 and 1 under the normal curve with \\(\\mu = 0\\) and \\(\\sigma = 1\\).\nThis problem can be broken down by first computing the area under the normal curve for all values lower than 1 and then the area under the curve for all values lower than - 2 and subtract the latter from the former. In R:\n\npnorm(1) - pnorm(-2)\n\n[1] 0.8185946\n\n\nor 82 %.\n\n\n\nExample 1: Area under the normal curve between -2 and 1\n\n\nExample 3:\nFind the area to the right of 1 under the normal curve. This can be done by finding the area under the normal curve left from 1 first. Since the total area is 1, it must be the case that 1 minus the value of the area left from 1 is the same as the area right of 1.\n\n\nCode\n1 - pnorm(1)\n\n\n[1] 0.1586553\n\n\nor 16 %.\n\n\n\nExample 1: Area under the normal curve right of 1\n\n\n\n4.5.1 Exercises: Now you try\n\nFind the area under the normal curve:\n\n\nto the right of 1.25\nto the left of 0.80\nbetween -0.3 and 0.9\nto the left of -0.4\nbetween 0.4 and 1.3\noutside -1.5 to 1.5\n\n\nFill in the blanks:\n\n\nThe area between +/- under the normal curve equals 68%\nThe area between +/- under the normal curve equals 75 %."
  },
  {
    "objectID": "from_limited_data_to_populations.html#the-normal-approximation-for-data-1",
    "href": "from_limited_data_to_populations.html#the-normal-approximation-for-data-1",
    "title": "4  From limited data to populations",
    "section": "4.6 The normal approximation for data",
    "text": "4.6 The normal approximation for data\nIn certain cases the normal curve may be used to approximate data. We go back to our example with the data on the height of adult humans.\nSay we have the information that the height of adult humans were on average 172.7 cm with a standard deviation of 4.83 cm. Assuming the distribution of the data can be approximated by a normal curve. What is the estimated percentage of humans with a height between 168 and 178 cm?\nWe can proceed in two steps. First we convert to standard units:\n\nl &lt;- (168 - 172.7)/4.83\nu &lt;- (178 - 172.7)/4.83\nl\n\n[1] -0.9730849\n\nu\n\n[1] 1.097308\n\n\nThen use the pnorm() function to estimate the area:\n\npnorm(u) - pnorm(l)\n\n[1] 0.6984912\n\n\nwhich is about 70 %.\nLet us check this approximation with the real data. We use logical subsetting to count the share of height values between \\(l\\) and \\(u\\).\n\nidx &lt;- (data$Height &gt;= 168 & data$Height &lt;= 178)\nmean(idx)\n\n[1] 0.69868\n\n\nYou see that in this case the approximation works really well. So using the normal approximation we can get a fairly precise generalization from study population to the target population, even though the target population is larger than our study population. What we have done is that we have replaced the histogram of the empirical data by a normal curve before computing areas.\nNot all histograms follow a normal curve. But it is a remarkable fact, which we will explore in more detail later in the course, that many histograms do. In this case the mean and the standard deviation are good summary statistics."
  },
  {
    "objectID": "from_limited_data_to_populations.html#percentiles",
    "href": "from_limited_data_to_populations.html#percentiles",
    "title": "4  From limited data to populations",
    "section": "4.7 Percentiles",
    "text": "4.7 Percentiles\nWhile the mean and the standard deviation are good statistics to summarize data which follow a normal distribution, they are not so suited to summarize other distributions.\nLet us go back to one of these distributions we have encountered before: Primary energy consumption per capita in the world in the year 2019.\n\n\nCode\nlibrary(JWL)\ndat &lt;- with(energy_consumption_per_capita, energy_consumption_per_capita[Year == 2019, ])\n\nhist_info &lt;- hist(dat$Cons, plot = FALSE)         # Store output of hist function\nhist_info$counts &lt;- hist_info$counts /    # Compute relative frequency values\n  sum(hist_info$counts) * 100\nplot(hist_info, freq = TRUE, xlab = \"Primary energy consumption in kilowatt-hours per person per year.\", ylab = \"Percent\", main = \"Primary energy Consuption per Capita 2019\")              # Plot histogram with percentages\n\n\n\n\n\nThe average primary energy consumption was 26883.02 kwh per person per year and a standard deviation of 33562.48 kwh per person per year.\nIf we would just blindly apply a normal approximation using these statistics we would conclude that 21.15 % of countries in the world had a negative primary energy consumption. This does not make sense.\nThe reason for this nonsensical result is that the distribution depicted by the histogram does not follow a normal distribution. To summarize such distributions, statisticians often use so called percentiles.\n\nround(quantile(dat$Cons, c(0.01,0.10,0.25,0.50,0.75,0.90,0.99)),2)\n\n       1%       10%       25%       50%       75%       90%       99% \n   249.66   1147.96   3992.01  16516.14  33573.17  64784.08 165031.69 \n\n\nLet us first explain what you see in the code snippet. R has a built in functions to compute percentiles. This function is called quantile()and takes the data as the first argumente and then a vector of percentiles in decimal form. For example quantile(dat$Cons, 0.01) would compute the 1 % percentile.\nWhat does this mean? The first percentile in our primary energy consumption distribution is 249.66. This means that in about 1 % of the countries of the world the primiary energy consumption per person per year is 250 kwh or less. 99 % of the countries had a primary energy consumption above that level. The 50 % percentile (0.5) is just the median. The interquartile range is the difference between the 75 % percentile (0.75) and the 25 % percentile (0.25). This is sometimes used as a measure of spread for distributions with long tails, such as in this example.\nSometimes data are distributed like a normal curve and sometimes they are not. We will later discuss a theoretical argument which helps to explain when precisely histograms should be approximated well by a normal curve."
  },
  {
    "objectID": "from_limited_data_to_populations.html#summary",
    "href": "from_limited_data_to_populations.html#summary",
    "title": "4  From limited data to populations",
    "section": "4.8 Summary",
    "text": "4.8 Summary"
  },
  {
    "objectID": "from_limited_data_to_populations.html#exercises",
    "href": "from_limited_data_to_populations.html#exercises",
    "title": "4  From limited data to populations",
    "section": "4.9 Exercises",
    "text": "4.9 Exercises"
  },
  {
    "objectID": "from_limited_data_to_populations.html#exercises-with-computer",
    "href": "from_limited_data_to_populations.html#exercises-with-computer",
    "title": "4  From limited data to populations",
    "section": "4.10 Exercises with Computer",
    "text": "4.10 Exercises with Computer\nOpen the notebook from_limited_data_to_populations.ipynb and work on the exercises. Write your solutions into the notebook. You will need the computer to do.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "modelling_relationships_using_regression.html#contents",
    "href": "modelling_relationships_using_regression.html#contents",
    "title": "6  Modelling relationships using regression",
    "section": "6.1 Contents",
    "text": "6.1 Contents\nRegression models provide a mathematical representation between a set of explanatory variables and a response.\n\nThe coefficients in a regression represent how much we expect the response to change when the explanatory variable is observed to change.\nRegression to the mean\nRegression models can incorporate different types of response variable\nExplanatory variables and non-linear relationships\nBe cautious in interpreting models and don’t take them literally."
  },
  {
    "objectID": "modelling_relationships_using_regression.html#outcome",
    "href": "modelling_relationships_using_regression.html#outcome",
    "title": "6  Modelling relationships using regression",
    "section": "6.2 Outcome",
    "text": "6.2 Outcome\nThe students should understand the concept of regression and how it works and should be correctly interpreted. They should develop a good understanding that a method like regression does not provide an automatism for making predictions and will always need cautious interpretation. The students should learn some tools and example what cautious interpretation means and what is helpful in this respect."
  },
  {
    "objectID": "algorithmic_prediction.html#contents",
    "href": "algorithmic_prediction.html#contents",
    "title": "7  Algorithmic prediction",
    "section": "7.1 Contents",
    "text": "7.1 Contents\nAlgorithms built from data can be used for classification and prediction in technological applications.\n\nImportance of guarding an algorithm against over fitting\nAlgorithms can be evaluated according to classification accuracy, their ability to discriminate between groups and their overall predictive accuracy\nComplex algorithms can lack transparency and it may be worth trading off some accuracy for comprehension.\nThere are many challenges in using algorithms and machine learning, be aware of them."
  },
  {
    "objectID": "algorithmic_prediction.html#outcome",
    "href": "algorithmic_prediction.html#outcome",
    "title": "7  Algorithmic prediction",
    "section": "7.2 Outcome",
    "text": "7.2 Outcome\nWith respect to algorithmic prediction the students should have seen what it is and see a not too complex example, for instance in classification. They should be able to see the close similarity between regression and machine learning methods and be able to understand the jargon. Both regression and machine learning use sometimes different notions for the same thing."
  },
  {
    "objectID": "how_sure_can_we_be.html#contents",
    "href": "how_sure_can_we_be.html#contents",
    "title": "8  How sure can we be about what is going on",
    "section": "8.1 Contents",
    "text": "8.1 Contents\nProbability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable."
  },
  {
    "objectID": "how_sure_can_we_be.html#outcome",
    "href": "how_sure_can_we_be.html#outcome",
    "title": "8  How sure can we be about what is going on",
    "section": "8.2 Outcome",
    "text": "8.2 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "9  Probability: Quantifying uncertainty and variablility",
    "section": "",
    "text": "10 Contents\nProbability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable.\n\n\n11 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability_and_statistics.html#contents",
    "href": "probability_and_statistics.html#contents",
    "title": "10  Putting probability and statistics together",
    "section": "10.1 Contents",
    "text": "10.1 Contents\nThis will be conceptually the most difficult part of the course. The main ideas that should be conveyed in this unit are\n\nUsing probability theory we can derive the sampling distribution of summary statistics from which formulae for confidence intervals can be derived.\nExplain what a 95 % confidence interval means\nThe central limit theorem and the normal distribution\nThe role of systematic error due to non random causes and the role of judgment\nExplain the idea that confidence intervals can be calculated even when we observe all the data which then represent uncertainty about the parameters of an underlying metaphorical population."
  },
  {
    "objectID": "probability_and_statistics.html#outcome",
    "href": "probability_and_statistics.html#outcome",
    "title": "10  Putting probability and statistics together",
    "section": "10.2 Outcome",
    "text": "10.2 Outcome\nThe students should gain a firm understanding of confidence intervals and how they help us in quantifying uncertainty of predictions we make based on our available data. They should see and understand how and why it is sometimes more convenient and parsimonious to have formulae for confidence intervals rather than quantifying the uncertainty from simulation. The intuitive understanding of the limit theorems and when they can be legitimately applied will be important here."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html#contents",
    "href": "answering_questions_and_claiming_discoveries.html#contents",
    "title": "11  Answering questions and claiming discoveries",
    "section": "11.1 Contents",
    "text": "11.1 Contents\nFormal statistical testing as a major empirical tool for answering questions and claiming discoveries.\n\nTests of null hypothesis as a major part of statistical practice\np-value as the measure of incompatibility between the observed data and the null hypothesis\nThe traditional p value thresholds.\nThe need to adjust thresholds with multiple tests\nCorrespondence between p-values and confidence intervals\nNeyman-Pearson theory (alternative hypothesis and type 1 and type 2 error).\nSequential testing\nThe misinterpretation of p-values."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html#outcomes",
    "href": "answering_questions_and_claiming_discoveries.html#outcomes",
    "title": "11  Answering questions and claiming discoveries",
    "section": "11.2 Outcomes",
    "text": "11.2 Outcomes\nStudents should learn the basic ideas of hypothesis testing and the terminology around it."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Antony, Volk, and Atkinson Jeremy. 2013. “Infant and Child Death\nin the Human Environment of Evolutionary Adaptation.”\nEvolution and Human Behavior 34: 182–92.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for\nCommunication. New Riders.\n\n\nHellebrand, Tomas, and Paolo Mauro. 2015. “The Future of Worldwide\nIncome Distribution.” Brooking Institution Working Paper\nSeries 7 (15). file:///home/martinsummer/R/Statistics_JWL/data/income/Hellebrand_Mauro_wp15-7.pdf.\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nMcElrath, Richard. 2020. Statistical Resthinking. CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nRoser, Max. 2019. “Child Mortality Is an Everyday Tragedy of\nEnormous Scale That Rarely Makes the Headlines.” https://ourworldindata.org/child-mortality-everyday-tragedy-no-headlines.\n\n\nRosling, Hans, Ola Rosling, and Anna Rosling-Rönnlund. 2018.\nFactfullness, 10 Reasons Why We Are Wrong about the World - and Why\nThings Are Better Than You Think. Sceptre.\n\n\nSmil, Vaclav. 2020. Numbers Don’t Lie: 71 Things You Need to Know\nabout the World. Penguin Books.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from\nData. Pelican Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "what_causes_what.html",
    "href": "what_causes_what.html",
    "title": "5  What causes what?",
    "section": "",
    "text": "6 Observational studies"
  },
  {
    "objectID": "what_causes_what.html#correlation-does-not-imply-causation",
    "href": "what_causes_what.html#correlation-does-not-imply-causation",
    "title": "5  What causes what?",
    "section": "5.1 Correlation does not imply causation",
    "text": "5.1 Correlation does not imply causation\nMany data sources we have just rely on education policy outcomes somebody has observed and recorded, so called observational studies. For instance, there is a large literature, mainly from before the mid 1990ies in education research, which tried to answer the following question: Do increased financial resources for public schools improve educational outcomes as measured by standard education achievement tests?\nA look across countries would - for instance - give the following picture.\n\n\nCode\nplot(expenditure_outcome_dat$Expenditure, \n     expenditure_outcome_dat$Outcome, \n     main = (\"Average learning outcomes by total education expenditure per capita\"), \n     xlab = (\"Public and private per capita expenditure on education (PPP, constant 2011-intl $)\"), \n     ylab = (\"Average harmonized learning outcome in 2005 - 2015\"), pch = 16)\n\naxis(1, at = seq(500, 3500, by = 500))\n\n\n\n\n\nIndeed a study by Larry Hedges (1994) confirmed patterns like these. There is strong evidence that education spending and educational outcomes are positively correlated. Remember our discussion of measuring the relation between two variables in section 3.5?\nBut does this evidence support the conclusion that there is a causal relation between increased educational spending and student outcomes? There is a straightforward reason why this can not be the case.\nWe compare very different students or schools across countries in the scatter plot we produced before. In the study by Larry Hedges (1994) the variables that were related were students from different households across different schools. In the plot we show here it is students from different households accross different counties. For the general point we want to make here, this is immaterial. What matters is that simply comparing outcomes of students across countries or students across schools with different levels of spending does not tell us whether the different spending levels are causal for the variation in outcomes we observe. There may be many other differences between countries and students that are shaping this relationship.33 There are whole websites documenting examples where two variables are highly correlated but it is obvious that there can not be a causal relation. These examples are quite funny. One site you can find for example here: https://tylervigen.com/spurious-correlations\nStatisticians have summarized this observation in the mantra that “correlation does not imply causation”. If we want to establish a causal relation from data we need to work harder."
  },
  {
    "objectID": "what_causes_what.html#what-does-it-mean-that-a-variable-is-causal",
    "href": "what_causes_what.html#what-does-it-mean-that-a-variable-is-causal",
    "title": "5  What causes what?",
    "section": "5.2 What does it mean that a variable is “causal”?",
    "text": "5.2 What does it mean that a variable is “causal”?\nWhen you ask a doctor today about the risks of smoking, he will usually give you a warning and advise against it. One of the arguments will be that the evidence shows that smoking causes lung cancer.\nBut assume you have been a smoker and now come into the unfortunate situation that you are diagnosed with lung cancer. How do you know that your lung cancer occurred, because you were smoking and you would not have got lung cancer anyway, perhaps for an entirely different reason?\nIt took the medical profession decades to accept the causality of smoking of lung cancer? The many studies that were undertaken on this subject since the 1930es usually compared people who smoke with non-smokers and tried to take conclusions about the effect from this comparison. In this comparisons the smokers came out badly, showing among many other things higher incidence of lung cancer. So many studies together established a strong correlation between smoking and lung cancer. If smoking cause lung cancer, this would be an explanation of the association between smoking and lung cancer. But this evidence is only circumstantial. There could be some hidden factor - so called confounding factor - which makes people smoke and also makes them get lung cancer. In this case there would be no point in quitting smoking.\nThe famous statistician Sir R.A. Fisher doubted the causal role of cigarettes for lung cancer and suggested possible confounding factors. Epidemiologists conducted careful studies that accumulated evidence that these confounding factors were not plausible. Over time the evidence taken together convinced the medical community that smoking is indeed causal for lung cancer. But causal here has a meaning that takes into account the unavoidable variability that underlies real life.\nThe statistical notion of causation or causality is not deterministic. If statisticians say there is evidence that smoking is causal for lung cancer it does not mean that every smoker will end up with lung cancer. It does also not mean that lung cancer can occur only if you are a smoker. It rather means that among the group of smokers lung cancer will occur more often. Thus we can not use statistical methods to establish causation in a specific case. The only thing we can possibly establish by data is that smoking increases the proportion of times lung cancer will occur in a population.\nThis has two important consequences for what needs to be done if we want to find out what causes what. We need to intervene, that is consciously change a variable, and perform experiments. But since we are in a situation of uncertainty we need to intervene more than once, to establish strong enough evidence.\nFor this reason much of research in medicine and epidemiology today relies on evidence gained from so called controlled experiments. Let us explain next what the ideas behind such an approach is. These ideas have also been extended to other field, like the research on what works in policies attempting to improve education and to other fields."
  },
  {
    "objectID": "what_causes_what.html#controlled-experiments",
    "href": "what_causes_what.html#controlled-experiments",
    "title": "5  What causes what?",
    "section": "5.3 Controlled experiments",
    "text": "5.3 Controlled experiments"
  },
  {
    "objectID": "what_causes_what.html#field-trials-for-early-polio-vaccines-in-the-us-in-the-1950ies",
    "href": "what_causes_what.html#field-trials-for-early-polio-vaccines-in-the-us-in-the-1950ies",
    "title": "5  What causes what?",
    "section": "5.4 Field trials for early polio vaccines in the US in the 1950ies",
    "text": "5.4 Field trials for early polio vaccines in the US in the 1950ies\nLet us take an example from medicine first. Suppose a new drug is found and we would like to introduce it as a medication that can be prescribed by doctors to their patients.\nThe basic idea of testing the effectiveness of the medicine is comparison. An experiment is designed in which the medicine is given to a treatment group. But there are other subjects that are not treated and form another group, the control group. Then the responses of the two groups are compared. The assignemt of subjects to both groups should be random and the experiment should be run double blind: Neither the subjects nor the researchers who measure the response should know who was in the control group.\nThe key of randomization in this protocol is that it keeps the two groups equal except for the treatment. Then, if you measure different outcomes you can conclude with some confidence that the observed difference must be the result of the treatment. This would then establish causality.\nOne of the first epidemiological studies implementing these ideas was a randomized controlled experiment to show the effectiveness of a vaccine against polio in the US in 1954 . It was conducted by the Public Health Service and the National Foundation for Infantile Paralysis (NFPI).\nThe field trial was conducted in select school districts among the children in the age group most vulnerable. Children could, however, only be vaccinated with their parents consent, so only children whose parents agreed to the vaccination could go into the treatment group. Since parents with higher income would were more likely to consent to treatment. This biases the design against the vaccine, since polio is a disease of hygiene. Children in more hygienic surroundings have less exposure to mild cases of polio in early childhood. This prevents the generation of antibodies which protect them against severe infections at a later age.\nMany experts recognized the flaw in the design. The assignment to treatment and control was not random and thus the treatment and control group were systematically different. The idea, however, is that both groups should be as similar as possible except for treatment. If the two groups differ to some factor other than treatment this factor my confound or get mixed up with the effect of treatment.\nTo exclude the bias in assignment introduced by parental consent in the NFPI design, it was suggested that the control group has to chosen from the same population as the treatment group using an impartial chance procedure. Such an approach is called randomized controlled.\nThe idea of double blindness was also already used in a new improved design. The children in the control group were given an injection of salt dissolved in water rather than the vaccine, a so called placebo. During the experiment therefore the subjects did not know whether they were in the control group or in the treatment group. Also the doctors were not told which group the child belonged to and neither knew those who evaluated the responses.\nA subsequent program evaluation published in a scientific journal (Francis (1955)), showed the bias in the NFPI study in comparison with the randomized controlled double blind experiment:\n\n\nCode\nRCT &lt;- data.frame(Group = c(\"Treatment\", \"Control\", \"No consent\"), Size = c(200000, 200000, 350000), Rate = c(28, 71, 46))\n\nNFIP &lt;- data.frame(Group = c(\"Grade 2 (vaccine)\", \"Grades 1 and 2 (control)\", \"Grade 2 (no consent\"), Size = c(225000, 725000, 125000), Rate = c(25, 54, 44))\n\nlibrary(knitr)\n\n# table on the left\nkable(RCT)\n# table on the right\nkable(NFIP)\n\n\n\n\n\nRandomized Trial\n\n\nGroup\nSize\nRate\n\n\n\n\nTreatment\n200000\n28\n\n\nControl\n200000\n71\n\n\nNo consent\n350000\n46\n\n\n\n\n\n\nNFPI study\n\n\nGroup\nSize\nRate\n\n\n\n\nGrade 2 (vaccine)\n225000\n25\n\n\nGrades 1 and 2 (control)\n725000\n54\n\n\nGrade 2 (no consent\n125000\n44\n\n\n\n\n\n\nResults of the 1954 polio vaccine trials\n\n\n\nThe two tables show the results of the randomized trial design with the NFPI design. The size of groups and the rates of polio cases per 100.000 in each group are shown in the columns Size and Rate.\nIn the randomized controlled trial the vaccination cut the polio rate from 71 to 28 per hundred thousand. The NFPI study - by contrast - shows a reduction from 54 to 25 per hundred thousand. The source of the bias which accounts for the difference was confounding. The NFPI treatment group contained only only children whose partents consented to the vaccination. However the control group contained also children whose partents would not have consented. Therefore control and treatment group were not comparable."
  },
  {
    "objectID": "what_causes_what.html#school-subsidies-for-the-poor-the-mexican-progresa-poverty-program",
    "href": "what_causes_what.html#school-subsidies-for-the-poor-the-mexican-progresa-poverty-program",
    "title": "5  What causes what?",
    "section": "5.5 School subsidies for the poor: The Mexican progresa poverty program",
    "text": "5.5 School subsidies for the poor: The Mexican progresa poverty program\nUse article by Schultz in the JDE: Illustrate the core elements of a RCT again in this case."
  },
  {
    "objectID": "what_causes_what.html#how-do-randomized-controlled-trials-differ-from-observational-studies",
    "href": "what_causes_what.html#how-do-randomized-controlled-trials-differ-from-observational-studies",
    "title": "5  What causes what?",
    "section": "6.1 How do randomized controlled trials differ from observational studies?",
    "text": "6.1 How do randomized controlled trials differ from observational studies?"
  },
  {
    "objectID": "what_causes_what.html#labor-market-consequences-of-schooling-data-from-indonesia",
    "href": "what_causes_what.html#labor-market-consequences-of-schooling-data-from-indonesia",
    "title": "5  What causes what?",
    "section": "6.2 Labor market consequences of schooling: Data from Indonesia",
    "text": "6.2 Labor market consequences of schooling: Data from Indonesia\nUse article by Esther Duflo in AER to show what is needed to establish causality in an observational study.\n\n\n\n\nBanerjee, Abhijit, and Esther Duflo. 2012. Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty. Public Affairs.\n\n\nFrancis, Thomas. 1955. “An Evaluation of the 1954 Poliomyelitis Vaccine Trials - Summary Report.” American Journal of Public Health 45: 1–63.\n\n\nLarry Hedges, Rob Greenwald, Richard Laine. 1994. “Does Money Matter? A Meta-Analysis of Studies of the Effects of Differential School Inputs on Student Outcomes.” Educational Researcher 3 (23): 5–14.\n\n\nRoser, Max. 2021. “Access to Basic Education: Almost 60 Million Children of Primary School Age Are Not in School.” https://ourworldindata.org/children-not-in-school."
  }
]