[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics",
    "section": "",
    "text": "These are lecture notes for my introductory statistics and data literarcy course I am developing for Jesuit worldwide learning (JWL) together with my colleagues from Seitwerk."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "What is the biggest source of electricity production in Kenya? Answering this question needs data listing and recording various forms of electricity production in specific countries. An internationally acknowledged organisation, which collects and reports these data is the International Energy Agency (IEA).\nNow let’s take a quiz and guess. What do you think is actually the biggest source of electricity production in Kenya?\nIf you are not familiar with the details of electricity production using geothermal- and hydropower or if you are not completely sure how to interpret a number like 38 %, don’t worry for now. The point here is that you see that one useful consequence of being able to access, read and interpret data is that it can help to establish facts about the world. In this way data can help us to perhaps correct misconceptions we might have had about these facts.\nIn this course you will learn how to work with data and how to learn from these data in a systematic way.\nLearning from data entails more than just establishing facts. This might not always be possible, either because you cannot access the relevant data or you cannot completely access them, since doing so would be way too expensive. When working with data you also need rigorous definitions of concepts, so that you can transform your experience about the world into data\nThink for a moment about the following interesting example, which I learned from a wonderful book by the British statistician David Spiegelhalter (Spiegelhalter 2019). The example shows that even just categorizing and labeling things in the world to measure them and turn them into data can be challenging. A very basic question raised in the introduction of this book is:\nBut even with the definition at hand you cannot just go around the planet and count every plant that meets the criteria. So this is what the researchers investigating that question did according to (Spiegelhalter 2019):\n“…They first took a series of of areas with a common type of landscape, known as a biome and counted the average number of trees per square kilometer. They then used satellite imaging to estimate the total area of the planet covered by each type of biome, carried out some complex statistical modelling, and eventually came up with an estimate of 3.04 trillion (3,040,000,000,000) trees on the planet. This sounds a lot, except that they reckoned there used to be twice this number.”\nNow imagine that if long expert discussions are needed to precisely define something so seemingly obvious as a tree, clearly more complex concepts such as unemployment or the definition of the total value of goods and services produced in a country in a given year, known as Gross Domestic Product or GDP, is even more challenging.\nThere is no automatism or mechanical receipt how we can turn experience into data and the statistics that we use and produce are constructed on the basis of judgement. It is a starting point for a deeper understanding of the world around us. This is one of the reasons why in this course statistics is not only referred to as science, which it arguably is to some degree, but also as an art.\nOne of the limitations of data as a source of knowledge about the world is that anything we choose to measure will differ across places, across persons or across time. When analyzing and trying to understand data we will always face the problem how we can extract meaningful insights from this apparent random variation. One challenge faced by statistics and one core topic in this course will thus be how we can distinguish in data the important relationships from the unimportant background variability.\nExploring and finding such meaningful relationships or patterns in data using the science of statistics and computational tools is one of the skills you will learn in this course.\nAn example of a pattern in data is if we can spot a trend, data values which are for example increasing or decreasing.\nLet us have a look at a table showing the first 10 observations of this share\nThe table shows that the share of people living in extreme poverty has been decreasing year after year for the first 10 years since the year 2000. This is a pattern which is called a downward trend.\nNote that we did not show the whole series of numbers. The data points in our data-set actually range until the year 2017. Printing them all in a table quickly produces very large and unwieldy number array which is awkward to read.\nExploring data and detecting patterns is usually easier when we use the power of the human visual system. Humans are very good in finding visual patterns. Looking for patterns is almost viscerall for us. We can’t help but looking for patterns. This almost instinctive human urge can also be misleading and suggesting patterns to us where there are in fact none. In data exploration we can make use of the power of visualization by plotting data and looking at them graphically. The modern computer has made this form of displaying data particularly easy and powerful and visualizing data in data exploration is another core skill you are going to learn in this course.\nSo let us visualize the world poverty data. In our plot we draw the year on the x-axes and the share of people living in extreme poverty on the y-axes. This will give us a point for each year. To facilitate the spotting of a trend, we connect the annual observations by a line.\nBut does this trend mean that extreme poverty must disappear some years down the road? No, because nobody can tell whether the trend of the last two decades will go on also in the future.\nStatistics can help us to think more systematically about patterns and in making systematic guesses how a pattern might continue in the future. This is another core skill you will learn in this course: Making predictions, which means using available data and information to make informed guesses about data and information we do not have.\nLet us go back to the share of people in the world living in extreme poverty. As reported in our data the last actual observation for the global share in extreme poverty from the world bank is from 2017. There are more recent data for some regions but the global data since then are by now forecasts based on statistical techniques. The basic ideas of these techniques and how to apply them to data is a core skill you will learn in this course.\nNow what does the World bank predict for the share of extreme poverty in the world? Let us look at the data again graphically to visualize the prediction.\nObserve that the line showing the share of extreme poverty in the world takes different shapes as we make predictions. What does this mean?\nAt the root of the predictions is an abstract model, how the share of poverty changes over time. If the underlying data would correspond to a world before Covid the falling trend in poverty would just continue to fall, as it has done continuously from 2000 onward. In the graph you can see this scenario if you follow the black and the blue line. But taking the pandemic and the consequences into account the prediction of the World Bank is that extreme poverty after a almost two decades downward trend will rise again. This you can see by following the green and the red line. How much, this rise actually will be in the end depends on data we can not yet know.\nWhen we make informed guesses based on observed data on information we do not yet have there is uncertainty involved. Using the theory of probability in combination with statistics we can quantify this uncertainty. Quantifying the uncertainty attached to predictions is the third basic skill you will learn in this course."
  },
  {
    "objectID": "introduction.html#the-course-broken-down-by-units",
    "href": "introduction.html#the-course-broken-down-by-units",
    "title": "1  Introduction",
    "section": "1.1 The course broken down by units",
    "text": "1.1 The course broken down by units\nIn this course you will learn these three basic skills that will allow you to achieve a level of data literacy supporting your future studies and providing you with powerful know how for your future professional life in various fields. The modern world is full of data. But reading these data with a critical mind and being able to extract information from them you need to know statistics and its basic techniques. This is exactly what you will learn in this course if you actively participate and work through all the exercises and tasks as well as the assigned projects.\nThe course is split into 8 units in total.\n\n\n\n\n\n\nUnit 1: Overview; Categorical Data and Proportions\n\n\n\nIn unit 1 we will give an overview of the course and we begin with the analysis and understanding of binary variables, variables that can be imagined as simple yes or no questions and how they can be summarized as proportions or percentages. You will learn about how the idea of expected frequency will promote the understanding of the meaning of these shares and how this provides a basic understanding of the importance of these numbers.\n\n\n\n\n\n\n\n\nUnit 2: Summarizing and Communication lots of data; From limited data to populations\n\n\n\nIn unit 2 we learn how to deal with lots of data, the typical situation we will face when we do statistics. When there are lots of data we need instruments and tools to summarize them and to get an overview. This overview is usually also very important for communicating the data and the information they might contain. We will also learn how we can use statistics to learn properties about large populations by only making limited observations on some appropriately chosen subset of individuals from this population. The gold standard in making such inferences possible is the concept of a so called random sample. But at each step of the sampling procedure bias can crop up and invalidate our results leading to wrong conclusions. The circumstances under which inference from samples to populations can be made and how we can make sure to minimize bias we need a firm understanding of the opportunities and limits of this important technique.\n\n\n\n\n\n\n\n\nUnit 3: What causes what?\n\n\n\nIn unit 3 we discuss when data analysis allows us to say something about what causes what. We learn about the important concept of randomized trials. We will also learn what an observational study is and how it differs from a randomized trial. This is an important concept we will learn through the discussion of real world examples.\n\n\n\n\n\n\n\n\nUnit 4: Modelling relationships using regression and algorithmic predictions\n\n\n\nIn unit 4 we will learn a key concept needed to make predictions. The technical term for this concept in statistics is regression. It is a simple mathematical model describing how a set of explanatory variables varies systematically with a response variable. We will learn to construct such models and to interpret them correctly. We will also cover related techniques which have become very important recently and entered the statistical toolbox from the field of computer science. These methods are known under the notion of algorithmic prediction or machine learning.\n\n\n\n\n\n\n\n\nUnit 5: How sure can we be about what is going on: Estimates and Intervals.\n\n\n\nIn unit 5 we encounter the first time tools for quantifying uncertainty. We learn how to determine and use uncertainty intervals by using a technique which is called the bootstrap. Being able to determine such intervals is extremely important in communicating statistics and for supporting a systematic and sound answer to the question: How sure can we be about an estimate.\n\n\n\n\n\n\n\n\nUnit 6: Probability: Quantifying uncertainty and variability\n\n\n\nIn unit 6 we deepen the knowledge how to quantify uncertainty by introducing basic ideas of probability theory. Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable.\n\n\n\n\n\n\n\n\nUnit 7: Putting Probability and Statistics together\n\n\n\nIn unit 7 we put statistics and probability theory together. This allows us to both simplify ideas and techniques how to quantify uncertainty. The combination of the two field makes the tools for quantifying uncertainty at the same time more powerful. Combining statistics and probability theory is at the heart of statistics as a science. It makes all the ideas developed in unit 1 to 6 very versatile and powerful.\n\n\n\n\n\n\n\n\nUnit 8: Answering questions and claiming discoveries\n\n\n\nIn unit 8 we learn how to leverage the knowledge of this course to answer questions and to claim discoveries. You will learn how statistics is used in the sciences and how it supports to develop our knowledge of the world. It pulls many ideas of the whole course together and when you have mastered this unit you have mastered all the basic skills we want to develop in this course, data exploration, prediction and quantifying uncertainty."
  },
  {
    "objectID": "introduction.html#activity-guessing-ages-cancer-etc.",
    "href": "introduction.html#activity-guessing-ages-cancer-etc.",
    "title": "1  Introduction",
    "section": "1.2 Activity: Guessing ages, cancer etc.",
    "text": "1.2 Activity: Guessing ages, cancer etc.\n\n\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "categorical_data_and_proportions.html",
    "href": "categorical_data_and_proportions.html",
    "title": "2  Categorical Data and Proportions",
    "section": "",
    "text": "Introduce binary variables, variables that can be imagined as yes-no questions and how they can be summarized as proportions.\n\nThe importance of framing\nRelative and absolute risk\nHow to think in expected frequencies can promote understanding and provide an appropriate sense of importance, what are odds rations, why we need to understand what they mean and why we should avoid them in communication. All these concepts and discussions should be accompanied by data visualization and students will learn how to visualize the data on the computer."
  },
  {
    "objectID": "categorical_data_and_proportions.html#outcome",
    "href": "categorical_data_and_proportions.html#outcome",
    "title": "2  Categorical Data and Proportions",
    "section": "2.2 Outcome",
    "text": "2.2 Outcome\nThe outcome of learning from the introduction and this chapter, which together form unit 1, students should have an overview of\n\nThe contents of the course\nShould have formed expectations that this course will actively involve them and require their hands on participation\nKnow how to start and stop R (Python) and R studio (Jupyter notebooks) and have played with one or two meaningful visualizations right away.\n\nThe way to achieve this is to prepare a visualization code in an interactive notebook, which students need not understand in all details. But they can change details, for instance if the variables contain countries, they could change the code such that the data for their own country are shown and rerun the visualization code.\nAfter this unit the students should feel familiar with proportions and their meaning and interpretation. We will also have gathered data from an activity which will be used later in the course."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html",
    "href": "summarizing_and_communicating_lots_of_data.html",
    "title": "3  Summarizing and communicating lots of data",
    "section": "",
    "text": "Statistics usually involves lots of data and we need ways to communicate and summarize these data. This chapter introduces the most important concepts.\n\nThe empirical distribution of data points\nMeasures of location and spread.\nSkewed data distributions are common and some summary statistics are very sensitive to outlying values.\nSummaries always hide some detail.\nHow to summarize sets of numbers graphically (histograms and box plots)\nUseful transformations to reveal patterns\nLooking at pairs of numbers, scatter plots, time series as line graphs.\nThe primary aim in data exploration is to get an idea of the overall variation."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#outcome",
    "href": "summarizing_and_communicating_lots_of_data.html#outcome",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.2 Outcome",
    "text": "3.2 Outcome\nUnderstand these concepts and work through may examples showing how to apply these summary measures to data on the computer."
  },
  {
    "objectID": "from_limited_data_to_populations.html",
    "href": "from_limited_data_to_populations.html",
    "title": "4  From limited data to populations",
    "section": "",
    "text": "Inductive inference requires working from data to study sample and study population to target population.\nAt each stage bias can crop up. The best way to proceed from sample to study population is if you have drawn a random sample. Introduce the idea that a population can be thought of as a group of individuals but also as a probability distribution for a random observation drawn from that population Populations can be summarized using parameters that mirror the summary statistics of sample data. It often occurs that data do not arise as a sample from a literal population. We can always imagine data as drawn from a metaphorical population of events that could have occurred but didn’t."
  },
  {
    "objectID": "from_limited_data_to_populations.html#outcome",
    "href": "from_limited_data_to_populations.html#outcome",
    "title": "4  From limited data to populations",
    "section": "4.2 Outcome",
    "text": "4.2 Outcome\nMake the concept of a random sample and a probability distribution tangible by using the computer."
  },
  {
    "objectID": "what_causes_what.html",
    "href": "what_causes_what.html",
    "title": "5  What causes what?",
    "section": "",
    "text": "In this lecture we discuss what causation means in a statistical sense, why we need be careful to distinguish between causation and correlation.\n\nCausation in a statistical sense means that when we intervene, the chances of different outcomes vary systematically.\nCausation is difficult to establish statistically, but well designed randomized trials are the best available framework. 3.Principles that helped clinical trials to identify effects. 4, Observational data may have background factors influencing the apparent relationships between exposure and outcome which may be either observed confounders or lurking factors.\nStatistical methods do not suspend judgment which is always required for the confidence with which causation can be claimed."
  },
  {
    "objectID": "what_causes_what.html#outcome",
    "href": "what_causes_what.html#outcome",
    "title": "5  What causes what?",
    "section": "5.2 Outcome",
    "text": "5.2 Outcome\nThis is a conceptually difficult topic. It is, however, not technically difficult but it will need lots of examples. Fortunately there are many good (and bad) real world examples that I hope will stick with the students as reference examples after the course. Students should understand the idea of randomized trials and observational studies as well as the general ideas of comparison in statistical analysis."
  },
  {
    "objectID": "modelling_relationships_using_regression.html",
    "href": "modelling_relationships_using_regression.html",
    "title": "6  Modelling relationships using regression",
    "section": "",
    "text": "Regression models provide a mathematical representation between a set of explanatory variables and a response.\n\nThe coefficients in a regression represent how much we expect the response to change when the explanatory variable is observed to change.\nRegression to the mean\nRegression models can incorporate different types of response variable\nExplanatory variables and non-linear relationships\nBe cautious in interpreting models and don’t take them literally."
  },
  {
    "objectID": "modelling_relationships_using_regression.html#outcome",
    "href": "modelling_relationships_using_regression.html#outcome",
    "title": "6  Modelling relationships using regression",
    "section": "6.2 Outcome",
    "text": "6.2 Outcome\nThe students should understand the concept of regression and how it works and should be correctly interpreted. They should develop a good understanding that a method like regression does not provide an automatism for making predictions and will always need cautious interpretation. The students should learn some tools and example what cautious interpretation means and what is helpful in this respect."
  },
  {
    "objectID": "algorithmic_prediction.html",
    "href": "algorithmic_prediction.html",
    "title": "7  Algorithmic prediction",
    "section": "",
    "text": "Algorithms built from data can be used for classification and prediction in technological applications.\n\nImportance of guarding an algorithm against over fitting\nAlgorithms can be evaluated according to classification accuracy, their ability to discriminate between groups and their overall predictive accuracy\nComplex algorithms can lack transparency and it may be worth trading off some accuracy for comprehension.\nThere are many challenges in using algorithms and machine learning, be aware of them."
  },
  {
    "objectID": "algorithmic_prediction.html#outcome",
    "href": "algorithmic_prediction.html#outcome",
    "title": "7  Algorithmic prediction",
    "section": "7.2 Outcome",
    "text": "7.2 Outcome\nWith respect to algorithmic prediction the students should have seen what it is and see a not too complex example, for instance in classification. They should be able to see the close similarity between regression and machine learning methods and be able to understand the jargon. Both regression and machine learning use sometimes different notions for the same thing."
  },
  {
    "objectID": "how_sure_can_we_be.html",
    "href": "how_sure_can_we_be.html",
    "title": "8  How sure can we be about what is going on",
    "section": "",
    "text": "Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable."
  },
  {
    "objectID": "how_sure_can_we_be.html#outcome",
    "href": "how_sure_can_we_be.html#outcome",
    "title": "8  How sure can we be about what is going on",
    "section": "8.2 Outcome",
    "text": "8.2 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "9  Probability: Quantifying uncertainty and variablility",
    "section": "",
    "text": "11 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability_and_statistics.html",
    "href": "probability_and_statistics.html",
    "title": "10  Putting probability and statistics together",
    "section": "",
    "text": "This will be conceptually the most difficult part of the course. The main ideas that should be conveyed in this unit are\n\nUsing probability theory we can derive the sampling distribution of summary statistics from which formulae for confidence intervals can be derived.\nExplain what a 95 % confidence interval means\nThe central limit theorem and the normal distribution\nThe role of systematic error due to non random causes and the role of judgment\nExplain the idea that confidence intervals can be calculated even when we observe all the data which then represent uncertainty about the parameters of an underlying metaphorical population."
  },
  {
    "objectID": "probability_and_statistics.html#outcome",
    "href": "probability_and_statistics.html#outcome",
    "title": "10  Putting probability and statistics together",
    "section": "10.2 Outcome",
    "text": "10.2 Outcome\nThe students should gain a firm understanding of confidence intervals and how they help us in quantifying uncertainty of predictions we make based on our available data. They should see and understand how and why it is sometimes more convenient and parsimonious to have formulae for confidence intervals rather than quantifying the uncertainty from simulation. The intuitive understanding of the limit theorems and when they can be legitimately applied will be important here."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html",
    "href": "answering_questions_and_claiming_discoveries.html",
    "title": "11  Answering questions and claiming discoveries",
    "section": "",
    "text": "Formal statistical testing as a major empirical tool for answering questions and claiming discoveries.\n\nTests of null hypothesis as a major part of statistical practice\np-value as the measure of incompatibility between the observed data and the null hypothesis\nThe traditional p value thresholds.\nThe need to adjust thresholds with multiple tests\nCorrespondence between p-values and confidence intervals\nNeyman-Pearson theory (alternative hypothesis and type 1 and type 2 error).\nSequential testing\nThe misinterpretation of p-values."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html#outcomes",
    "href": "answering_questions_and_claiming_discoveries.html#outcomes",
    "title": "11  Answering questions and claiming discoveries",
    "section": "11.2 Outcomes",
    "text": "11.2 Outcomes\nStudents should learn the basic ideas of hypothesis testing and the terminology around it."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from\nData. Pelican Books."
  }
]