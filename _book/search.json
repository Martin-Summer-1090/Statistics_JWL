[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics",
    "section": "",
    "text": "These are lecture notes for my introductory statistics and data literarcy course I am developing for Jesuit worldwide learning (JWL) together with my colleagues from Seitwerk."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Welcome to the course Statistics: The art and science of learning from data. In this introductory chapter you will get an overview of what you will learn in this course. We will also start with some examples, which will require your participation and collect our first data sets which will accompany us throughout this course."
  },
  {
    "objectID": "introduction.html#experiencing-the-world-through-data-exploration-prediction-and-quantification-of-uncertainty",
    "href": "introduction.html#experiencing-the-world-through-data-exploration-prediction-and-quantification-of-uncertainty",
    "title": "1  Introduction",
    "section": "1.1 Experiencing the world through data: Exploration, Prediction and Quantification of Uncertainty",
    "text": "1.1 Experiencing the world through data: Exploration, Prediction and Quantification of Uncertainty\nWhat is the biggest source of electricity production in Kenya? Answering this question needs data listing and recording various forms of electricity production in specific countries. An internationally acknowledged organisation, which collects and reports these data is the International Energy Agency (IEA).\n\n\n\n\n\n\nAccessing IEA data on the internet\n\n\n\nIf you have access to the internet, you can look up data and reports by the IEA on it’s website https://www.iea.org/\n\n\nNow let’s take a quiz and guess. What do you think is actually the biggest source of electricity production in Kenya?\n\nCoal\nRenewable energy\nNatural gas\n\nConsulting the IEA data, you will find that the correct answer is renewable energy. A very interesting website called gapminder, which analyzes the answers of many people to this question, finds that 61 % give a wrong answer to this question. Maybe they find it hard to imagine that 80 % of energy production in Kenya is already fossil free thanks to huge sources of geothermal- and hydropower.1 Even if you break the answers down by country you see that 38 % of people from Kenya get the answer wrong. Among the people from the UK, who answer this question even 72 % answer wrongly.1 Geothermal electricity generation uses the earth’s natural heating energy - geothermal energy. A country needs to be located on a geothermal hot spot to make effective use of this energy source for electricity generation. At such a hotspot there are high temperatures beneath the earth’s surface which naturally produces steam. This steam can be used to spin turbines connected to a generator. This mechanism then produces electricity. Hydropower uses the water cycle to generate electricity by using dams to alter the flow of a river. The kinetic energy of the water spins turbines connected to a generator which produces electricity.\nIf you are not familiar with the details of electricity production using geothermal- and hydropower or if you are not completely sure how to interpret a number like 38 %, don’t worry for now. The point here is that you see that one useful consequence of being able to access, read and interpret data is that it can help to establish facts about the world. In this way data can help us to perhaps correct misconceptions we might have had about these facts.\n\n\n\n\n\n\nThe gapminder webpage\n\n\n\nIf you have internet access you can reach the gapminder webpage at https://www.gapminder.org/). I encourage you to visit this page at an occasion when you have access to the internet. You will be surprised how often you might have a wrong guess about basic facts in the world.\n\n\nIn this course you will learn how to work with data and how to learn from these data in a systematic way.\nLearning from data entails more than just establishing facts. This might not always be possible, either because you cannot access the relevant data or you cannot completely access them, since doing so would be way too expensive. When working with data you also need rigorous definitions of concepts, so that you can transform your experience about the world into data\nThink for a moment about the following interesting example, which I learned from a wonderful book by the British statistician David Spiegelhalter (Spiegelhalter 2019). The example shows that even just categorizing and labeling things in the world to measure them and turn them into data can be challenging. A very basic question raised in the introduction of this book is:\n\n\n\n\n\n\nQuestion:\n\n\n\nHow many trees are there on the planet?\n\n\nIt is clear that answering this question is more challenging than the task the IEA had to solve when listing energy sources by country around the world in a given time period. But before you go about to think how you might count all the trees on the planet, you have to answer an even more basic question, namely: What is a tree? Some of you might think this is a silly and obvious question, which every child can answer. But what some might consider a tree others will consider just a shrub. Turning experience into data requires rigorous definitions. It turns out that such definitions can be given for trees. 22 For example the forestry expert Michael Kuhns writes: “…Though no scientific definition exists to separate trees and shrubs, a useful definition for a tree is a woody plant having one erect perennial stem (trunk) at least three inches in diameter at a point 4-1/2 feet above the ground, a definitely formed crown of foliage, and a mature height of at least 13 feet. This definition works fine, though some trees may have more than one stem and young trees obviously don’t meet the size criteria. A shrub can then be defined as a woody plant with several perennial stems that may be erect or may lay close to the ground. It will usually have a height less than 13 feet and stems no more than about three inches in diameter.” (Kuhns, n.d.)\nBut even with the definition at hand you cannot just go around the planet and count every plant that meets the criteria. So this is what the researchers investigating that question did according to (Spiegelhalter 2019):\n“…They first took a series of of areas with a common type of landscape, known as a biome and counted the average number of trees per square kilometer. They then used satellite imaging to estimate the total area of the planet covered by each type of biome, carried out some complex statistical modelling, and eventually came up with an estimate of 3.04 trillion (3,040,000,000,000) trees on the planet. This sounds a lot, except that they reckoned there used to be twice this number.”\nNow imagine that if long expert discussions are needed to precisely define something so seemingly obvious as a tree, clearly more complex concepts such as unemployment or the definition of the total value of goods and services produced in a country in a given year, known as Gross Domestic Product or GDP, is even more challenging.\nThere is no automatism or mechanical receipt how we can turn experience into data and the statistics that we use and produce are constructed on the basis of judgement. It is a starting point for a deeper understanding of the world around us. This is one of the reasons why in this course statistics is not only referred to as science, which it arguably is to some degree, but also as an art.\nOne of the limitations of data as a source of knowledge about the world is that anything we choose to measure will differ across places, across persons or across time. When analyzing and trying to understand data we will always face the problem how we can extract meaningful insights from this apparent random variation. One challenge faced by statistics and one core topic in this course will thus be how we can distinguish in data the important relationships from the unimportant background variability.\nExploring and finding such meaningful relationships or patterns in data using the science of statistics and computational tools is one of the skills you will learn in this course.\nAn example of a pattern in data is if we can spot a trend, data values which are for example increasing or decreasing.\nConsider the following data from the World Bank, reporting the share of people in the world who are living in extreme poverty. Extreme poverty is defined by the World Bank, an international development finance organisation for low and middle income countries located in the US3 as the percentage of people in the world who have to live on less that $ 1.90 per day.3 The Worldbank is an international financial institution founded along with the International Monetary Fund in the Bretton Wods conference in 1944. It is located in Washington D.C. and finances projects in low and middle income countries. It also collects and processes data globally to support its activities and conduct development research. The Worldbank makes its data public in print or through its website https://www.worldbank.org/en/home\nLet us have a look at a table showing the first 10 observations of this share\n\n\nCode\n# read poverty data from our project data folder\npovdat_by_country <- read.csv(\"data/extreme_poverty/share-of-population-in-extreme-poverty.csv\")\n# select the years from 2000\npovdat_world <- with(povdat_by_country, povdat_by_country[Entity == \"World\" & Year >= 2000, ])\n# Keep only the year and the share\nplot_data <- povdat_world[,c(3,4)]\n# Rename variables\nnames(plot_data) <- c(\"Year\", \"Share\")\n\n# produce a table\nlibrary(knitr)\nkable(head(plot_data, n= 10), row.names = F, digits = 1)\n\n\n\n\nTable 1.1: Share of world polpulation living in extreme poverty, Source: World Bank\n\n\nYear\nShare\n\n\n\n\n2000\n27.8\n\n\n2001\n26.9\n\n\n2002\n25.7\n\n\n2003\n24.7\n\n\n2004\n22.9\n\n\n2005\n21.0\n\n\n2006\n20.3\n\n\n2007\n19.1\n\n\n2008\n18.4\n\n\n2009\n17.6\n\n\n\n\n\n\nThe table shows that the share of people living in extreme poverty has been decreasing year after year for the first 10 years since the year 2000. This is a pattern which is called a downward trend.\nNote that we did not show the whole series of numbers. The data points in our data-set actually range until the year 2017. Printing them all in a table quickly produces very large and unwieldy number array which is awkward to read.\nExploring data and detecting patterns is usually easier when we use the power of the human visual system. Humans are very good in finding visual patterns. Looking for patterns is almost viscerall for us. We can’t help but looking for patterns. This almost instinctive human urge can also be misleading and suggesting patterns to us where there are in fact none. In data exploration we can make use of the power of visualization by plotting data and looking at them graphically. The modern computer has made this form of displaying data particularly easy and powerful and visualizing data in data exploration is another core skill you are going to learn in this course.\nSo let us visualize the world poverty data. In our plot we draw the year on the x-axes and the share of people living in extreme poverty on the y-axes. This will give us a point for each year. To facilitate the spotting of a trend, we connect the annual observations by a line.\n\n\nCode\nlibrary(ggplot2)\n\np <- ggplot(plot_data, aes(x = Year, y = Share)) + \n     geom_line() +\n     geom_point() +\n     xlab(\"\")\np\n\n\n\n\n\nFigure 1.1: Share of world population living in extreme poverty from 2000 - 2017\n\n\n\n\nVisualizing trends in world extreme poverty as an example of data exploration. In this case the data pattern reveals a stunning fact. Over almost two decades we can see a sharp fall in the share of extremely poor people when looked at from a global perspective. Of course when we drill down to the level of individual countries this trend will not look the same everywhere and there might be countries where the share has actually increased. But overall we have seen a breathtaking steady decline. This is good news.44 When you have access to the internet you can have a closer look at these data at the very interesting website “our world in data” maintained by a consortium of Oxford University and University College London. See https://ourworldindata.org/extreme-poverty. The website has many interesting visualizations and options to select individual countries, country aggregates and make other selections of the data.\nBut does this trend mean that extreme poverty must disappear some years down the road? No, because nobody can tell whether the trend of the last two decades will go on also in the future.\nStatistics can help us to think more systematically about patterns and in making systematic guesses how a pattern might continue in the future. This is another core skill you will learn in this course: Making predictions, which means using available data and information to make informed guesses about data and information we do not have.\nLet us go back to the share of people in the world living in extreme poverty. As reported in our data the last actual observation for the global share in extreme poverty from the world bank is from 2017. There are more recent data for some regions but the global data since then are by now forecasts based on statistical techniques. The basic ideas of these techniques and how to apply them to data is a core skill you will learn in this course.\nNow what does the World bank predict for the share of extreme poverty in the world? Let us look at the data again graphically to visualize the prediction.\n\n\nCode\nobsdat <- data.frame(Year = plot_data$Year, Scenario = rep(\"Observed\", 18), Share = plot_data$Share)\n\nadd_dat <- data.frame(Year = 2018, Scenario = \"Observed\", Share = 8.6)\n\npred_dat_precovid <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Pre-Covid-19\", 4), Share = c(8.6, 8.4, 7.9, 7.5))\n\npred_dat_covidbase <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Covid-19-Baseline\", 4), Share = c(8.6, 8.4, 9.1, 8.9))\n\npred_dat_coviddown <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Covid-19-Downside\", 4), Share = c(8.6, 8.4, 9.4, 9.4))\n\ndat <- rbind(obsdat, add_dat, pred_dat_precovid, pred_dat_covidbase, pred_dat_coviddown)\n\np <- ggplot(dat, aes(x = Year, y = Share, group = Scenario, color = Scenario)) + \n  geom_line(alpha = 0.5)+\n  geom_point()+\n  scale_color_manual(values=c('green', 'red', 'black', 'blue'))\np\n\n\n\n\n\nFigure 1.2: Share of world population living in extreme poverty from 2000 - 2017 with predictions until 2021\n\n\n\n\nObserve that the line showing the share of extreme poverty in the world takes different shapes as we make predictions. What does this mean?\nAt the root of the predictions is an abstract model, how the share of poverty changes over time. If the underlying data would correspond to a world before Covid the falling trend in poverty would just continue to fall, as it has done continuously from 2000 onward. In the graph you can see this scenario if you follow the black and the blue line. But taking the pandemic and the consequences into account the prediction of the World Bank is that extreme poverty after a almost two decades downward trend will rise again. This you can see by following the green and the red line. How much, this rise actually will be in the end depends on data we can not yet know.\nWhen we make informed guesses based on observed data on information we do not yet have there is uncertainty involved. Using the theory of probability in combination with statistics we can quantify this uncertainty. Quantifying the uncertainty attached to predictions is the third basic skill you will learn in this course.\n\n\n\n\n\n\nThe three basic skills you will learn in this course\n\n\n\nAfter successfully completing this course students you will have learned three core skills:\n\nData exploration or finding patterns in data and information through visualisation and computation.\nMaking predictions, which means using available data and information to make informed guesses about data and information we do not have.\nTo quantify the uncertainty we have to attach to our predictions."
  },
  {
    "objectID": "introduction.html#the-course-broken-down-by-units",
    "href": "introduction.html#the-course-broken-down-by-units",
    "title": "1  Introduction",
    "section": "1.2 The course broken down by units",
    "text": "1.2 The course broken down by units\nIn this course you will learn these three basic skills that will allow you to achieve a level of data literacy supporting your future studies and providing you with powerful know how for your future professional life in various fields. The modern world is full of data. But reading these data with a critical mind and being able to extract information from them you need to know statistics and its basic techniques. This is exactly what you will learn in this course if you actively participate and work through all the exercises and tasks as well as the assigned projects.\nThe course is split into 8 units in total.\n\n\n\n\n\n\nUnit 1: Overview; Categorical Data and Proportions\n\n\n\nIn unit 1 we will give an overview of the course and we begin with the analysis and understanding of binary variables, variables that can be imagined as simple yes or no questions and how they can be summarized as proportions or percentages. You will learn about how the idea of expected frequency will promote the understanding of the meaning of these shares and how this provides a basic understanding of the importance of these numbers.\n\n\n\n\n\n\n\n\nUnit 2: Summarizing and Communication lots of data; From limited data to populations\n\n\n\nIn unit 2 we learn how to deal with lots of data, the typical situation we will face when we do statistics. When there are lots of data we need instruments and tools to summarize them and to get an overview. This overview is usually also very important for communicating the data and the information they might contain. We will also learn how we can use statistics to learn properties about large populations by only making limited observations on some appropriately chosen subset of individuals from this population. The gold standard in making such inferences possible is the concept of a so called random sample. But at each step of the sampling procedure bias can crop up and invalidate our results leading to wrong conclusions. The circumstances under which inference from samples to populations can be made and how we can make sure to minimize bias we need a firm understanding of the opportunities and limits of this important technique.\n\n\n\n\n\n\n\n\nUnit 3: What causes what?\n\n\n\nIn unit 3 we discuss when data analysis allows us to say something about what causes what. We learn about the important concept of randomized trials. We will also learn what an observational study is and how it differs from a randomized trial. This is an important concept we will learn through the discussion of real world examples.\n\n\n\n\n\n\n\n\nUnit 4: Modelling relationships using regression and algorithmic predictions\n\n\n\nIn unit 4 we will learn a key concept needed to make predictions. The technical term for this concept in statistics is regression. It is a simple mathematical model describing how a set of explanatory variables varies systematically with a response variable. We will learn to construct such models and to interpret them correctly. We will also cover related techniques which have become very important recently and entered the statistical toolbox from the field of computer science. These methods are known under the notion of algorithmic prediction or machine learning.\n\n\n\n\n\n\n\n\nUnit 5: How sure can we be about what is going on: Estimates and Intervals.\n\n\n\nIn unit 5 we encounter the first time tools for quantifying uncertainty. We learn how to determine and use uncertainty intervals by using a technique which is called the bootstrap. Being able to determine such intervals is extremely important in communicating statistics and for supporting a systematic and sound answer to the question: How sure can we be about an estimate.\n\n\n\n\n\n\n\n\nUnit 6: Probability: Quantifying uncertainty and variability\n\n\n\nIn unit 6 we deepen the knowledge how to quantify uncertainty by introducing basic ideas of probability theory. Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable.\n\n\n\n\n\n\n\n\nUnit 7: Putting Probability and Statistics together\n\n\n\nIn unit 7 we put statistics and probability theory together. This allows us to both simplify ideas and techniques how to quantify uncertainty. The combination of the two field makes the tools for quantifying uncertainty at the same time more powerful. Combining statistics and probability theory is at the heart of statistics as a science. It makes all the ideas developed in unit 1 to 6 very versatile and powerful.\n\n\n\n\n\n\n\n\nUnit 8: Answering questions and claiming discoveries\n\n\n\nIn unit 8 we learn how to leverage the knowledge of this course to answer questions and to claim discoveries. You will learn how statistics is used in the sciences and how it supports to develop our knowledge of the world. It pulls many ideas of the whole course together and when you have mastered this unit you have mastered all the basic skills we want to develop in this course, data exploration, prediction and quantifying uncertainty."
  },
  {
    "objectID": "introduction.html#on-the-use-of-the-computer",
    "href": "introduction.html#on-the-use-of-the-computer",
    "title": "1  Introduction",
    "section": "1.3 On the use of the computer",
    "text": "1.3 On the use of the computer\nOur approach to teach you basic ideas of statistics and data analysis will be very much problem and activity oriented. The application of specific statistical techniques will be only one component in a howl package of activities you will need to engage in when you work with data in real world applications. Preparing data appropriately for analysis as well as communicating the conclusions for your analysis will be important elements of the whole process of statistical analysis. Today these activities involve using a computer. In this course you will also learn how to use the computer. It will play an important role for developing your skills. We will make no assumptions of prior knowledge of computers and programming and will introduce the use of the computer step by step. Using a computer requires a language in which we can tell the computer what to do and which the computer can understand. The language of our choice for this course is called R. It is one of the most widely used and most powerful languages for data analysis and statistics. We will introduce you to the language and its use step by step as we go along and in parallel with the statistical concepts we develop."
  },
  {
    "objectID": "introduction.html#activities-in-the-study-center",
    "href": "introduction.html#activities-in-the-study-center",
    "title": "1  Introduction",
    "section": "1.4 Activities in the study center",
    "text": "1.4 Activities in the study center\n\n1.4.1 Visualizing the share of extremely poor people for different countries\nHere we assume that students have a running R and R-Studio installation on their laptops at the study center. We would give the code in a notebook with the code chunk shown in the source file, to play with.\nLets go back to figure Figure 1.1 for a moment. This figure has been created by the use of the computer.\nIn the following box you see the computer code that has read the data from a file and then plotted the share for the world and for a particular country, say China. Don’t worry if you do not (yet) understand the details of the code. Think of it as a language that tells the computer what to do with the data. You can edit the code and delete China and insert another country instead. If you click the green arrow at the upper right corner of the box the computer will run the code again and generate a new graphic.\nTry to play and experiment with the code in this way to see what happens. Soon you will know yourself how to make interesting and beautiful data visualizations yourself.\n\n\nCode\nlibrary(ggplot2)\n\ndemo_data <- read.csv(\"data/extreme_poverty/share-of-population-in-extreme-poverty.csv\")\n\nnames(demo_data) <- c(\"Country\", \"Code\", \"Year\", \"Share\")\n\npl_dat <- with(demo_data, demo_data[Country %in% c(\"World\", \"China\") & Year >= 2000, ])\n\np <- ggplot(pl_dat, aes(x = Year, y = Share, color = Country)) +\n     geom_point() +\n     geom_line() +\n     xlab(\"\")\n\np\n\n\n\n\n\n\n\n1.4.2 Guessing ages\nThis is an exercise which needs the leadership of an instructor at the study center. It would be great to collect the data of the exercise in a readable file for later use in the course.\nWe have 10 photos of persons whose age is known to us but not to the students. We divide students into 10 groups A through J. Each group gets one of the photos. The students in each group are asked to estimate the age of the person in their photograph and write the guess into a form. Each group must come up with a single estimate.\nExplain that each group will be estimating the ages of all 10 photos and that groups are competing to get the lowest error. Each group passes its card to the next group (A to B, B to C, etc. J back to A) and estimates the age of the new photo. This is continued until each group has seen all photos.\nThe data are written in a table where the rows are groups and the columns are card numbers. We can discuss the expected accuracy of the guesses (within how many years do you think you can guess). Then start with card 1 and ask groups to give their guess, then reveal the true age and write it at the bottom margin of the first column.\nIntroduce the concept of error - guessed age minus actual age - and wrote the errors in place of the guessed ages. Then fill the whole table. Ask the students to compute the absolute average error.\nStudents get an indea about: uncertainty, empirical analysis and data display. There are many statistical ideas in this game and the data can be taken up throughout the course (variance, bias, experimental design, randomization, linear regression, two-way tables, statistical significance).\n\n\n1.4.3 Where are the cancers\n\n\n1.4.4 Estimating a big number\n\n\n1.4.5 Collect data from students.\n\n\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "categorical_data_and_proportions.html",
    "href": "categorical_data_and_proportions.html",
    "title": "2  Categorical Data and Proportions",
    "section": "",
    "text": "Introduce binary variables, variables that can be imagined as yes-no questions and how they can be summarized as proportions.\n\nThe importance of framing\nRelative and absolute risk\nHow to think in expected frequencies can promote understanding and provide an appropriate sense of importance, what are odds rations, why we need to understand what they mean and why we should avoid them in communication. All these concepts and discussions should be accompanied by data visualization and students will learn how to visualize the data on the computer."
  },
  {
    "objectID": "categorical_data_and_proportions.html#outcome",
    "href": "categorical_data_and_proportions.html#outcome",
    "title": "2  Categorical Data and Proportions",
    "section": "2.2 Outcome",
    "text": "2.2 Outcome\nThe outcome of learning from the introduction and this chapter, which together form unit 1, students should have an overview of\n\nThe contents of the course\nShould have formed expectations that this course will actively involve them and require their hands on participation\nKnow how to start and stop R (Python) and R studio (Jupyter notebooks) and have played with one or two meaningful visualizations right away.\n\nThe way to achieve this is to prepare a visualization code in an interactive notebook, which students need not understand in all details. But they can change details, for instance if the variables contain countries, they could change the code such that the data for their own country are shown and rerun the visualization code.\nAfter this unit the students should feel familiar with proportions and their meaning and interpretation. We will also have gathered data from an activity which will be used later in the course."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html",
    "href": "summarizing_and_communicating_lots_of_data.html",
    "title": "3  Summarizing and communicating lots of data",
    "section": "",
    "text": "Statistics usually involves lots of data and we need ways to communicate and summarize these data. This chapter introduces the most important concepts.\n\nThe empirical distribution of data points\nMeasures of location and spread.\nSkewed data distributions are common and some summary statistics are very sensitive to outlying values.\nSummaries always hide some detail.\nHow to summarize sets of numbers graphically (histograms and box plots)\nUseful transformations to reveal patterns\nLooking at pairs of numbers, scatter plots, time series as line graphs.\nThe primary aim in data exploration is to get an idea of the overall variation."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#outcome",
    "href": "summarizing_and_communicating_lots_of_data.html#outcome",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.2 Outcome",
    "text": "3.2 Outcome\nUnderstand these concepts and work through may examples showing how to apply these summary measures to data on the computer."
  },
  {
    "objectID": "from_limited_data_to_populations.html",
    "href": "from_limited_data_to_populations.html",
    "title": "4  From limited data to populations",
    "section": "",
    "text": "Inductive inference requires working from data to study sample and study population to target population.\nAt each stage bias can crop up. The best way to proceed from sample to study population is if you have drawn a random sample. Introduce the idea that a population can be thought of as a group of individuals but also as a probability distribution for a random observation drawn from that population Populations can be summarized using parameters that mirror the summary statistics of sample data. It often occurs that data do not arise as a sample from a literal population. We can always imagine data as drawn from a metaphorical population of events that could have occurred but didn’t."
  },
  {
    "objectID": "from_limited_data_to_populations.html#outcome",
    "href": "from_limited_data_to_populations.html#outcome",
    "title": "4  From limited data to populations",
    "section": "4.2 Outcome",
    "text": "4.2 Outcome\nMake the concept of a random sample and a probability distribution tangible by using the computer."
  },
  {
    "objectID": "what_causes_what.html",
    "href": "what_causes_what.html",
    "title": "5  What causes what?",
    "section": "",
    "text": "In this lecture we discuss what causation means in a statistical sense, why we need be careful to distinguish between causation and correlation.\n\nCausation in a statistical sense means that when we intervene, the chances of different outcomes vary systematically.\nCausation is difficult to establish statistically, but well designed randomized trials are the best available framework. 3.Principles that helped clinical trials to identify effects. 4, Observational data may have background factors influencing the apparent relationships between exposure and outcome which may be either observed confounders or lurking factors.\nStatistical methods do not suspend judgment which is always required for the confidence with which causation can be claimed."
  },
  {
    "objectID": "what_causes_what.html#outcome",
    "href": "what_causes_what.html#outcome",
    "title": "5  What causes what?",
    "section": "5.2 Outcome",
    "text": "5.2 Outcome\nThis is a conceptually difficult topic. It is, however, not technically difficult but it will need lots of examples. Fortunately there are many good (and bad) real world examples that I hope will stick with the students as reference examples after the course. Students should understand the idea of randomized trials and observational studies as well as the general ideas of comparison in statistical analysis."
  },
  {
    "objectID": "modelling_relationships_using_regression.html",
    "href": "modelling_relationships_using_regression.html",
    "title": "6  Modelling relationships using regression",
    "section": "",
    "text": "Regression models provide a mathematical representation between a set of explanatory variables and a response.\n\nThe coefficients in a regression represent how much we expect the response to change when the explanatory variable is observed to change.\nRegression to the mean\nRegression models can incorporate different types of response variable\nExplanatory variables and non-linear relationships\nBe cautious in interpreting models and don’t take them literally."
  },
  {
    "objectID": "modelling_relationships_using_regression.html#outcome",
    "href": "modelling_relationships_using_regression.html#outcome",
    "title": "6  Modelling relationships using regression",
    "section": "6.2 Outcome",
    "text": "6.2 Outcome\nThe students should understand the concept of regression and how it works and should be correctly interpreted. They should develop a good understanding that a method like regression does not provide an automatism for making predictions and will always need cautious interpretation. The students should learn some tools and example what cautious interpretation means and what is helpful in this respect."
  },
  {
    "objectID": "algorithmic_prediction.html",
    "href": "algorithmic_prediction.html",
    "title": "7  Algorithmic prediction",
    "section": "",
    "text": "Algorithms built from data can be used for classification and prediction in technological applications.\n\nImportance of guarding an algorithm against over fitting\nAlgorithms can be evaluated according to classification accuracy, their ability to discriminate between groups and their overall predictive accuracy\nComplex algorithms can lack transparency and it may be worth trading off some accuracy for comprehension.\nThere are many challenges in using algorithms and machine learning, be aware of them."
  },
  {
    "objectID": "algorithmic_prediction.html#outcome",
    "href": "algorithmic_prediction.html#outcome",
    "title": "7  Algorithmic prediction",
    "section": "7.2 Outcome",
    "text": "7.2 Outcome\nWith respect to algorithmic prediction the students should have seen what it is and see a not too complex example, for instance in classification. They should be able to see the close similarity between regression and machine learning methods and be able to understand the jargon. Both regression and machine learning use sometimes different notions for the same thing."
  },
  {
    "objectID": "how_sure_can_we_be.html",
    "href": "how_sure_can_we_be.html",
    "title": "8  How sure can we be about what is going on",
    "section": "",
    "text": "Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable."
  },
  {
    "objectID": "how_sure_can_we_be.html#outcome",
    "href": "how_sure_can_we_be.html#outcome",
    "title": "8  How sure can we be about what is going on",
    "section": "8.2 Outcome",
    "text": "8.2 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "9  Probability: Quantifying uncertainty and variablility",
    "section": "",
    "text": "11 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability_and_statistics.html",
    "href": "probability_and_statistics.html",
    "title": "10  Putting probability and statistics together",
    "section": "",
    "text": "This will be conceptually the most difficult part of the course. The main ideas that should be conveyed in this unit are\n\nUsing probability theory we can derive the sampling distribution of summary statistics from which formulae for confidence intervals can be derived.\nExplain what a 95 % confidence interval means\nThe central limit theorem and the normal distribution\nThe role of systematic error due to non random causes and the role of judgment\nExplain the idea that confidence intervals can be calculated even when we observe all the data which then represent uncertainty about the parameters of an underlying metaphorical population."
  },
  {
    "objectID": "probability_and_statistics.html#outcome",
    "href": "probability_and_statistics.html#outcome",
    "title": "10  Putting probability and statistics together",
    "section": "10.2 Outcome",
    "text": "10.2 Outcome\nThe students should gain a firm understanding of confidence intervals and how they help us in quantifying uncertainty of predictions we make based on our available data. They should see and understand how and why it is sometimes more convenient and parsimonious to have formulae for confidence intervals rather than quantifying the uncertainty from simulation. The intuitive understanding of the limit theorems and when they can be legitimately applied will be important here."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html",
    "href": "answering_questions_and_claiming_discoveries.html",
    "title": "11  Answering questions and claiming discoveries",
    "section": "",
    "text": "Formal statistical testing as a major empirical tool for answering questions and claiming discoveries.\n\nTests of null hypothesis as a major part of statistical practice\np-value as the measure of incompatibility between the observed data and the null hypothesis\nThe traditional p value thresholds.\nThe need to adjust thresholds with multiple tests\nCorrespondence between p-values and confidence intervals\nNeyman-Pearson theory (alternative hypothesis and type 1 and type 2 error).\nSequential testing\nThe misinterpretation of p-values."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html#outcomes",
    "href": "answering_questions_and_claiming_discoveries.html#outcomes",
    "title": "11  Answering questions and claiming discoveries",
    "section": "11.2 Outcomes",
    "text": "11.2 Outcomes\nStudents should learn the basic ideas of hypothesis testing and the terminology around it."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from\nData. Pelican Books."
  }
]