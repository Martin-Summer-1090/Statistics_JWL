[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics",
    "section": "",
    "text": "These are lecture notes for my introductory statistics and data literacy course I am developing for Jesuit Worldwide Learning (JWL) together with my colleagues from Seitwerk. I would like to thank Peter Balleis, Mathias Beck, Martha Habash, Stefan Hengst and Anny Mayr for all their generous help and support in kickstarting this challenging project.\nThe goal of the notes is to develop the core contents of the course systematically and to support the production of the online units. I also write up in these notes material and instructions for teaching material, which is meant to be used in the local study centers. My vision is that after we are through with the production of the online units, the notes will be overhauled and can then when the course actually begins, be used by the students as a textbook and reading material.\nFor the time beeing it is more like a systematic notebook and a scenario. I will add questions and comments for the Seitwerk team during this phase as collapsible callout notes like this:\n\n\n\n\n\n\nComment or Remark for Seitwerk\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user.\n\n\n\nThis helps me to keep a better overview of the overall development of the text.\nI will put the developing material on our MS-teams collaboration platform as we go along. Data and graphics will be put on the platform as separate files. In addition, mainly for the interaction with my past and future self, I keep a versioning of the notes on a public github archive, which may also be accessed by colleagues, and of course by the Seitwerk team. The github repository’s address is:\nhttps://github.com/Martin-Summer-1090/Statistics_JWL"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Welcome to the course Statistics: The art and science of learning from data. In this introductory chapter you will get an overview of the basic skills you will learn in this course by discussing a few examples.\nOver time and with your active participation these skills will add a powerful seventh sense to your ability of experiencing the world. It will allow you to experience the world through data. Understanding data by the art and science of statistics is an essential skill you will need to navigate the modern world and to study many fields in the sciences.\nTO DO: Overview of Unit 1 i.e. Introduction and Categorical Data"
  },
  {
    "objectID": "introduction.html#experiencing-the-world-through-data",
    "href": "introduction.html#experiencing-the-world-through-data",
    "title": "1  Introduction",
    "section": "1.1 Experiencing the world through data",
    "text": "1.1 Experiencing the world through data\n\n1.1.1 What are the major sources of electricity production in Kenya?\nWhat is the biggest source of electricity production in Kenya? Answering this question needs data, listing and recording various forms of electricity production in specific countries. An internationally acknowledged organisation, which collects and reports these data is the International Energy Agency (IEA).\n\n\n\n\n\n\nAccessing IEA data on the internet\n\n\n\nIf you have access to the internet, you can look up data and reports by the IEA on it’s website https://www.iea.org/11 A screenshot I have taken in August 2022 shows the website like this. In the upperm right part of the website there is a link called data which will bring you to the data collected by IEA. \n\n\nNow let’s take a quiz and guess. What do you think is actually the biggest source of electricity production in Kenya?\n\nCoal\nRenewable energy\nNatural gas\n\n\n\n\n\n\n\nSeitwerk: Expand for reading comment.\n\n\n\n\n\nIt would be great to make this like an online quiz, where you can click the answer and get right or wrong. Renewable energy is the right answer.\n\n\n\nConsulting the IEA data, you will find that the correct answer is renewable energy. A very interesting website called gapminder, which analyzes the answers of many people to this question, finds that 61 % give a wrong answer to this question. Maybe they find it hard to imagine that 80 % of energy production in Kenya is already fossil free thanks to huge sources of geothermal- and hydropower.2 Even if you break the answers down by country you see that 38 % of people from Kenya get the answer wrong. Among the people from the UK, who answer this question even 72 % answer wrongly.2 Geothermal electricity generation uses the earth’s natural heating energy - geothermal energy. A country needs to be located on a geothermal hot spot to make effective use of this energy source for electricity generation. At such a hotspot there are high temperatures beneath the earth’s surface which naturally produces steam. This steam can be used to spin turbines connected to a generator. This mechanism then produces electricity. Hydropower uses the water cycle to generate electricity by using dams to alter the flow of a river. The kinetic energy of the water spins turbines connected to a generator which produces electricity.\nIf you are not familiar with the details of electricity production using geothermal- and hydropower or if you are not completely sure how to interpret a number like 38 %, don’t worry for now. The point here is that you see that one useful consequence of being able to access, read and interpret data is that it can help to establish facts about the world. In this way data can help us to perhaps correct misconceptions we might have had about these facts.\n\n\n\n\n\n\nThe gapminder webpage\n\n\n\nIf you have internet access you can reach the gapminder webpage at https://www.gapminder.org/. I encourage you to visit this page at an occasion when you have access to the internet. You will be surprised how often you might have a wrong guess about basic facts in the world.33 Here is how the website looks like as of August 2022: \n\n\nIn this course you will learn how to work with data and how to learn from these data in a systematic way.\n\n\n1.1.2 How many trees are there on the planet?\nLearning from data entails more than just establishing facts. This might not always be possible, either because you cannot access the relevant data or you cannot completely access them, since doing so would be way too expensive. When working with data you also need rigorous definitions of concepts, so that you can actually transform your experience about the world into data\nThink for a moment about the following interesting example, which I learned from a wonderful book by the British statistician David Spiegelhalter (Spiegelhalter 2019). The example shows that even just categorizing and labeling things in the world to measure them and turn them into data can be challenging. A very basic question raised in the introduction of this book is:\n\n\n\n\n\n\nQuestion:\n\n\n\nHow many trees are there on the planet?\n\n\nIt is clear that answering this question is more challenging than the task the IEA had to solve when listing energy sources by country around the world in a given time period. But before you go about to think how you might count all the trees on the planet, you have to answer an even more basic question, namely: What is a tree?\nSome of you might think this is a silly and obvious question, which every child can answer. But what some might consider a tree others will consider just a shrub. Turning experience into data requires rigorous definitions. It turns out that such definitions can be given for trees. 44 For example the forestry expert Michael Kuhns writes: “…Though no scientific definition exists to separate trees and shrubs, a useful definition for a tree is a woody plant having one erect perennial stem (trunk) at least three inches in diameter at a point 4-1/2 feet above the ground, a definitely formed crown of foliage, and a mature height of at least 13 feet. This definition works fine, though some trees may have more than one stem and young trees obviously don’t meet the size criteria. A shrub can then be defined as a woody plant with several perennial stems that may be erect or may lay close to the ground. It will usually have a height less than 13 feet and stems no more than about three inches in diameter.” (Kuhns, n.d.)\nBut even with the definition at hand you cannot just go around the planet and count every plant that meets the criteria. So this is what the researchers investigating that question did according to (Spiegelhalter 2019):\n“…They first took a series of of areas with a common type of landscape, known as a biome and counted the average number of trees per square kilometer. They then used satellite imaging to estimate the total area of the planet covered by each type of biome, carried out some complex statistical modelling, and eventually came up with an estimate of 3.04 trillion (3,040,000,000,000) trees on the planet. This sounds a lot, except that they reckoned there used to be twice this number.”\nNow imagine that if long expert discussions are needed to precisely define something so seemingly obvious as a tree, clearly more complex concepts such as unemployment or the definition of the total value of goods and services produced in a country in a given year, known as Gross Domestic Product or GDP, is even more challenging.\nThere is no automatism or mechanical receipt how we can turn experience into data and the statistics that we use and produce are constructed on the basis of judgement. It is a starting point for a deeper understanding of the world around us. This is one of the reasons why in this course statistics is not only referred to as science, which it arguably is to some degree, but also as an art.\nTo guard against the trap of mindlessly fall into mechanical thinking or automatism it is sometimes helpful to make a rough plausibility estimates about the order of magnitude you might expect as a result for a really big number, like the number of trees on the globe.\n\n\n\n\n\n\nComment for Seitwerk: Please uncollapse\n\n\n\n\n\nHere would be a good opportunity to engage students in an activity estimating a big number. Since this has an interactive part, which better works when in class together, one could set up the problem with some guidance in the online unit and then discuss solutions in class. It might be fun to add a competitive element by splitting the students in teams or pairs and rewarding the team/pair who comes closest to the true value. Let them first just guess and then lead them through the guestimation a bit more systematically: One way to lead students through this exercise might be: How many people are there in country X, how many children of school age, how many of them ride the bus, how many students can be transported by a bus etc. The estimate will have lots of uncertainty but be hopefully closer to the truth than most of the original unguided guesses. It is a first encounter for the students with propagation of uncertainty, an important topic in statistics more generally. The activity contains further interesting aspects like reliability of data sources and the design of data collection. I am not quite sure whether and how to build this in at this stage but maybe you have an idea or suggestion.\nA good example would for instance be: How many school buses are there in country X?\n\n\n\n\n\n1.1.3 What has happened to extreme poverty on the globe in the last 20 years?\nOne of the limitations of data as a source of knowledge about the world is that anything we choose to measure will differ across places, across persons or across time. When analyzing and trying to understand data we will always face the problem how we can extract meaningful insights from this apparent random variation. One challenge faced by statistics and one core topic in this course will thus be how we can distinguish in data the important relationships from the unimportant background variability.\nExploring and finding such meaningful relationships or patterns in data using the science of statistics and computational tools is one of the skills you will learn in this course.\nAn example of a pattern in data is if we can spot a trend, data values which are for example increasing or decreasing.\nConsider the following data from the World Bank, reporting the share of people in the world who are living in extreme poverty. Extreme poverty is defined by the World Bank, an international development finance organisation for low and middle income countries located in the US5 as the percentage of people in the world who have to live on less that $ 1.90 per day.5 The Worldbank is an international financial institution founded along with the International Monetary Fund in the Bretton Wods conference in 1944. It is located in Washington D.C. and finances projects in low and middle income countries. It also collects and processes data globally to support its activities and conduct development research. The Worldbank makes its data public in print or through its website https://www.worldbank.org/en/home\nLet us have a look at a table showing the first 10 observations of this share\n\n\nCode\n# read poverty data from our project data folder\npovdat_by_country <- read.csv(\"data/extreme_poverty/share-of-population-in-extreme-poverty.csv\")\n# select the years from 2000\npovdat_world <- with(povdat_by_country, povdat_by_country[Entity == \"World\" & Year >= 2000, ])\n# Keep only the year and the share\nplot_data <- povdat_world[,c(3,4)]\n# Rename variables\nnames(plot_data) <- c(\"Year\", \"Share\")\n\n# produce a table\nlibrary(knitr)\nkable(head(plot_data, n= 10), row.names = F, digits = 1)\n\n\n\n\nTable 1.1: Share of world polpulation living in extreme poverty, Source: World Bank\n\n\nYear\nShare\n\n\n\n\n2000\n27.8\n\n\n2001\n26.9\n\n\n2002\n25.7\n\n\n2003\n24.7\n\n\n2004\n22.9\n\n\n2005\n21.0\n\n\n2006\n20.3\n\n\n2007\n19.1\n\n\n2008\n18.4\n\n\n2009\n17.6\n\n\n\n\n\n\nCode\nwrite.csv(head(plot_data, n= 10), file = \"tables/table_1_1_world_poverty.csv\", row.names = FALSE)\n\n\nThe table shows that the share of people living in extreme poverty has been decreasing year after year for the first 10 years since the year 2000. This is a pattern which is called a downward trend.\nNote that we did not show the whole series of numbers. The data points in our data-set actually range until the year 2017. Printing them all in a table quickly produces very large and unwieldy number array which is awkward to read.\nExploring data and detecting patterns is usually easier when we use the power of the human visual system. Humans are very good in finding visual patterns. Looking for patterns is almost visceral for us. We can’t help but looking for patterns. This almost instinctive human urge can also be misleading and suggesting patterns to us where there are in fact none.\nIn data exploration we can make use of the power of visualization by plotting data and looking at them graphically. The modern computer has made this form of displaying data particularly easy and powerful and visualizing data in data exploration is another core skill you are going to learn in this course.\nSo let us visualize the world poverty data. In our plot we draw the year on the x-axes and the share of people living in extreme poverty on the y-axes. This will give us a point for each year. To facilitate the spotting of a trend, we connect the annual observations by a line.\n\n\nCode\nlibrary(ggplot2)\n\np <- ggplot(plot_data, aes(x = Year, y = Share)) + \n     geom_line() +\n     geom_point() +\n     xlab(\"\")\nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world.png\")\np\n\n\n\n\n\nFigure 1.1: Share of world population living in extreme poverty from 2000 - 2017\n\n\n\n\nVisualizing trends in world extreme poverty as an example of data exploration. In this case the data pattern reveals a stunning fact. Over almost two decades we can see a sharp fall in the share of extremely poor people when looked at from a global perspective.\nOf course when we drill down to the level of individual countries this trend will not look the same everywhere and there might be countries where the share has actually increased. But overall we have seen a breathtaking steady decline. This is good news.66 When you have access to the internet you can have a closer look at these data at the very interesting website “our world in data” maintained by a consortium of Oxford University and University College London. See https://ourworldindata.org/extreme-poverty. The website has many interesting visualizations and options to select individual countries, country aggregates and make other selections of the data.\nBut does this trend mean that extreme poverty must disappear some years down the road? No, because nobody can tell whether the trend of the last two decades will go on also in the future.\nStatistics can help us to think more systematically about patterns and in making systematic guesses how a pattern might continue in the future. This is another core skill you will learn in this course: Making predictions, which means using available data and information to make informed guesses about data and information we do not have.\nLet us go back to the share of people in the world living in extreme poverty. As reported in our data the last actual observation for the global share in extreme poverty from the world bank is from 2017. There are more recent data for some regions but the global data since then are by now forecasts based on statistical techniques. The basic ideas of these techniques and how to apply them to data is a core skill you will learn in this course.\nNow what does the World bank predict for the share of extreme poverty in the world? Let us look at the data again graphically to visualize the prediction.\n\n\nCode\nobsdat <- data.frame(Year = plot_data$Year, Scenario = rep(\"Observed\", 18), Share = plot_data$Share)\n\nadd_dat <- data.frame(Year = 2018, Scenario = \"Observed\", Share = 8.6)\n\npred_dat_precovid <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Pre-Covid-19\", 4), Share = c(8.6, 8.4, 7.9, 7.5))\n\npred_dat_covidbase <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Covid-19-Baseline\", 4), Share = c(8.6, 8.4, 9.1, 8.9))\n\npred_dat_coviddown <- data.frame(Year = c(2018, 2019, 2020, 2021), Scenario = rep(\"Covid-19-Downside\", 4), Share = c(8.6, 8.4, 9.4, 9.4))\n\ndat <- rbind(obsdat, add_dat, pred_dat_precovid, pred_dat_covidbase, pred_dat_coviddown)\n\np <- ggplot(dat, aes(x = Year, y = Share, group = Scenario, color = Scenario)) + \n  geom_line(alpha = 0.5)+\n  geom_point()+\n  scale_color_manual(values=c('green', 'red', 'black', 'blue'))\nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world-forecast.png\")\np\n\n\n\n\n\nFigure 1.2: Share of world population living in extreme poverty from 2000 - 2017 with predictions until 2021\n\n\n\n\nObserve that the line showing the share of extreme poverty in the world takes different distinct future paths as we make predictions. What does this mean?\nAt the root of the predictions is an abstract model7, how the share of poverty changes over time. If the underlying data would correspond to a world before Covid the falling trend in poverty would just continue to fall, as it has done continuously from 2000 onward. In the graph you can see this scenario if you follow the black and the blue line. But taking the pandemic and the consequences into account the prediction of the World Bank is that extreme poverty after a almost two decades downward trend will rise again. This you can see by following the green and the red line. How much, this rise actually will be in the end depends on data we can not yet know.7 If you have difficulties now to imagine what this means, don’t worry. We will learn in detail what a model is and how it can be interpret. In practice a model is usually an equation which provides a low-dimensional summary of a dataset. This summary is then used to make predictions.\nWhen we make informed guesses based on observed data on information we do not yet have there is uncertainty involved. Using the theory of probability in combination with statistics we can quantify this uncertainty. Quantifying the uncertainty attached to predictions is the third basic skill you will learn in this course.\n\n\n1.1.4 Does taking your time in college pay off?\nA newspaper in Germany reported that the more semesters needed to complete an academic program at the university the greater the starting salary in the first year of a job. The report was based on a study that used a random sample8 of 24 people who had recently completed an academic program.8 We will later in the course learn in detail what a random sample is. For the moment imagine that there is a mechanism which allows to select these 24 students at random from the large population of all university students in Germany. When a sample is random, every member in the sample has the same probability of beeing chosen from the population.\nInformation was collected on the number of semesters each person in the sample needed to complete the program and the starting salary, in thousand Euros, at the beginning of the job.\nThe data are shown in the following plot\n\n\nCode\ndat <- read.csv(\"data/college_years_salaries/coll_sal.csv\")\n\nlibrary(ggplot2)\n\np <- ggplot(dat, aes(x=Time, y=Salary)) +\n     geom_point()\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm and the starting salary in the first year in the job.\n\n\n\n\nWhat you see in this picture is a so called scatter-plot. It takes the data and plots all pairs of time in semesters needed to complete the academic university program and the starting salary in the job, where the first value is shown on the x-axis and the second on the y-axes. The points you draw like this are “scattered” all over the place, but it seems that “by and large” there is also some trend - shown as a blue line - like this:\n\n\nCode\np <- ggplot(dat, aes(x=Time, y=Salary)) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = FALSE)\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary_with_trend.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm and the starting salary in the first year in the job with a linear trend fitted to data.\n\n\n\n\nLooking at the data, does this plot support the claim of the Newspaper?\nApparently the journalist writing the article saw a pattern, shown here as the blue line, which suggests that on average the salaries are really increasing with the semesters spent at the university. But is this pattern plausible? What do you think?\nAn independent researcher, who doubted the result, received the data from the newspaper and did a new analysis by separating the data into three groups based on the major of each person.\n\n\nCode\np <- ggplot(dat, aes(x=Time, y=Salary, group = Major, color = Major)) +\n     geom_point()\nggsave(plot=p, filename=\"figures/Years_in_college_versus_starting_salary_by_major.png\")\np\n\n\n\n\n\nRelation between the semesters needed by a random sample of 24 German students to complete an academic university programm by major and the starting salary in the first year in the job.\n\n\n\n\nNow, looking at this plot, describe the relation for students with a major in business. How could the newspaper report be modified to describe the data?\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nThis example works particularly well for involving students. I am not quiet sure how to build this interactive element in the online unit. maybe you have an idea.\n\n\n\nYou see in this example that looking for patterns is not as easy as it seems. Again we see that there are no automatism. You will need the skills you learn in this course to gain competence in distinguishing actual patterns from spurious ones.\nWhy did I go through all these example with you in the beginning, pinning down the share of renewable energy in the electricity production in Kenya, estimating the number of trees on the planet, long term trends in world extreme poverty, and the relation between study time and beginning salaries for students in Germany? All these examples illustrate some special skills you will learn in this course. Hopefully it also convinced most of you that statistics and working with data is an exciting field and a way to engage with real world issues.\nSo these are the three core skills you will learn in this course:"
  },
  {
    "objectID": "introduction.html#the-three-basic-skills-you-will-learn-in-this-course",
    "href": "introduction.html#the-three-basic-skills-you-will-learn-in-this-course",
    "title": "1  Introduction",
    "section": "1.2 The three basic skills you will learn in this course",
    "text": "1.2 The three basic skills you will learn in this course\n\n\n\n\n\n\nThe three basic skills you will learn in this course\n\n\n\nAfter successfully completing this course students you will have learned three core skills:\n\nData exploration or finding patterns in data and information through visualisation and computation.\nMaking predictions, which means using available data and information to make informed guesses about data and information we do not have.\nTo quantify the uncertainty we have to attach to our predictions.\n\n\n\nThe course is split into 8 units in total. Units 1, 2 and 3 will be mostly be concerned with data exploration and with making comparisons based on data. You will also learn step by step how you can use the computer for data exploration and visualization. We assume no prior knowledge and start from scratch. We also do not strive for completeness. The idea is that you acquire the practically most important skills and get maturity to drill deeper for yourself after the course either in your further studies or on the job.\nUnit 4 and unit 5 will be predominantly be concerned with models and using models for prediction. Here you will learn how to spot trends in data, how you can discern actual information in data, so called signals, from random variation, called noise.\nUnits 6, 7 and 8 will focus on how to quantify uncertainty related to prediction and inference from data. Here is the place where probability theory combines with statistics to provide analytically and practically powerful tools which ground data analysis in a firm scientific foundation. This is also the most challenging part of the course and it will require lots of practice and participation from you to acquire this important skill.\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nIn your template you suggest that in the introduction I give a minute overview of the details of topics learned in the course. I would prefer to abstain from this, because it contains lot of terms the students will learn step by step and it is probably boring at this stage. I would rather prefer the big picture approach outlined here with the three basic skills as the guiding posts. If listing the details is a must, it would look roughly like this:\n\n\n\n\n\n\nUnit 1: Overview; Categorical Data and Proportions\n\n\n\nIn unit 1 we will give an overview of the course and we begin with the analysis and understanding of binary variables, variables that can be imagined as simple yes or no questions and how they can be summarized as proportions or percentages. You will learn about how the idea of expected frequency will promote the understanding of the meaning of these shares and how this provides a basic understanding of the importance of these numbers.\n\n\n\n\n\n\n\n\nUnit 2: Summarizing and Communication lots of data; From limited data to populations\n\n\n\nIn unit 2 we learn how to deal with lots of data, the typical situation we will face when we do statistics. When there are lots of data we need instruments and tools to summarize them and to get an overview. This overview is usually also very important for communicating the data and the information they might contain. We will also learn how we can use statistics to learn properties about large populations by only making limited observations on some appropriately chosen subset of individuals from this population. The gold standard in making such inferences possible is the concept of a so called random sample. But at each step of the sampling procedure bias can crop up and invalidate our results leading to wrong conclusions. The circumstances under which inference from samples to populations can be made and how we can make sure to minimize bias we need a firm understanding of the opportunities and limits of this important technique.\n\n\n\n\n\n\n\n\nUnit 3: What causes what?\n\n\n\nIn unit 3 we discuss when data analysis allows us to say something about what causes what. We learn about the important concept of randomized trials. We will also learn what an observational study is and how it differs from a randomized trial. This is an important concept we will learn through the discussion of real world examples.\n\n\n\n\n\n\n\n\nUnit 4: Modelling relationships using regression and algorithmic predictions\n\n\n\nIn unit 4 we will learn a key concept needed to make predictions. The technical term for this concept in statistics is regression. It is a simple mathematical model describing how a set of explanatory variables varies systematically with a response variable. We will learn to construct such models and to interpret them correctly. We will also cover related techniques which have become very important recently and entered the statistical toolbox from the field of computer science. These methods are known under the notion of algorithmic prediction or machine learning.\n\n\n\n\n\n\n\n\nUnit 5: How sure can we be about what is going on: Estimates and Intervals.\n\n\n\nIn unit 5 we encounter the first time tools for quantifying uncertainty. We learn how to determine and use uncertainty intervals by using a technique which is called the bootstrap. Being able to determine such intervals is extremely important in communicating statistics and for supporting a systematic and sound answer to the question: How sure can we be about an estimate.\n\n\n\n\n\n\n\n\nUnit 6: Probability: Quantifying uncertainty and variability\n\n\n\nIn unit 6 we deepen the knowledge how to quantify uncertainty by introducing basic ideas of probability theory. Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable.\n\n\n\n\n\n\n\n\nUnit 7: Putting Probability and Statistics together\n\n\n\nIn unit 7 we put statistics and probability theory together. This allows us to both simplify ideas and techniques how to quantify uncertainty. The combination of the two field makes the tools for quantifying uncertainty at the same time more powerful. Combining statistics and probability theory is at the heart of statistics as a science. It makes all the ideas developed in unit 1 to 6 very versatile and powerful.\n\n\n\n\n\n\n\n\nUnit 8: Answering questions and claiming discoveries\n\n\n\nIn unit 8 we learn how to leverage the knowledge of this course to answer questions and to claim discoveries. You will learn how statistics is used in the sciences and how it supports to develop our knowledge of the world. It pulls many ideas of the whole course together and when you have mastered this unit you have mastered all the basic skills we want to develop in this course, data exploration, prediction and quantifying uncertainty."
  },
  {
    "objectID": "introduction.html#on-the-use-of-the-computer",
    "href": "introduction.html#on-the-use-of-the-computer",
    "title": "1  Introduction",
    "section": "1.3 On the use of the computer",
    "text": "1.3 On the use of the computer\nOur approach to teach you basic ideas of statistics and data analysis will be very much problem and activity oriented. The application of specific statistical techniques will be only one component in a whole package of activities you will need to engage in when you work with data in real world applications. Preparing data appropriately for analysis as well as communicating the conclusions for your analysis will be important elements of the whole process of statistical analysis. Today these activities involve using a computer.\nIn this course you will also learn how to use the computer. It will play an important role for developing your skills. We will make no assumptions of prior knowledge of computers and programming and will introduce the use of the computer step by step.\nUsing a computer requires a language in which we can tell the computer what to do and which the computer can understand. The language of our choice for this course is called R. It is one of the most widely used and most powerful languages for data analysis and statistics. We will introduce you to the language and its use step by step as we go along and in parallel with the statistical concepts we develop."
  },
  {
    "objectID": "introduction.html#activities-in-the-study-center",
    "href": "introduction.html#activities-in-the-study-center",
    "title": "1  Introduction",
    "section": "1.4 Activities in the study center",
    "text": "1.4 Activities in the study center\n\n1.4.1 Visualizing the share of extremely poor people for different countries\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nHere we assume that students have a running R and R-Studio or R with Jupyter Notebook or Jupyter Lab installation on their laptops at the study center. We would give the code in a notebook with the code chunk shown in the source file, to play with.\nWhile we need not pin down all the details of the computational infrastructure yet, we need a discussion how to integrate the computer instructions into the course and the material I have to prepare for that. I would very much prefer a minimalist solution with R and some kind of notebook but not more.\n\n\n\nLets go back to figure Figure 1.1 for a moment. This figure has been created by the use of the computer.\nIn the following box you see the computer code that has read the data from a file and then plotted the share for the world and for a particular country, say China. Don’t worry if you do not (yet) understand the details of the code. Think of it as a language that tells the computer what to do with the data. You can edit the code and delete China and insert another country instead. If you click the green arrow at the upper right corner of the box the computer will run the code again and generate a new graphic.\n\n\n\n\n\n\nComment for Seitwerk\n\n\n\n\n\nThe details of this will of course depend on the kind of notebook we use. We can use in principle three options. Option 1: Have an installation of Rstudio (https://www.rstudio.com/). RStudio is the most popular (and free) IDE for development of R code. In R Studio we can open quarto notebooks (qdm) and run code interactively there. Pro: Works seamlessly with my notes and enhances production efficiency. Con: You need to explain students the IDE in addition to R, though no big deal it is an additional complication. Option 2: Jupyter Notebook with an R kernel (ipynp). Pro: If JWE decides to work more with Jupyter notebooks on a broader base, seamless integration for this strategy. Con: same as with R studio and just a tiny inch more awkward to produce for me. Option 3: Work in the R console only. Pro: No IDE or notebook. Con: perhaps a bit too difficult to use for students without Computer Science background.\n\n\n\nTry to play and experiment with the code in this way to see what happens. Soon you will know yourself how to make interesting and beautiful data visualizations yourself.\n\n\nCode\nlibrary(ggplot2)\n\ndemo_data <- read.csv(\"data/extreme_poverty/share-of-population-in-extreme-poverty.csv\")\n\nnames(demo_data) <- c(\"Country\", \"Code\", \"Year\", \"Share\")\n\npl_dat <- with(demo_data, demo_data[Country %in% c(\"World\", \"China\") & Year >= 2000, ])\n\np <- ggplot(pl_dat, aes(x = Year, y = Share, color = Country)) +\n     geom_point() +\n     geom_line() +\n     xlab(\"\")\nggsave(plot=p, filename=\"figures/fig-share-of-people-in-extreme-poverty-world-china.png\")\n\n\nSaving 7 x 5 in image\n\n\nCode\np\n\n\n\n\n\n\n\n1.4.2 Guessing ages\nThis is an exercise which needs the leadership of an instructor at the study center. It would be great to collect the data of the exercise in a readable file for later use in the course.\nWe have 10 photos of persons whose age is known to us but not to the students. We divide students into 10 groups A through J. Each group gets one of the photos. The students in each group are asked to estimate the age of the person in their photograph and write the guess into a form. Each group must come up with a single estimate.\nExplain that each group will be estimating the ages of all 10 photos and that groups are competing to get the lowest error. Each group passes its card to the next group (A to B, B to C, etc. J back to A) and estimates the age of the new photo. This is continued until each group has seen all photos.\nThe data are written in a table where the rows are groups and the columns are card numbers. We can discuss the expected accuracy of the guesses (within how many years do you think you can guess). Then start with card 1 and ask groups to give their guess, then reveal the true age and write it at the bottom margin of the first column.\nIntroduce the concept of error - guessed age minus actual age - and wrote the errors in place of the guessed ages. Then fill the whole table. Ask the students to compute the absolute average error.\nStudents get an idea about: uncertainty, empirical analysis and data display. There are many statistical ideas in this game and the data can be taken up throughout the course (variance, bias, experimental design, randomization, linear regression, two-way tables, statistical significance).\nTo illustrate the idea more precisely:\nFor each card your group is given, estimate the age of the person on the card and write your guess on the table below in the row corresponding to this numbered card. Later students are told the true ages and they can compute the error. The error is defined as estimated minus actual age.\n\nExample for group card, guessing ages\n\n\nCard\nEstimated\nActual\nError\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n5\n\n\n\n\n\n6\n\n\n\n\n\n7\n\n\n\n\n\n8\n\n\n\n\n\n9\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n1.4.3 Collect data from students.\nOne data collection exercise, which might be fun here is to collect the height and hand span from students in the class. Later when we teach regression we can compare to the data Pearson collected on university students over 100 years ago.\n\n\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books."
  },
  {
    "objectID": "categorical_data_and_proportions.html",
    "href": "categorical_data_and_proportions.html",
    "title": "2  Categorical Data and Proportions",
    "section": "",
    "text": "The death of a child is a tragedy to its family. The numbers of such events across the globe add up to millions. Max Roser, in an article on the website “Our world in Data” (Roser 2019), reports that from 56 million people who died for example in the year 2017, 5.4 millions were children under the age of 5. This is a number of enormous scale that never makes the headlines.\nIn his article Roser (Roser 2019) puts this scale into perspective. He writes:\n“…The suffering and dying of children remains immense, yet these daily tragedies continue without receiving the attention this injustice deserves. A comparison of the tragedy of child deaths with those tragedies that do receive public attention puts it in perspective. A large jumbo jet can carry up to 620 passengers. The number of child deaths is that of 24 jumbo jet crashes, with only children on board, every single day. Single events – such as plane crashes – always make the headlines. Daily tragedies – even the worst ones like the deaths of thousands of children – never make the headlines.\nIn pre-industrial societies infant- and child-mortality were uniformly and cruelly high. They were a universal and frequent experience for families all over the world.\nMortality statistics distinguish between infant- and child-mortality. If a child dies within the first year of his life, this is classified and counted as an infant death and the statistics is referred to as infant-mortality statistics. If a child dies before its fifth year this is classified as child-mortality.\nMortality statistics are reported in different formats. Often the concept of a mortality-rate is used in such reports. These are usually mortality rates standardized to 1000 life births. To compute an infant mortality rate in a given year in a given geographic area one would, for instance, need to know how many babies were born alive in a given period in a given area and how many babies who were born alive died before their first birthday during the same period. The number of deaths is the divided by the number of births. This rate is then multiplied by 1000, so that the rate reflects the number of infant deaths per 1000 life births in a standardized manner. This is just a reporting convention. We also could multiply by 10.000 of 100.000 depending on the level of comparison that is required. Reporting mortality statistics as mortality rates has the advantage that it communicates the risk as an expected frequency. Instead of percentages of probabilities it conveys directly what it means, for example, for 1000 births.\nThe production and reporting of mortality statistics is a modern phenomenon. In many countries there are now statistical registry systems that record infant-deaths in legally regulated processes. But this is not the case all over the world. Poorer countries find it hard to have such processes in place. Still for these countries, where no registry data are available, the United Nations Health Organisation conduct high quality surveys that allow a fairly precise estimate of infant mortality rates. We will learn later in the course how you can draw conclusions from a survey to the whole population with a certain confidence. Today using either registry data of surveys we know a lot more about infant mortality based on a rich amount of data.\nHistorically these statistics can only be based on estimation and proxies of different and varied sources. In a scientific publication Anthony Volk and Jeremy Atkinson made an attempt to report mortality rates of infants and children in the past, collecting data from a wide range of geographic locations and cultures reaching far back in history (Antony and Jeremy 2013).\nThe authors find that in the entire sample about 27 % of children died in their first year of life and 46 % died before they reached adulthood. These numbers are surprisingly similar across regions and cultures: Every forth newborn child died in the first year of life, every second child during its early childhood. These high mortality rates are a major cause of the slow population growth of past centuries before the onset of the industrial revolution at around 1800.\nThe table shows infant mortality in the year 1860 as percent in decimal notation. So 0.124, for example, means 12.4 %. This is the percentage of infant deaths in Sweden in 1860. It is better than the historical numbers reported by (Antony and Jeremy 2013) but still fairly high. Others at that time bigger countries with a much more heterogeneous population like Germany and Austria were pretty much at where the rates had always been in the past. The countries for which we have data from 1860 are all in Europe. Some of the bigger countries by this time had begun to register and report infant deaths in a mortality statistics.\nBefore we deal with the information conveyed by these data, let us take the opportunity to discuss some terminology and technical terms that we will always use when we deal with tabular data like the ones shown in Table 2.1.\nIn this chapter we will use counts and proportions as the leading example to discuss the basic presentation of statistics and data. We will learn about alternative ways to display data. You will learn how design choices for the display of data help or prevent engagement and readability for your respective audience. You will also learn the first steps in R and how to use the computer to analyze and communicate data."
  },
  {
    "objectID": "categorical_data_and_proportions.html#communicating-counts-and-proportions",
    "href": "categorical_data_and_proportions.html#communicating-counts-and-proportions",
    "title": "2  Categorical Data and Proportions",
    "section": "2.1 Communicating Counts and Proportions",
    "text": "2.1 Communicating Counts and Proportions\nLet us follow the history of infant mortality around the globe. When we looked at the year 1860 rates of infant mortality were very high, not much better than it had been all the centuries before industrialization has set in in Europe.\nBut then about 100 years later we already see a significant reduction in the Western countries, with rates reduced still much further until now in the most affluent countries. Let us look at the country group for which we had data in 1860 already, again today in 2020, before we go to the global picture. We will use this example to discuss some important aspects of communication counts and proportions.\n\n\nCode\n# Select the country group used in the historical 1860 data\n\nrate_2020 <- infant_mortality[infant_mortality$Country %in% rate_1860$Country & infant_mortality$Year == 2020, c(\"Country\", \"Continent\", \"Mortality\")]\n\n# display the table and write text file of table\n\nlibrary(knitr)\nkable(rate_2020, digits = 4, row.names = FALSE)\n\n\n\n\nTable 2.2: Infant mortality in some European countries in 2020.\n\n\nCountry\nContinent\nMortality\n\n\n\n\nAustria\nEurope\n0.0030\n\n\nBelgium\nEurope\n0.0034\n\n\nDenmark\nEurope\n0.0031\n\n\nFrance\nEurope\n0.0034\n\n\nGermany\nEurope\n0.0031\n\n\nNorway\nEurope\n0.0018\n\n\nSpain\nEurope\n0.0027\n\n\nSweden\nEurope\n0.0021\n\n\n\n\n\n\nCode\nwrite.csv(rate_2020, file = \"tables/table_2_2_infant_mortality_2020.csv\", row.names = FALSE)\n\n\nThis is a spectacular improvement for this group of countries. By 2020 we have much more data, covering more regions and we will follow the global story later. But let us stick for the moment with this group of European Countries, which we followed over a time span of 160 year.\nWithin a bit more than a century the mortality rate has been reduced from 30 % to below 0.3 %. This amounts to a reduction by a factor of about 100. Isn’t this a stunning achievement?\n\n\n\n\n\n\nNow you try\n\n\n\nThis is a good opportunity to practice and refresh your skills in manipulating percentages. Compute and report the reduction in the share of infant mortality for each of the 8 countries between 1860 and 2020 based on the numbers reported in Table 2.1 and Table 2.2\n\n\nIn a recent book (Smil 2020), the Canadian researcher Vaclav Smil has pointed out that such low rates are impossible without the combination of a number of critical conditions, such as good healthcare in general, appropriate prenatal, perinatal and neonatal5 care, proper maternal and infant nutrition, adequate sanitary living conditions as well as access to social support for disadvantages families. All of these factors require relevant government and private spending and on infrastructures that can be universally used and accessed. Infant mortality is thus a very powerful indicator of quality of life in a country.5 prenatal is a word rooted in ancient Latin and means before birth. Perinatal means around the time of birth and neonatal means the first month after birth of a child.\nWhen data, such as counts and proportions are reported in a table we should make careful considerations how the data are precisely presented.\nFor instance in the table about infant mortality in some European countries in 1860, we could have reported the same information by presenting survival rates instead of mortality rates. Such a choice in reporting is generally known as framing.\nFraming can have effects on the impact of communication. Depending on how you frame the communication of data, the same information might affect and engage your audience differently. The same table with survival rates instead of mortality rates would the look like this.\n\n\nCode\nsrate_1860 <- rate_1860\nsrate_1860$Survival <- 1 - rate_1860$Mortality\n\n# table for 1860 survival rates\n\nsrate_1860 <- with(srate_1860, srate_1860[ , c(\"Country\", \"Continent\", \"Survival\")])\n\nlibrary(knitr)\nkable(srate_1860, digits = 4, row.names = FALSE)\n\n\n\n\nTable 2.3: Infant survival rates in some European countries in 1860\n\n\nCountry\nContinent\nSurvival\n\n\n\n\nAustria\nEurope\n0.763\n\n\nBelgium\nEurope\n0.861\n\n\nDenmark\nEurope\n0.864\n\n\nFrance\nEurope\n0.850\n\n\nGermany\nEurope\n0.740\n\n\nNorway\nEurope\n0.898\n\n\nSpain\nEurope\n0.826\n\n\nSweden\nEurope\n0.876\n\n\n\n\n\n\nCode\nwrite.csv(srate_1860, file = \"tables/table_2_3_infant_survival_1860.csv\", row.names = FALSE)\n\n\nTake for instance the data for Germany 1860. If we had reported a survival rate of 74 % it might sound to many better than if we had reported the equivalent information of a mortality rate of 26 %. So whenever you report counts or proportions be mindful of framing effects.\nRisk impression can often be made more clear if we report expected frequencies as well\nas percentages. In this way the risk can be imagined as an actual crowd of people. For example we could visualize the infant mortality rate in Germany by creating a picture of the mortality rate, the actual number of cases per 1000 life births. If there are relatively few cases, as in 2020 Germany, this arbitrary normalisation leads to an artificial number such as 3.1 deaths per 1000 life births. Of course this is an artifact of the normalisation because there is nothing as an event with 0.1 deaths. If we had normalized to 10000 this would amount to 31 in 10000 which sounds more like an actual count. So for the visualization we take an infant mortality rate as infant deaths per 1000 life births to be 3.\nSuch a visualization could then look for example like this:\n\n\nCode\nlibrary(waffle)\nwaffle(c(\"Death\" = 260, \"Survived\" = 740), rows = 20, colors = c(\"#FD6F6F\", \"#93FB98\"), use_glyph = \"child\", glyph_size = 4, equal = F)\n\n\n\n\n\nIn 1860 in Germany among 1000 newborns 260 infants died in the first year of their life.\n\n\n\n\nHere you visualize the numbers as differently colored crowds. There are 20 rows with 50 people symbols each. This multiplies to a crowd of 1000 people. It makes the magnitude of the mortality rate (as well as the survival rate) tangible. The share of infant death per 1000 life births in Germany in 1860 becomes now more tangible than reporting just a percentage in a table.\nWhen you plot the same graph for the data of 2020 the enormous improvement that took place within a time span of one and a half centuries in Germany become perhaps more obvious than just looking at the rates alone. The same kind of visualization for the 2020 data would then look like this\n\n\nCode\nlibrary(waffle)\nwaffle(c(\"Death\" = 3, \"Survived\" = 997), rows = 20, colors = c(\"#FD6F6F\", \"#93FB98\"), use_glyph = \"child\", glyph_size = 4, equal = F)\n\n\n\n\n\nIn 2020 in Germany among 1000 newborns 3 infants died in the first year of their life.\n\n\n\n\nFrom this perspective the improvement in infant mortality rates in Germany are truly spectacular.\nStill even at the low mortality rate this can mean a huge amount of individual tragedies. In 2020 Germany had 760.378 births. 0.31 % of this amounts to 2357 individual tragedies. Even if this rate could be brought down further, say to 0.28 % which sounds like a tiny improvement, it would mean 228 infant lives that could be saved.\n\n\n\n\n\n\nNow you try\n\n\n\nDavid Spiegelhalter (Spiegelhalter 2019) whose book we have encountered when we discussed the question of how many trees there are on the planet, describes an advertisement in the London Underground, saying that 99 % of young Londoners do not commit serious crimes. The add was presumably intended to reassure passengers that riding on the London Underground is very safe. Try to imagine what this statement would mean when you think about this information in terms of crowds of young Londoners, assuming that “young” means between 15 and 25. Try to exlpain how the same information presented differently may have a different impact. What kind of communication tools we have just heard about, have been applied here?\n\n\n\n\n\n\n\n\nSeitwerk: A remark on answer.\n\n\n\n\n\nTurn a positive frame into a negative one and then imagine actual crowds of people instead of percentages. This or an answer like this I would have in mind that studnets come up with if they reflect this example. I think an answer to such a reflection is best placed in the interactive part of the course or in the study center meet ups.\n\n\n\n\n\n\n\n\n\nBe mindful of framing\n\n\n\nA standard we should strive for when reporting data is providing impartial information. For this we should think carefully about our framing and should perhaps provide both positive and negative frames.\n\n\nBut even if we achieve this, we should consider other things, like the ordering of the rows. Let us discuss this aspect in the next session together with your first steps in R."
  },
  {
    "objectID": "categorical_data_and_proportions.html#a-first-acquaintance-with-r-visualizing-infant-mortality-rates",
    "href": "categorical_data_and_proportions.html#a-first-acquaintance-with-r-visualizing-infant-mortality-rates",
    "title": "2  Categorical Data and Proportions",
    "section": "2.2 A first acquaintance with R: Visualizing infant mortality rates",
    "text": "2.2 A first acquaintance with R: Visualizing infant mortality rates\nWhile a table might be the medium of choice for displaying data it is often more powerful to convey information visually. The example of how imagining actual crowds might give us sometimes a more tangible picture of the same information, was one illustration of this.\nToday when we visualize data as one important tool of data exploration, the tool of choice is a computer and an appropriate language that enables us to tell the computer what to do.\nFor us in this course this will be the R language and now we start to learn the first steps in this language.\n\n2.2.1 Starting an quitting R\nWe assume that the computer you work with has a recent version of R installed. R will work with the most common operating systems such as Windows, OSX or Linux.66 Installing R yourself is not complicated. To install R yourself you need a computer where you have the privileges to install software and you need an internet connection. You get the newest version of R at the website https://cran.r-project.org/. A screenshot of the website is here \nWe will use R in two ways: First from the R command line and then also by working with Jupyter Notebooks.77 Another very powerful and popular program to work with R and R code is RStudio, which you can find here https://www.rstudio.com/. RStudio is a so called IDE, an integrated development environment. If you have access to a computer with internet connection and the permissions to install software yourself, feel free to experiment also with this tool. The basic version is free of charge. Again you need a computer where you have the privileges to install software and you need an internet connection.\nThe notebooks, which we already encountered in an example in the introduction, will not only allow to write R code and send this code to R for execution. Notebooks also allows you to store commands, to comment them and make them available as files for later use. This is especially useful, once you work on longer and more complicated tasks. It is then essential that you can reproduce what you did in the past days, that you can write easily readable comments and that you can collaborate in a team, where you can share your work with others. This circle of collaborators includes your future and past self.\nFor a start we just work with the R command line or the R-console, as it is called, introducing the use of R via the notebooks a bit later.\nYou start R by either typing R into the terminal or by clicking the R icon on your computer. The R console shows a prompt, a symbol that looks like this >. When you see the prompt in the R console, R is ready to receive commands.\nTo end your R session write quit() at the prompt > of the console. Congratulations. Now that you can start and quit R we are ready to go.\n\n\n\n\n\n\nSeitwerk: A remark on a demo.\n\n\n\n\n\nThis would be perhaps best shown in a brief video. Of course when producing the video it is important to think about which operating system the students will use, and the R version that they will have available. If this is open, so the demo for windows, mac and linux. It does only take marginally more time and effort. Maybe this whole intro is perfect for a video demo.\n\n\n\n\n\n2.2.2 First steps\nHere is an easy command you can send to R. Just try to type 1 + 1 at the prompt\n\n1 + 1\n\n[1] 2\n\n\nSure enough, R gives you the result of the addition, which is 2. But what is [1]? This is just a row label. If there were more outputs to your command, then they would be labelled [2], [3], [4], and more. We will go into this aspect of R’s output display later in more detail.\nSo R can do all the usual computations. For instance if you knew that in Germany in 1860 there were 270 infant death per 1000 life birth you could compute the mortality rate in a decimal format by dividing the counts by 1000, because the convention is to report mortality as cases per 1000 life briths. The R command for division is /. So by typing\n\n270/1000\n\n[1] 0.27\n\n\nyou will get the mortality rate 0.27 or 27 %.\nYou can do all the usual arithmetic operations, like with a calculator in R. For instance subtraction\n\n3-2\n\n[1] 1\n\n\nOr multiplication\n\n10*10\n\n[1] 100\n\n\nYou can raise a number to the power of another, like\n\n3^2\n\n[1] 9\n\n\nand of course you can combine all of these operations:\n\n(1+3)^2 - 5*4 + 12^3 - (13/2)\n\n[1] 1717.5\n\n\nThe comma is represented in R as a dot .. So the above output reads 1717and one half or 0.5.\n\n\n\n\n\n\nNow you try\n\n\n\nUse R to transform the Mortality numbers reported in percent in Table 2.1 and Table 2.2 in mortality rates, i.e. the (approximate) number of infant-deaths per 1000 life births.\n\n\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance:\n5*\nIn this case R will show the expression 5* followed by a + instead of showing a new prompt. This means that the expression is incomplete. When R shows + after entering a command instead of the output and a new prompt, it means that it expects more input. If we complete the expression, the expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Errors are usually printed in red, and it instinctively might create a feeling of alarm. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n\n5%3\n\nError: <text>:1:2: unexpected input\n1: 5%3\n     ^\n\n\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, which is particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity.\nWith this knowledge we can already do a first example, by continuing our discussion on infant mortality data we started in this lecture. On the way we learn a few more things about R and the R language.\n\n\n2.2.3 Storing and reusing results\nWhen our operations become just a bit more complex than just typing in a simple arithmetic operation, it becomes useful if we can store answers and use these answers, which might be an intermediate result of some transformed data or something else. In R this problem is solved very easily. We assign the answer to a name we choose ourselves. Here is an example.\n\na <- 1+1\n\nThe symbol <- tells R pleas assign to the name a the result of the computation 1+1.88 The assignment operator is used so often that it is useful to type it using a keyboard shortcut instead of typing first <and then -. The same result can be achieved by pressing the ALTkey followed by the - key. \nNow see why we have said that the assignement has stored your result. Enter on your keyboard the name you have just chosen:\n\na\n\n[1] 2\n\n\nAnd you can use the stored value to do further computations with it, like\n\na^2\n\n[1] 4\n\n\nWhen you assign a new value to the old name a, the old value will be overwritten by the new value.\nWhat names should you use? You could uses actually anything but you have to follow a few rules. A name in R must for example not begin with a number, a dot . or an underscore _. So for example var_1, var.1, var1 and VAR1 and myVar1 are all allowed names but 1var, .var and _var1 are not\n\n\n2.2.4 A first data visualization\nWe showed the infant mortality rates of a group of European countries before in Table 2.1 and Table 2.2.\nHumans are very visual creatures. Thus using our visual system to explore data and absorb information in these data visualizations can be very powerful. To deploy their power, we must - however - follow some principles, which we will learn step by step over this course.\nAssume we would like to display the information in our tables in a so called bar chart. A bar chart would combine in a plot bars for each country in the table with the bar length proportional to the mortality rate. This will give us a visual impression how the countries differ in one view, which might be more informative as just looking at the numbers themselves in a table.\n\n2.2.4.1 Functions\nR is not just a calculator and data storage device. What makes R very powerful is that it comes equipped with many functions which we can use to do things with data, like for instance producing plots like a bar chart.\nFunctions in R have a name followed by parenthesis. In the very first step we typed for example quit() at the prompt. This is a function and by typing its name followed by the parenthesis, R knows that it has to close the program and shut down.\nFunctions can also have arguments, which we can assign certain values to. For example, R has a function which would round numbers. This function is called round(). It has also arguments. You need to tell R which numbers to round and the number of digits the rounding should consider.\n\nround( x = 2.4356789123456, digits = 2)\n\n[1] 2.44\n\n\nThe first argument in the function round is x. We can give x a value, which we assign by =. The second argument is called digits and we assign to it the value 2. The output is then, not very surprisingly, 2.44.\nNote that R is programmed such that we could also have typed:\n\nround(2.4356789123456, 2)\n\n[1] 2.44\n\n\nR would have known automatically that the first value is assigned to xand the second to digits.\nWe will encounter a lot of R functions during this course. We will also learn how to access R documentation to know for so many different functions, what is their name, which arguments they accept as input and how we can use them.\n\n\n2.2.4.2 Visualizing the infant mortality data\nR has a built in function for plotting bar charts, which is called barplot(). Let is make use of this function to show the infant mortality rates of 1860 as a bar chart. The arguments taken by R are the data. Then we can add additional arguments which determine details of the plot display and appearance.\nLet us first store the data in an object with the name mrfor mortality rates of 1860.\n\nmr_1860 <- c(0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)\n\nHere we see another important function of R which we will need all of the time, the c()function. This function concatenates values in a vector of values. So the output of the operation will be a vector \\((0.237, 0.139, 0.136, 0.150, 0.260, 0.102, 0.174, 0.124)\\) with the name mr_1860. Thus when we type\n\nmr_1860\n\n[1] 0.237 0.139 0.136 0.150 0.260 0.102 0.174 0.124\n\n\nR will print the whole vector as one object. This is why the counting label is [1] and not [8]. The c() function concatenates the number so a single object, a vector containing all eight numbers.99 In case you are worried about the technical term of a vector, don’t worry. We use this term here losely and not in a rigorous mathematical sense. You can think for the moment of a vector in R as a single object that can hold several data at once, like numbers, or characters. I avoided the term list in the text to avoid confusion with the list data structure which is a special data structure in R which we learn about in the course later.\nNow lets see what happens when we give mr_1860 as an argument to barplot.\n\nbarplot(mr_1860)\n\n\n\n\nWe see on the y axis the infant mortality rates from 0 to 0.25 and on the x axis a bar for each country with a length proportional to the infant mortality rate in this country.\nBut here it is difficult to connect the bars to the countries. So let us store the country names in another vector and call them ctr\n\nctr <- c(\"Austria\", \"Belgium\", \"Denmark\", \"France\", \n         \"Germany\", \"Norway\", \"Spain\", \"Sweden\")\n\nNote that the names of the countries had to be written between quotation marks \" \". This is the way to tell R that the sequence of letters are characters. Characters are a specific data type representing text. Now we have a vector of words, the country names. We can give the country names as an argument to barplot()like this:\n\nbarplot(mr_1860, names.arg = ctr)\n\n\n\n\nNot too bad. But some country names are missing. It seems that the width of the bars is not wide enough that R is able to print all names.\nIt would be more convenient to flip the chart around and interchange the x and the y axes here. This can be done by another argument to barplot(). This argument is called horiz and it assumes a logical value. A logical is another R data type which allows us to express whether something is true or false. Logical true and false values are expressed as TRUE and FALSE in R.\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE)\n\n\n\n\nThis does not yet help much, because this flip of coordinates can only support a better display of the data if the country names are also printed horizontally.\nYou might guess it already: This can be controlled by another argument which is called in the case of this function las. If las gets value 1 we get what we want.\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE, las = 1)\n\n\n\n\nNow we have visualized the information we had displayed in a table before. It is not yet perfect because the names of countries with longer names are cut off a bit. This could be fixed by additional function arguments, but let us not go too much in the details of the barplot()function at this stage. We will learn a lot about powerful visualization techniques in R as we go along.\n\n\n\n\n\n\nNow you try\n\n\n\nUse R to redo the barplot visualization we just did for the 1860 data for the 2020 data.\n\n\nBefore we close this first encounter with R and data visualization, let me point out an important aspect of bar charts. The visual impression is powerful and truthful, if we choose the origin of the bars carefully. It is usually the best idea to start the bars at zero. So we see clearly the relative lengths and the magnitude of differences in the context of the entire dataset.\nChanging the origin with not enough care can visually exaggerate the differences between countries. This is a manipulative visualization, which should be avoided but which is encountered often. So be mindful about the choice of origin in a bar chart.\nLet me show you what I mean by telling R, for example, to start the plot of the data for the 1880 data at \\(0.08\\) instead as of \\(0\\).\n\nbarplot(mr_1860, names.arg = ctr, horiz = TRUE, las = 1, \n        xlim = c(0.08, 0.275), xpd = F)\n\n\n\n\nDo you see that now the differences appear bigger? The choice of origin can have a big influence on the appearance of differences between the length of the bars. Always be mindful of this effect and reflect what happens if for some reason you have to choose a different origin for the bar chart than zero. Alberto Cairo, who is the author of an influential book on data visualization (Cairo 2016) recommends to always choose a “…logical and meaningful baseline”.\n\n\n\n\n\n\nBe mindful in choosing a logical and meaningful origin for barcharts\n\n\n\nWhen you compare porportions visually with a bar chart always think of choosing a logical and meaningful origin. In most cases this will be 0. If 0 is not possible, think about what would be a choice that gives a truthfull and not exaggerated display of differences.\n\n\nBefore we close this digression into visualization, let me briefly discuss another aspect of data presentation which you need to consider. The ordering of the rows in the data table, or in this case, the order of the bars in the barchart has to be carefully considered.\nIf you look at the bar chart we have produced, you see that the bars have an order corresponding to the alphabetical order of the countries, starting with A for Austria and ending with S for Sweden.\nNow consider we had ordered the data according to mortality rate like this:\n\n\nCode\nbarplot(rate_1860[order(-rate_1860$Mortality) , ]$Mortality, \n        names.arg = rate_1860[order(-rate_1860$Mortality) , ]$Country, horiz = TRUE, las = 1)\n\n\n\n\n\nNow the bar chart could suggests that the infant mortality rate is an important and meaningful way of comparing this particular group of countries.\nSuch ranking comparisons are very popular in the media but they can be misleading. They can be misleading because the differences could be there just by chance.\nThere could be also systematic differences between countries affecting infant mortality rates. For example, countries that are small with populations under 10 Million and that have very homogeneous populations and low birth rates tend to show lower infant mortality rates just because of these demographic features. So an ordering like the one presented in the graph might suggest a ranking that is in fact spurious and is not really substantial.\n\n\n\n\n\n\nBe mindful about order in displaying the data\n\n\n\nWhen you display proportions in a table or a bar chart be mindful of the ordering of data and avoid spurious rankings. Choose a particular ordering only if there is a meaningful and logical reason to do so."
  },
  {
    "objectID": "categorical_data_and_proportions.html#categorical-variables-causes-of-infant-mortality",
    "href": "categorical_data_and_proportions.html#categorical-variables-causes-of-infant-mortality",
    "title": "2  Categorical Data and Proportions",
    "section": "2.3 Categorical variables: Causes of infant mortality",
    "text": "2.3 Categorical variables: Causes of infant mortality\nWe have discussed binary data. These are data that can take two values, like death or alive, in the examples of infant mortality data we have studied so far. A generalization of binary variables are called categorical variables in statistics. Categorical variables are measures that can take two or more values, which can be either unordered, such as eye color, countries or study center locations at which JWL courses take place. They can also be ordered, like positions in a hierarchy.\nAn example of categorical data arises if we study the issue of infant mortality further and ask for the causes. Why do infants die in the first year of their life?\nHere are the causes that have been registered by the Global Burden of Disease Study in 2019 by the Institute for Health Metrics and Evaluation as well as the share of each cause in the overall cases.1010 See https://www.healthdata.org/gbd/2019\n\n\nCode\n# read data from JWL package\n\nlibrary(JWL)\ncauses <- infant_mortality_causes\n# order by share\ncauses_ordered <- causes[order(-causes$Share) , ]\n\ndat <- causes_ordered[, c(\"Entity\", \"Share\")]\nnames(dat) <- c(\"Cause\", \"Share\")\n\nlibrary(knitr)\nkable(dat, digits = 3, row.names = FALSE)\n\n\n\n\n\nCause\nShare\n\n\n\n\nPreterm birth\n0.211\n\n\nEncephalopathy due to birth asphyxia and trauma\n0.180\n\n\nLower respiratory infections\n0.161\n\n\nBirth defects\n0.128\n\n\nDiarrheal diseases\n0.092\n\n\nHeart anomalies\n0.048\n\n\nMalaria\n0.044\n\n\nSyphilis\n0.026\n\n\nMeningitis\n0.020\n\n\nWhooping cough\n0.016\n\n\nNutritional deficiencies\n0.015\n\n\nDigestive anomalies\n0.013\n\n\nSudden infant death syndrome\n0.009\n\n\nTuberculosis\n0.008\n\n\nMeasles\n0.006\n\n\nHIV/AIDS\n0.006\n\n\nDigestive diseases\n0.005\n\n\nTetanus\n0.005\n\n\nEncephalitis\n0.003\n\n\nAcute hepatitis\n0.002\n\n\nDiabetes and kidney diseases\n0.002\n\n\n\n\n\nCode\nwrite.csv(dat, file = \"tables/table_2_4_infant_mortality_causes.csv\", row.names = FALSE)\n\n\n\n\n\n\n\n\nNow you try\n\n\n\nUse R to visualize this table in a barchart. Don’t worry if some of the causes are cut off at the left of the graph. We will learn during the course how to better control the appearance of a visualization and control for details like this.\n\n\n\n\n\n\n\n\nThis is how an answer can look like.\n\n\n\n\n\nSince students have not yet learned subsetting, they need to use the table and retype the values and the causes manually. This is intended because it will familiarize them with the c() function, the difference between numerical and character data type and is an excellent opportunity to review what we did on simple R bar-charts. In the example cause below I use - of course - the convenience of subsetting\n\n\nCode\nbarplot(dat$Share, \n        names.arg = dat$Cause, horiz = TRUE, las = 1, cex.names = 0.3)\n\n\n\n\n\n\n\n\nIn the news but also in many publications you will often find proportions and how they add up to a total represented in so called pie charts. A pie chart for the causes of infant mortality visualizing our data will look like this\n\n\nCode\npie(dat$Share, labels = dat$Cause, radius = 1, cex = 0.5)\n\n\n\n\n\nNow compare this to a display of the same information using the bar chart. If you have done the exercise before correctly, it will look like this\n\n\nCode\nbarplot(dat$Share, \n        names.arg = dat$Cause, horiz = TRUE, las = 1, cex.names = 0.5)\n\n\n\n\n\nApart from some imperfections of displaying the labels in a easily readable way, why does the bar chart work much better for comparing the proportions of causes than the pie chart?\nThe reason is that the bar chart is supported better by by our visual perception capacity. In the case of the bar chart we have to compare lengths, which our visual system can do well. For decoding the pie we have to compare areas of slices or angles formed by the slices at the circle center. But our visual system does not work well for such tasks. It works even worse, if the display is a three dimensioal pie chart, as offered by various apps. In this case the pie would look like this:\n\n\nCode\nlibrary(plotrix)\npie3D(dat$Share, labels = dat$Cause, radius = 1, labelcex = 0.5)\n\n\n\n\n\nNow it is even worse, because our visual system now has to additionally decode perspective on top of area, something it is naturally also not good at.\nThere are ways to visually display data that work because they are naturally supported by our ability of visual perception and cognition, while others are not.\n\n\n\n\n\n\nDon’t use pie charts to visually compare proportions\n\n\n\nWhen you display proportions choose bar charts and avoid pie charts. Bar charts are naturally supported by the ability of our visual system to compare lengths, while pie charts require the comparison of areas and angles, a task our visual system is not so good at."
  },
  {
    "objectID": "categorical_data_and_proportions.html#compairing-pairs-of-proportions",
    "href": "categorical_data_and_proportions.html#compairing-pairs-of-proportions",
    "title": "2  Categorical Data and Proportions",
    "section": "2.4 Compairing pairs of proportions",
    "text": "2.4 Compairing pairs of proportions\nOften proportions or percentages are used to communicate risks by comparing pairs of proportions.\nWhen pairs of proportions are compared, we need to understand how such comparisons can be made in a meaningful way. Especially in the media it is popular to often run spectacular headlines of how specific behaviors affect your likelihood of developing a disease.\nMuch of the spectacle in the headlines is due to the exclusive reporting of relative risks. It is important to understand that such statements need to be put into context by also reporting baseline of absolute risk.\nLet me illustrate the issue using the example of consuming processed meat and the risk of developing bowel cancer.\nFrom epidemiological research it is known that the chance of a person of developing bowel cancer is based on factors such as meat consumption, the level of physical activity, the body weight or income.\nThe relative risk is the risk of developing bowel cancer in a group of people compared to another group of people with different behaviors, different physical conditions of different environments.\nFor instance when we compare meat eaters versus vegetarians, a statement about relative risk would be that the consumption of processed meat increases the risk of developing bowel cancer by 18 %. This sounds spectacular but it does not tell the full story.\nTo see this go back to our previous example of imagining crowds of people instead of percentages.\n\n\nCode\nwaffle(c(2, 8), rows = 2, use_glyph = \"male\", \n       colors = c(\"#FD6F6F\", \"#93FB98\"), glyph_size = 16, \n       equal = F, legend_pos = \"\")\n\n\n\n\n\nHere we have an absolute risk of two out of 10 persons developing bowel cancer. If the relative risk would increase by 50 % this would mean in terms of absolute risk that now we have\n\n\nCode\nwaffle(c(2, 1, 7), rows = 2, use_glyph = \"male\", \n       colors = c(\"#FD6F6F\", \"#3A9ABD\", \"#93FB98\"), glyph_size = 16, equal = F, legend_pos = \"\", size = 0.2)\n\n\n\n\n\nThe risk increases now to three out of 10."
  },
  {
    "objectID": "categorical_data_and_proportions.html#now-you-try-5",
    "href": "categorical_data_and_proportions.html#now-you-try-5",
    "title": "2  Categorical Data and Proportions",
    "section": "2.5 Now you try",
    "text": "2.5 Now you try\nAssume the absolute risk is 4 out of 10 and the relative risk is 50 %. How many out of 10 are now at risk? :::\nThese examples illustrate that you need baseline or absolute risks as a vital information to understand what an 18 % increase in risk really means.\nGoing back to the example of processed meat consumption and bowel cancer risk, the estimated lifetime risk of developing bowel cancer is 5.6 %. Expressed as a relative frequency this is about 6 out of 100 or in a chart\n\n\nCode\nwaffle(c(6, 94), rows = 5, use_glyph = \"male\", \n       colors = c(\"#FD6F6F\", \"#93FB98\"), glyph_size = 8, \n       equal = F, legend_pos = \"\")\n\n\n\n\n\nThe estimated life time risk of developing bowel cancer if you eat 50 g of processed meat per day increases your relative risk by 18 %. At this heigthened risk level the new absolute risk is now 6.6 % or about 7 out of 100\n\n\nCode\nwaffle(c(6, 1, 93), rows = 5, use_glyph = \"male\", \n       colors = c(\"#FD6F6F\", \"#3A9ABD\", \"#93FB98\"), glyph_size = 8, \n       equal = F, legend_pos = \"\")\n\n\n\n\n\nIn the research literature proportions are often expressed by the odds ratio. This ratio expresses the chance of an event happening relative to the chance of an event not happening. In the example of processed meat and bowel cancer this would be \\(6/94\\), because 6 out of hundered people develop bowel cancer in their life time and 94 out of hundred do not. While odds are very common in the research literture they are not a very intuitive way to communicate the comparison of proportions. Spiegelhalter, from whom I took this example therefore recommends not to use odds ratios outside a scientific context, since it easily invites misunderstanding."
  },
  {
    "objectID": "categorical_data_and_proportions.html#activities-in-the-study-center",
    "href": "categorical_data_and_proportions.html#activities-in-the-study-center",
    "title": "2  Categorical Data and Proportions",
    "section": "2.6 Activities in the study center",
    "text": "2.6 Activities in the study center\n\n2.6.1 The story of infant mortality around the globe\nFor this activity we continue to look into the issue of infant mortality around the globe by working with a Jupyter notebook. To work on this assignment, please open the notebook xy and follow the instructions given there.\n\n\n2.6.2 Visualizing food waste\n\n\n2.6.3 The relative and absolute risk of flying compared to just beeing alive\n\n\n2.6.4 Exercises to do at home\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAntony, Volk, and Atkinson Jeremy. 2013. “Infant and Child Death in the Human Environment of Evolutionary Adaptation.” Evolution and Human Behavior 34: 182–92.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for Communication. New Riders.\n\n\nRoser, Max. 2019. “Child Mortality Is an Everyday Tragedy of Enormous Scale That Rarely Makes the Headlines.” https://ourworldindata.org/child-mortality-everyday-tragedy-no-headlines.\n\n\nSmil, Vaclav. 2020. Numbers Don’t Lie: 71 Things You Need to Know about the World. Penguin Books.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from Data. Pelican Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html",
    "href": "summarizing_and_communicating_lots_of_data.html",
    "title": "3  Summarizing and communicating lots of data",
    "section": "",
    "text": "Statistics usually involves lots of data and we need ways to communicate and summarize these data. This chapter introduces the most important concepts.\n\nThe empirical distribution of data points\nMeasures of location and spread.\nSkewed data distributions are common and some summary statistics are very sensitive to outlying values.\nSummaries always hide some detail.\nHow to summarize sets of numbers graphically (histograms and box plots)\nUseful transformations to reveal patterns\nLooking at pairs of numbers, scatter plots, time series as line graphs.\nThe primary aim in data exploration is to get an idea of the overall variation."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#outcome",
    "href": "summarizing_and_communicating_lots_of_data.html#outcome",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.2 Outcome",
    "text": "3.2 Outcome\nUnderstand these concepts and work through may examples showing how to apply these summary measures to data on the computer."
  },
  {
    "objectID": "from_limited_data_to_populations.html",
    "href": "from_limited_data_to_populations.html",
    "title": "4  From limited data to populations",
    "section": "",
    "text": "Inductive inference requires working from data to study sample and study population to target population.\nAt each stage bias can crop up. The best way to proceed from sample to study population is if you have drawn a random sample. Introduce the idea that a population can be thought of as a group of individuals but also as a probability distribution for a random observation drawn from that population Populations can be summarized using parameters that mirror the summary statistics of sample data. It often occurs that data do not arise as a sample from a literal population. We can always imagine data as drawn from a metaphorical population of events that could have occurred but didn’t."
  },
  {
    "objectID": "from_limited_data_to_populations.html#outcome",
    "href": "from_limited_data_to_populations.html#outcome",
    "title": "4  From limited data to populations",
    "section": "4.2 Outcome",
    "text": "4.2 Outcome\nMake the concept of a random sample and a probability distribution tangible by using the computer."
  },
  {
    "objectID": "what_causes_what.html",
    "href": "what_causes_what.html",
    "title": "5  What causes what?",
    "section": "",
    "text": "In this lecture we discuss what causation means in a statistical sense, why we need be careful to distinguish between causation and correlation.\n\nCausation in a statistical sense means that when we intervene, the chances of different outcomes vary systematically.\nCausation is difficult to establish statistically, but well designed randomized trials are the best available framework. 3.Principles that helped clinical trials to identify effects. 4, Observational data may have background factors influencing the apparent relationships between exposure and outcome which may be either observed confounders or lurking factors.\nStatistical methods do not suspend judgment which is always required for the confidence with which causation can be claimed."
  },
  {
    "objectID": "what_causes_what.html#outcome",
    "href": "what_causes_what.html#outcome",
    "title": "5  What causes what?",
    "section": "5.2 Outcome",
    "text": "5.2 Outcome\nThis is a conceptually difficult topic. It is, however, not technically difficult but it will need lots of examples. Fortunately there are many good (and bad) real world examples that I hope will stick with the students as reference examples after the course. Students should understand the idea of randomized trials and observational studies as well as the general ideas of comparison in statistical analysis."
  },
  {
    "objectID": "modelling_relationships_using_regression.html",
    "href": "modelling_relationships_using_regression.html",
    "title": "6  Modelling relationships using regression",
    "section": "",
    "text": "Regression models provide a mathematical representation between a set of explanatory variables and a response.\n\nThe coefficients in a regression represent how much we expect the response to change when the explanatory variable is observed to change.\nRegression to the mean\nRegression models can incorporate different types of response variable\nExplanatory variables and non-linear relationships\nBe cautious in interpreting models and don’t take them literally."
  },
  {
    "objectID": "modelling_relationships_using_regression.html#outcome",
    "href": "modelling_relationships_using_regression.html#outcome",
    "title": "6  Modelling relationships using regression",
    "section": "6.2 Outcome",
    "text": "6.2 Outcome\nThe students should understand the concept of regression and how it works and should be correctly interpreted. They should develop a good understanding that a method like regression does not provide an automatism for making predictions and will always need cautious interpretation. The students should learn some tools and example what cautious interpretation means and what is helpful in this respect."
  },
  {
    "objectID": "algorithmic_prediction.html",
    "href": "algorithmic_prediction.html",
    "title": "7  Algorithmic prediction",
    "section": "",
    "text": "Algorithms built from data can be used for classification and prediction in technological applications.\n\nImportance of guarding an algorithm against over fitting\nAlgorithms can be evaluated according to classification accuracy, their ability to discriminate between groups and their overall predictive accuracy\nComplex algorithms can lack transparency and it may be worth trading off some accuracy for comprehension.\nThere are many challenges in using algorithms and machine learning, be aware of them."
  },
  {
    "objectID": "algorithmic_prediction.html#outcome",
    "href": "algorithmic_prediction.html#outcome",
    "title": "7  Algorithmic prediction",
    "section": "7.2 Outcome",
    "text": "7.2 Outcome\nWith respect to algorithmic prediction the students should have seen what it is and see a not too complex example, for instance in classification. They should be able to see the close similarity between regression and machine learning methods and be able to understand the jargon. Both regression and machine learning use sometimes different notions for the same thing."
  },
  {
    "objectID": "how_sure_can_we_be.html",
    "href": "how_sure_can_we_be.html",
    "title": "8  How sure can we be about what is going on",
    "section": "",
    "text": "Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable."
  },
  {
    "objectID": "how_sure_can_we_be.html#outcome",
    "href": "how_sure_can_we_be.html#outcome",
    "title": "8  How sure can we be about what is going on",
    "section": "8.2 Outcome",
    "text": "8.2 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "9  Probability: Quantifying uncertainty and variablility",
    "section": "",
    "text": "11 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability_and_statistics.html",
    "href": "probability_and_statistics.html",
    "title": "10  Putting probability and statistics together",
    "section": "",
    "text": "This will be conceptually the most difficult part of the course. The main ideas that should be conveyed in this unit are\n\nUsing probability theory we can derive the sampling distribution of summary statistics from which formulae for confidence intervals can be derived.\nExplain what a 95 % confidence interval means\nThe central limit theorem and the normal distribution\nThe role of systematic error due to non random causes and the role of judgment\nExplain the idea that confidence intervals can be calculated even when we observe all the data which then represent uncertainty about the parameters of an underlying metaphorical population."
  },
  {
    "objectID": "probability_and_statistics.html#outcome",
    "href": "probability_and_statistics.html#outcome",
    "title": "10  Putting probability and statistics together",
    "section": "10.2 Outcome",
    "text": "10.2 Outcome\nThe students should gain a firm understanding of confidence intervals and how they help us in quantifying uncertainty of predictions we make based on our available data. They should see and understand how and why it is sometimes more convenient and parsimonious to have formulae for confidence intervals rather than quantifying the uncertainty from simulation. The intuitive understanding of the limit theorems and when they can be legitimately applied will be important here."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html",
    "href": "answering_questions_and_claiming_discoveries.html",
    "title": "11  Answering questions and claiming discoveries",
    "section": "",
    "text": "Formal statistical testing as a major empirical tool for answering questions and claiming discoveries.\n\nTests of null hypothesis as a major part of statistical practice\np-value as the measure of incompatibility between the observed data and the null hypothesis\nThe traditional p value thresholds.\nThe need to adjust thresholds with multiple tests\nCorrespondence between p-values and confidence intervals\nNeyman-Pearson theory (alternative hypothesis and type 1 and type 2 error).\nSequential testing\nThe misinterpretation of p-values."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html#outcomes",
    "href": "answering_questions_and_claiming_discoveries.html#outcomes",
    "title": "11  Answering questions and claiming discoveries",
    "section": "11.2 Outcomes",
    "text": "11.2 Outcomes\nStudents should learn the basic ideas of hypothesis testing and the terminology around it."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Antony, Volk, and Atkinson Jeremy. 2013. “Infant and Child Death\nin the Human Environment of Evolutionary Adaptation.”\nEvolution and Human Behavior 34: 182–92.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for\nCommunication. New Riders.\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nRoser, Max. 2019. “Child Mortality Is an Everyday Tragedy of\nEnormous Scale That Rarely Makes the Headlines.” https://ourworldindata.org/child-mortality-everyday-tragedy-no-headlines.\n\n\nSmil, Vaclav. 2020. Numbers Don’t Lie: 71 Things You Need to Know\nabout the World. Penguin Books.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from\nData. Pelican Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  }
]