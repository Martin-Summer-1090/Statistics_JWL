[
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html",
    "href": "summarizing_and_communicating_lots_of_data.html",
    "title": "3  Summarizing and communicating lots of data",
    "section": "",
    "text": "When we analyze data, we usually have to look at lots of them. An example might be income data gained from household surveys. Such a survey will contain a huge number of data points, in the order of magnitude of ten thousands of data. These data need to be summarized, to understand their main characteristics. In this unit you will learn the most important tools for summarizing and communicating lost of data. You are going to learn the principles how data summaries are constructed, what are the properties of these summaries and what needs to be carefully considered. Clearly, when we need to deal with really large data sets, and most modern data sets are too large to be handled manually, we will need the computer. We already did some first steps in R. In this unit we will build on these first steps but enlarge them in a way that will enable you to deal and manipulate large datasets on the computer."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#understanding-variation-in-a-single-variable",
    "href": "summarizing_and_communicating_lots_of_data.html#understanding-variation-in-a-single-variable",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.1 Understanding variation in a single variable",
    "text": "3.1 Understanding variation in a single variable\nTo summarize data, statisticians often use a graph which is called a histogram. In this section we will discuss all you have to know about histograms and how to use them. Let us start by an example, where we have about 100 data points, which is a lot but not that large that we can not handle them by hand.\nThe data we want to look at come from measurements of the annual flow of the river Nile at Aswan (formerly) Assuan in Egypt from 1871 to 1970. The units of these measurement in which the annual flow is recorder are 100 Millions of cubic meters, i.e. \\(10^8 m^3\\).\nThis is one of the data sets that is bundled with the R distribution and is available to all users of R. They are stored in an R object called Nile.11 When you type data() at the R console, you get a list of all datasets that are available with the current distribution of R.\nThis is how the data look like, when we print them to the R console\n\noptions(width = 70)\nprint(Nile)\n\nTime Series:\nStart = 1871 \nEnd = 1970 \nFrequency = 1 \n  [1] 1120 1160  963 1210 1160 1160  813 1230 1370 1140  995  935 1110\n [14]  994 1020  960 1180  799  958 1140 1100 1210 1150 1250 1260 1220\n [27] 1030 1100  774  840  874  694  940  833  701  916  692 1020 1050\n [40]  969  831  726  456  824  702 1120 1100  832  764  821  768  845\n [53]  864  862  698  845  744  796 1040  759  781  865  845  944  984\n [66]  897  822 1010  771  676  649  846  812  742  801 1040  860  874\n [79]  848  890  744  749  838 1050  918  986  797  923  975  815 1020\n [92]  906  901 1170  912  746  919  718  714  740\n\n\nWe start the construction of a histogram by choosing for the horizontal axes ranges of numerical values - in our case of the river flow data - which are called bins or classes. There is no fixed rule as to how to choose the size of these ranges. These ranges should neither be to fine, nor too coarse. While there are a list of mechanical rules, which you can for example find on Wikipedia2, it is usually best to use your domain knowledge and some experimentation to find out the bin size that works best for your data.2 See https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width\nFor this example, assume we had chosen a bin size of 1003. When you study the list, you will find that the lowest value is at 456 while the highest value is at 1370. This is already quite tedious to find out by eyeballing the numbers with the small number of values we have chosen for this example. It is impossible to do for really large data sets.3 Note that this will mean \\(100\\times 10^8 m^3\\) per year.\nNow lets make a distribution table like this:\n\n\n\nFlow-bin\nFrequency\n\n\n\n\n400 - 500\n1\n\n\n500 - 600\n0\n\n\n600 - 700\n5\n\n\n700 - 800\n20\n\n\n800 - 900\n25\n\n\n900 - 1000\n19\n\n\n1000 - 1100\n12\n\n\n1100 - 1200\n11\n\n\n1200 - 1300\n6\n\n\n1300 - 1400\n1\n\n\n\nIn the column Flow-bin we have recorded the bins in steps of 100 and in the right column, Frequency, we have recorded the count of values that are in this bin.\nWhen we make such a tabulation we have to agree on an endpoint convention. This is important, since when a flow value would for instance be measures as exactly 500, in which bin should it be counted: 400-500 or 500-600? You the constructor of the histogram has to take this decision. Let us agree on the convention that when a value falls exactly at the endpoint of the bin, we put it in the next bin. In practice you will usually do a histogram by computer. The code of the computer program has to specify an endpoint convention, so the computer knows what to do when a value coincides with an endpoint.\nOn the Frequency axes you put the frequency scale: Counts of values. Then for each bin, you plot a bar, which has the width of the bin and the height of the frequency.\nDo this for all the bins you have tabulated and you are ready.\nThe histogram provides a certain aggregation of the data because it sorts the 100 data points into 10 bins, in our example. While loosing some local information on individual data points the global information conveyed by the summary gives us a pretty good idea of the overall pattern of variation on the Nile river flow data.\nWe can see, for instance, that the most frequent flow is between 800 and 900 and that the variation is fairly symmetric around this bin. In the extremes this most frequent value can half or almost double, so there is quite some spread in the data.\n\n\n\nConstructing the river flow histogram\n\n\nIf we had just plotted all individual data points, we also got a picture, though you probably agree that it is not particularly useful.\n\n\nCode\nplot(as.numeric(Nile), xlab = \"Observation\", ylab = \"Annual Flow\", pch = 16)\n\n\n\n\n\nHistograms are such a common tool in statistics to explore the variation in one variable and the shape, how it is roughly distributed that every statistical software has functions to produce histograms. In R, the language we use in this course there is also such a function. The function name is called hist() and it takes the data as an argument. This is the second graphic function of R you encounter in this course after we played with the barplot()function in the last lecture.\nTo produce a histogram from the river flow data, we type at the console\n\nhist(Nile)\n\n\n\n\n\n\n\n\n\n\nNow you try\n\n\n\nLet us check your understanding of histograms by a little quiz now. The histogram below shows the distribution of the final score in a certain class.\n\nWhich block represents the people who scored between 60 and 80?\nTen percent scored between 20 and 40 about what percentage scored between 40 and 60?\nAbout what percentage scored over 60?\n\n\n\n\nFinal Score"
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#contents",
    "href": "summarizing_and_communicating_lots_of_data.html#contents",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.2 Contents",
    "text": "3.2 Contents\nStatistics usually involves lots of data and we need ways to communicate and summarize these data. This chapter introduces the most important concepts.\n\nThe empirical distribution of data points\nMeasures of location and spread.\nSkewed data distributions are common and some summary statistics are very sensitive to outlying values.\nSummaries always hide some detail.\nHow to summarize sets of numbers graphically (histograms and box plots)\nUseful transformations to reveal patterns\nLooking at pairs of numbers, scatter plots, time series as line graphs.\nThe primary aim in data exploration is to get an idea of the overall variation."
  },
  {
    "objectID": "summarizing_and_communicating_lots_of_data.html#next-steps-in-r",
    "href": "summarizing_and_communicating_lots_of_data.html#next-steps-in-r",
    "title": "3  Summarizing and communicating lots of data",
    "section": "3.3 Next steps in R:",
    "text": "3.3 Next steps in R:\n\nvectors and indices\nextracting subsets from vectors\ncreating vectors using c()\nsubsetting vectors wit logicals\ndata frames\nfactor class\nextracting data from data frames\ntapply"
  },
  {
    "objectID": "from_limited_data_to_populations.html",
    "href": "from_limited_data_to_populations.html",
    "title": "4  From limited data to populations",
    "section": "",
    "text": "Inductive inference requires working from data to study sample and study population to target population.\nAt each stage bias can crop up. The best way to proceed from sample to study population is if you have drawn a random sample. Introduce the idea that a population can be thought of as a group of individuals but also as a probability distribution for a random observation drawn from that population Populations can be summarized using parameters that mirror the summary statistics of sample data. It often occurs that data do not arise as a sample from a literal population. We can always imagine data as drawn from a metaphorical population of events that could have occurred but didn’t."
  },
  {
    "objectID": "from_limited_data_to_populations.html#outcome",
    "href": "from_limited_data_to_populations.html#outcome",
    "title": "4  From limited data to populations",
    "section": "4.2 Outcome",
    "text": "4.2 Outcome\nMake the concept of a random sample and a probability distribution tangible by using the computer."
  },
  {
    "objectID": "what_causes_what.html",
    "href": "what_causes_what.html",
    "title": "5  What causes what?",
    "section": "",
    "text": "In this lecture we discuss what causation means in a statistical sense, why we need be careful to distinguish between causation and correlation.\n\nCausation in a statistical sense means that when we intervene, the chances of different outcomes vary systematically.\nCausation is difficult to establish statistically, but well designed randomized trials are the best available framework. 3.Principles that helped clinical trials to identify effects. 4, Observational data may have background factors influencing the apparent relationships between exposure and outcome which may be either observed confounders or lurking factors.\nStatistical methods do not suspend judgment which is always required for the confidence with which causation can be claimed."
  },
  {
    "objectID": "what_causes_what.html#outcome",
    "href": "what_causes_what.html#outcome",
    "title": "5  What causes what?",
    "section": "5.2 Outcome",
    "text": "5.2 Outcome\nThis is a conceptually difficult topic. It is, however, not technically difficult but it will need lots of examples. Fortunately there are many good (and bad) real world examples that I hope will stick with the students as reference examples after the course. Students should understand the idea of randomized trials and observational studies as well as the general ideas of comparison in statistical analysis."
  },
  {
    "objectID": "modelling_relationships_using_regression.html",
    "href": "modelling_relationships_using_regression.html",
    "title": "6  Modelling relationships using regression",
    "section": "",
    "text": "Regression models provide a mathematical representation between a set of explanatory variables and a response.\n\nThe coefficients in a regression represent how much we expect the response to change when the explanatory variable is observed to change.\nRegression to the mean\nRegression models can incorporate different types of response variable\nExplanatory variables and non-linear relationships\nBe cautious in interpreting models and don’t take them literally."
  },
  {
    "objectID": "modelling_relationships_using_regression.html#outcome",
    "href": "modelling_relationships_using_regression.html#outcome",
    "title": "6  Modelling relationships using regression",
    "section": "6.2 Outcome",
    "text": "6.2 Outcome\nThe students should understand the concept of regression and how it works and should be correctly interpreted. They should develop a good understanding that a method like regression does not provide an automatism for making predictions and will always need cautious interpretation. The students should learn some tools and example what cautious interpretation means and what is helpful in this respect."
  },
  {
    "objectID": "algorithmic_prediction.html",
    "href": "algorithmic_prediction.html",
    "title": "7  Algorithmic prediction",
    "section": "",
    "text": "Algorithms built from data can be used for classification and prediction in technological applications.\n\nImportance of guarding an algorithm against over fitting\nAlgorithms can be evaluated according to classification accuracy, their ability to discriminate between groups and their overall predictive accuracy\nComplex algorithms can lack transparency and it may be worth trading off some accuracy for comprehension.\nThere are many challenges in using algorithms and machine learning, be aware of them."
  },
  {
    "objectID": "algorithmic_prediction.html#outcome",
    "href": "algorithmic_prediction.html#outcome",
    "title": "7  Algorithmic prediction",
    "section": "7.2 Outcome",
    "text": "7.2 Outcome\nWith respect to algorithmic prediction the students should have seen what it is and see a not too complex example, for instance in classification. They should be able to see the close similarity between regression and machine learning methods and be able to understand the jargon. Both regression and machine learning use sometimes different notions for the same thing."
  },
  {
    "objectID": "how_sure_can_we_be.html",
    "href": "how_sure_can_we_be.html",
    "title": "8  How sure can we be about what is going on",
    "section": "",
    "text": "Probability theory provides a formal language and mathematics for dealing with chance phenomena. Probability is often counter intuitive but using the idea of expected frequency improves intuition. Probability ideas can be very useful in statistics even if there is no explicit use of a randomizing mechanism. Many social phenomena show a remarkable regularity in their overall pattern while individual events are entirely unpredictable."
  },
  {
    "objectID": "how_sure_can_we_be.html#outcome",
    "href": "how_sure_can_we_be.html#outcome",
    "title": "8  How sure can we be about what is going on",
    "section": "8.2 Outcome",
    "text": "8.2 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "9  Probability: Quantifying uncertainty and variablility",
    "section": "",
    "text": "11 Outcome\nStudents should learn the simplest and most basic facts about probability as a language to quantify uncertainty, the most important rules to manipulate probability as well as the concept of conditional probability."
  },
  {
    "objectID": "probability_and_statistics.html",
    "href": "probability_and_statistics.html",
    "title": "10  Putting probability and statistics together",
    "section": "",
    "text": "This will be conceptually the most difficult part of the course. The main ideas that should be conveyed in this unit are\n\nUsing probability theory we can derive the sampling distribution of summary statistics from which formulae for confidence intervals can be derived.\nExplain what a 95 % confidence interval means\nThe central limit theorem and the normal distribution\nThe role of systematic error due to non random causes and the role of judgment\nExplain the idea that confidence intervals can be calculated even when we observe all the data which then represent uncertainty about the parameters of an underlying metaphorical population."
  },
  {
    "objectID": "probability_and_statistics.html#outcome",
    "href": "probability_and_statistics.html#outcome",
    "title": "10  Putting probability and statistics together",
    "section": "10.2 Outcome",
    "text": "10.2 Outcome\nThe students should gain a firm understanding of confidence intervals and how they help us in quantifying uncertainty of predictions we make based on our available data. They should see and understand how and why it is sometimes more convenient and parsimonious to have formulae for confidence intervals rather than quantifying the uncertainty from simulation. The intuitive understanding of the limit theorems and when they can be legitimately applied will be important here."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html",
    "href": "answering_questions_and_claiming_discoveries.html",
    "title": "11  Answering questions and claiming discoveries",
    "section": "",
    "text": "Formal statistical testing as a major empirical tool for answering questions and claiming discoveries.\n\nTests of null hypothesis as a major part of statistical practice\np-value as the measure of incompatibility between the observed data and the null hypothesis\nThe traditional p value thresholds.\nThe need to adjust thresholds with multiple tests\nCorrespondence between p-values and confidence intervals\nNeyman-Pearson theory (alternative hypothesis and type 1 and type 2 error).\nSequential testing\nThe misinterpretation of p-values."
  },
  {
    "objectID": "answering_questions_and_claiming_discoveries.html#outcomes",
    "href": "answering_questions_and_claiming_discoveries.html#outcomes",
    "title": "11  Answering questions and claiming discoveries",
    "section": "11.2 Outcomes",
    "text": "11.2 Outcomes\nStudents should learn the basic ideas of hypothesis testing and the terminology around it."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Antony, Volk, and Atkinson Jeremy. 2013. “Infant and Child Death\nin the Human Environment of Evolutionary Adaptation.”\nEvolution and Human Behavior 34: 182–92.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for\nCommunication. New Riders.\n\n\nKuhns, Michael. n.d. “What Is a Tree?” https://forestry.usu.edu/tree-identification/what-is-a-tree.\n\n\nRoser, Max. 2019. “Child Mortality Is an Everyday Tragedy of\nEnormous Scale That Rarely Makes the Headlines.” https://ourworldindata.org/child-mortality-everyday-tragedy-no-headlines.\n\n\nSmil, Vaclav. 2020. Numbers Don’t Lie: 71 Things You Need to Know\nabout the World. Penguin Books.\n\n\nSpiegelhalter, David. 2019. The Art of Statistcis: Learning from\nData. Pelican Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  }
]